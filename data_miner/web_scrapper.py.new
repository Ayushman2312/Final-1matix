import requests
from bs4 import BeautifulSoup, Comment
import re
import random
import time
import csv
from urllib.parse import quote, urlparse, urljoin, unquote
import os
import socket
from typing import List, Dict, Tuple, Optional, Set, Union
import logging
from fake_useragent import UserAgent
import json
from playwright.async_api import async_playwright, Browser, Page, Playwright, TimeoutError as PlaywrightTimeoutError
import tldextract
import warnings
import asyncio
import sys
import platform
# Import our improved validation functions
try:
    # Try relative import first (when used as a package)
    from .improved_validators import validate_email, validate_indian_phone
except ImportError:
    # Fall back to absolute import (when run as a script)
    from improved_validators import validate_email, validate_indian_phone

# Fix for Windows asyncio pipe ResourceWarning issues
if platform.system() == 'Windows':
    # Silence the resource warnings that occur in Windows with asyncio
    import warnings
    warnings.filterwarnings("ignore", category=ResourceWarning, message="unclosed.*")
    
    # Replace the default event loop for Windows
    if sys.version_info >= (3, 8):
        # For Python 3.8+, use WindowsSelectorEventLoopPolicy
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    else:
        # For older Python versions
        asyncio.set_event_loop(asyncio.SelectorEventLoop())


class ContactScraper:
    def __init__(self, use_browser=True, debug_mode=False):
        # Set up logging
        logging.basicConfig(
            level=logging.DEBUG if debug_mode else logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            filename='scraper_log.txt'
        )
        self.logger = logging.getLogger('ContactScraper')
        
        # List of domains that should not be scraped (non required considerations)
        self.excluded_domains = [
            'indiamart.com',
            'm.indiamart.com',
            'dir.indiamart.com',
            'www.tradeindia.com',
            'tradeindia.com',
            'exportersindia.com',
            'justdial.com',
            'digitalinternationalintermesh.com',
            'wholesalebox.in',
            'www.exportersindia.com',
            'alibaba.com',
            'www.tradeindia.com'
        ]
        self.logger.info(f"Configured {len(self.excluded_domains)} excluded domains for ethical scraping")
        
        # Try to use fake_useragent for more realistic user agents
        try:
            self.ua = UserAgent(verify_ssl=False)
            self.logger.info("Using fake-useragent for more realistic user agents")
        except Exception as e:
            self.logger.warning(f"Could not initialize fake-useragent: {e}. Using fallback user agents.")
            self.ua = None
        
        # Fallback user agents with Indian locale hints
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:97.0) Gecko/20100101 Firefox/97.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.3 Safari/605.1.15',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36 Edg/98.0.1108.56'
        ]
        
        # Cookie and session management
        self.session = requests.Session()
        self.cookies = {}
        
        # Playwright configuration
        self.use_browser = use_browser
        self.browser = None
        self.browser_context = None
        self.page = None
        self.browser_initialized = False
        self.playwright = None
        
        # Royal Residential Proxies configuration
        self.proxy_host = 'geo.iproyal.com'
        self.proxy_port = 11200
        self.proxy_username = 'vnkl9BGvMRlmvWfO'
        self.proxy_password = 'EjFoKHcjcchVYwZ9'
        
        # Configure proxies with a larger pool, focusing on Indian IPs
        self.proxy_list = [
            {
                'http': f'socks5://{self.proxy_username}:{self.proxy_password}@{self.proxy_host}:{self.proxy_port}',
                'https': f'socks5://{self.proxy_username}:{self.proxy_password}@{self.proxy_host}:{self.proxy_port}'
            }
        ]
        
        # Add residential proxies with Indian IP addresses as priority
        countries = ['in', 'in', 'in', 'us', 'uk', 'sg']  # Multiple 'in' entries to increase probability
        for country_code in countries:
            self.proxy_list.append({
                'http': f'socks5://{self.proxy_username}:{self.proxy_password}@{self.proxy_host}:{self.proxy_port}',
                'https': f'socks5://{self.proxy_username}:{self.proxy_password}@{self.proxy_host}:{self.proxy_port}'
            })
        
        # Add fallback to direct connection (no proxy)
        self.proxy_list.append(None)
        
        # Add free proxies as backup (with Indian servers when possible)
        self.free_proxies = [
            {
                'http': f'socks5://{self.proxy_username}:{self.proxy_password}@{self.proxy_host}:{self.proxy_port}',
                'https': f'socks5://{self.proxy_username}:{self.proxy_password}@{self.proxy_host}:{self.proxy_port}'
            }
        ]
        
        # Extend proxy list with free proxies
        self.proxy_list.extend(self.free_proxies)
        
        # Regex patterns specifically for Indian phone numbers and emails
        # UPDATED: More comprehensive phone patterns to match various formats
        # Including number with country code, without country code, and with separators
        # This covers formats like:
        # +91 9876543210, +91-9876543210, 09876543210, 9876543210, 98765-43210, etc.
        self.phone_pattern = re.compile(r'''
            (?:
                # Format: +91 followed by 10 digits with optional separators
                (?:\+91[\s\-.]?)?(?:[6789]\d{9})
                |
                # Format: 10 digits with optional separators in between
                (?:[6789]\d{2,4}[\s\-.]?\d{2,4}[\s\-.]?\d{2,4})
                |
                # Format: Leading 0 followed by 10 digits with optional separators
                (?:0[6789]\d{1,2}[\s\-.]?\d{3,4}[\s\-.]?\d{3,4})
                |
                # Short format: 5 digits - 5 digits for some business numbers
                (?:[6789]\d{4}[\s\-.]?\d{5})
                |
                # STD Code followed by landline (e.g., 022-12345678)
                (?:0\d{2,4}[\s\-.]?\d{6,8})
                |
                # International format with country code and STD code
                (?:\+?91[\s\-.]?\d{2,4}[\s\-.]?\d{6,8})
                |
                # 8-digit landline without STD code
                (?:[2345]\d{7})
                |
                # Toll-free numbers
                (?:1(?:800|900|860)[\s\-.]?\d{3}[\s\-.]?\d{4})
                |
                # 5-digit special numbers (short codes)
                (?:\b\d{5}\b)
            )
        ''', re.VERBOSE)
        
        # Add a secondary pattern for more phone formats
        self.phone_pattern_alt = re.compile(r'''
            (?:
                # Common patterns with brackets
                (?:\(?\+91\)?[\s\-.]?)?(?:\(?\d{2,5}\)?[\s\-.]?\d{5,8})
                |
                # General 10-digit format (with or without country code)
                (?:(?:\+\d{1,2}[\s\-.]?)?\d{10})
                |
                # Parentheses format with STD code (e.g., (022) 12345678)
                (?:\(\d{2,4}\)[\s\-.]?\d{6,8})
                |
                # Formats with periods as separators
                (?:\+?91\.[\s\-]?\d{2,4}\.[\s\-]?\d{6,8})
                |
                # Formats with parentheses for country code
                (?:\(\+?91\)[\s\-.]?\d{2,4}[\s\-.]?\d{6,8})
                |
                # International format with + but without 91 (e.g., +22 12345678) - for businesses
                (?:\+\d{2}[\s\-.]?\d{8,10})
                |
                # Format with ISD+STD code commonly used by businesses 
                (?:00[\s\-.]?91[\s\-.]?\d{2,4}[\s\-.]?\d{6,8})
            )
        ''', re.VERBOSE)
        
        # UPDATED: More comprehensive email pattern
        # Matches common email formats while avoiding common false positives
        self.email_pattern = re.compile(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}')
        
        # Additional pattern for email extraction from obfuscated text
        self.obfuscated_email_pattern = re.compile(r'([a-zA-Z0-9._%+-]+)\s*(?:[\[\(]?\s*at\s*[\]\)]?|\[@\]|&#64;|@)\s*([a-zA-Z0-9.-]+)(?:[\[\(]?\s*(?:dot|\.)\s*[\]\)]?|\.|\s*\.)([a-zA-Z]{2,})')
        
        # Additional patterns for Indian domains and specific formats
        self.indian_domain_pattern = re.compile(r'\.in$|\.co\.in$|\.org\.in$|\.net\.in$')
        
        # Random request delay ranges
        self.min_delay = 2.0  # Minimum delay between requests in seconds
        self.max_delay = 10.0  # Maximum delay between requests in seconds
        
        # Anti-blocking measures
        self.recent_domains = set()  # Track recently accessed domains for rate limiting
        self.domain_access_times = {}  # Track access times per domain
        self.domain_min_interval = 20  # Minimum seconds between accessing same domain
        self.max_requests_per_domain = 3  # Max requests per domain in a session
        self.domain_request_count = {}  # Counter for requests per domain
        
        # If a domain blocks us, remember it
        self.blocked_domains = set()
        self.captcha_detected_domains = set()
        
        # Create directory for results if not exists
        os.makedirs('scraped_data', exist_ok=True)
        
        # Playwright lock to ensure only one browser operation at a time
        self.browser_lock = asyncio.Lock()
        
        # Debug mode flag
        self.debug_mode = debug_mode
        
        # Install required packages if missing
        self._ensure_dependencies()
        
        # Results tracking
        self.target_results = 0
        self.found_emails = set()
        self.found_phones = set()
        
    def _ensure_dependencies(self):
        """Ensure required dependencies are installed."""
        try:
            import socks
        except ImportError:
            print("Installing required packages for SOCKS proxy support...")
            try:
                import subprocess
                subprocess.check_call(['pip', 'install', 'PySocks'])
                print("PySocks installed successfully.")
            except Exception as e:
                print(f"Warning: Failed to install PySocks. SOCKS proxies may not work: {e}")
                print("Please run: pip install PySocks")
        
        # Check for Playwright
        try:
            import playwright
            try:
                print(f"Playwright version: {playwright.__version__}")
            except AttributeError:
                print("Playwright is installed (version not available)")
        except ImportError:
            print("Installing Playwright for browser automation...")
            try:
                import subprocess
                print("Running: pip install playwright")
                subprocess.check_call(['pip', 'install', 'playwright'])
                print("Playwright package installed successfully.")
                
                # Also install browsers
                print("Installing Playwright browsers (this may take a few minutes)...")
                try:
                    print("Running: playwright install chromium")
                    subprocess.check_call(['python', '-m', 'playwright', 'install', 'chromium'])
                    print("Chromium browser installed successfully.")
                    
                    # Install browser dependencies (important for some platforms)
                    print("Installing browser dependencies...")
                    subprocess.check_call(['python', '-m', 'playwright', 'install-deps', 'chromium'])
                    print("Browser dependencies installed successfully.")
                except Exception as browser_error:
                    print(f"Warning: Failed to install Playwright browsers: {browser_error}")
                    print("Please run manually: python -m playwright install chromium")
                    print("And: python -m playwright install-deps chromium")
            except Exception as e:
                print(f"Warning: Failed to install Playwright: {e}")
                print("Please manually run: pip install playwright")
                print("Then: python -m playwright install chromium")
                self.use_browser = False
                
    def _setup_proton_vpn_connection(self):
        """Setup ProtonVPN connection for more reliable access (if available)."""
        try:
            # Check if protonvpn-cli is installed
            import subprocess
            result = subprocess.run(['which', 'protonvpn-cli'], 
                                   stdout=subprocess.PIPE, 
                                   stderr=subprocess.PIPE)
            
            if result.returncode == 0:
                print("ProtonVPN CLI found, attempting to connect...")
                # Try to connect to ProtonVPN
                try:
                    # Connect to fastest server
                    connect_result = subprocess.run(['protonvpn-cli', 'connect', '-f'], 
                                                  stdout=subprocess.PIPE, 
                                                  stderr=subprocess.PIPE,
                                                  timeout=30)
                    
                    if connect_result.returncode == 0:
                        print("âœ… Connected to ProtonVPN")
                        return True
                    else:
                        print("âŒ Failed to connect to ProtonVPN")
                except Exception as e:
                    print(f"Error connecting to ProtonVPN: {e}")
        except Exception:
            # ProtonVPN not available, continue without it
            pass
        
        return False
            
    async def initialize_browser(self):
        """Initialize the Playwright browser with proper configuration for all operating systems.
        This method handles initialization for Windows, macOS, and Linux platforms.
        """
        if not self.use_browser:
            self.logger.warning("Browser usage is disabled. Cannot initialize browser.")
            return False
            
        # Check if already initialized
        if self.browser_initialized and self.browser:
            return True
        
        self.logger.info("Initializing Playwright browser")
        print("ðŸ”§ Initializing Playwright browser...")
        
        try:
            # Ensure Playwright is properly installed first
            try:
                import playwright
                from playwright.async_api import async_playwright
            except ImportError:
                print("Playwright not installed. Installing required packages...")
                import subprocess
                
                # Install playwright with pip
                try:
                    install_cmd = [sys.executable, "-m", "pip", "install", "--user", "playwright>=1.25.0"]
                    subprocess.run(install_cmd, check=True)
                    print("âœ… Playwright package installed")
                    
                    # Install browsers
                    install_browsers_cmd = [sys.executable, "-m", "playwright", "install", "chromium"]
                    subprocess.run(install_browsers_cmd, check=True)
                    print("âœ… Chromium browser installed")
                    
                    # On Windows, install additional dependencies
                    if platform.system() == 'Windows':
                        deps_cmd = [sys.executable, "-m", "playwright", "install-deps", "chromium"]
                        subprocess.run(deps_cmd, check=True)
                        print("âœ… Browser dependencies installed")
                    
                    # Reimport to ensure it's available
                    from playwright.async_api import async_playwright
                except subprocess.CalledProcessError as e:
                    print(f"âŒ Failed to install Playwright: {e}")
                    self.logger.error(f"Failed to install Playwright: {e}")
                    self.use_browser = False
                    return False
            
            # Properly configure the event loop based on the OS
            if platform.system() == 'Windows':
                # On Windows, ensure we're using the correct event loop policy
                asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
            
            # Explicitly create a new event loop if needed
            try:
                loop = asyncio.get_event_loop()
                if loop.is_closed():
                    asyncio.set_event_loop(asyncio.new_event_loop())
            except RuntimeError:
                # No event loop in this thread
                asyncio.set_event_loop(asyncio.new_event_loop())
            
            # Start playwright with clear error output
            try:
                self.playwright = await async_playwright().start()
            except Exception as e:
                self.logger.error(f"Failed to start Playwright: {e}")
                print(f"âŒ Failed to start Playwright: {e}")
                # Try to reinstall browser and dependencies
                try:
                    print("Attempting to repair Playwright installation...")
                    import subprocess  # Import subprocess here to ensure it's available
                    repair_cmd = [sys.executable, "-m", "playwright", "install", "--force", "chromium"]
                    subprocess.run(repair_cmd, check=True)
                    print("âœ… Chromium browser reinstalled")
                    
                    # Try starting Playwright again
                    self.playwright = await async_playwright().start()
                except Exception as e2:
                    print(f"âŒ Repair failed: {e2}")
                    self.use_browser = False
                    return False
            
            # Configure browser arguments based on OS
            browser_args = ['--no-sandbox', '--disable-dev-shm-usage', '--disable-setuid-sandbox']
            
            # Windows-specific configurations
            if platform.system() == 'Windows':
                browser_args.extend([
                    '--disable-gpu',
                    '--disable-features=IsolateOrigins,site-per-process',
                    '--disable-site-isolation-trials',
                    '--disable-web-security',
                    '--single-process'  # This can help with some Windows issues
                ])
            elif platform.system() == 'Linux':
                browser_args.extend([
                    '--disable-gpu',
                    '--single-process'
                ])
            
            # Launch with proper configuration
            try:
                self.browser = await self.playwright.chromium.launch(
                    headless=True,
                    args=browser_args,
                    timeout=60000  # 1 minute timeout
                )
            except Exception as e:
                self.logger.error(f"Failed to launch browser: {e}")
                print(f"âŒ Failed to launch browser: {e}")
                # Try again with minimal configuration
                try:
                    print("Trying minimal browser configuration...")
                    self.browser = await self.playwright.chromium.launch(
                        headless=True,
                        args=['--no-sandbox'],
                        timeout=60000
                    )
                except Exception as e2:
                    print(f"âŒ Minimal configuration failed: {e2}")
                    self.use_browser = False
                    return False
            
            # Create context with stealth settings
            try:
                self.browser_context = await self.browser.new_context(
                    viewport={"width": 1366, "height": 768},  # More common resolution
                    user_agent=self.get_random_user_agent(),
                    locale="en-US"
                )
                
                # Apply stealth settings to avoid detection
                await self._apply_stealth_settings()
            except Exception as e:
                self.logger.error(f"Failed to create browser context: {e}")
                print(f"âŒ Failed to create browser context: {e}")
                # Try with minimal context
                self.browser_context = await self.browser.new_context()
            
            # Create a new page
            self.page = await self.browser_context.new_page()
            
            # Test the browser with a simple navigation to make sure it works
            try:
                await self.page.goto("https://www.example.com", timeout=15000)
                content = await self.page.content()
                if len(content) > 100:
                    print("âœ… Browser test successful")
                    self.browser_initialized = True
                    self.logger.info("Playwright browser initialized successfully")
                    return True
                else:
                    print("âš ï¸ Browser loaded a page but content seems minimal")
                    # Continue anyway as we might still be able to use it
                    self.browser_initialized = True
                    return True
            except Exception as e:
                print(f"âš ï¸ Browser test failed: {e}")
                # Try with a different site as a backup test
                try:
                    await self.page.goto("https://www.bing.com", timeout=15000)
                    content = await self.page.content()
                    if len(content) > 100:
                        print("âœ… Browser test successful with backup site")
                        self.browser_initialized = True
                        return True
                    else:
                        print("âŒ Browser failed both test sites")
                        await self._cleanup_browser_resources()
                        self.browser_initialized = False
                        self.use_browser = False
                        return False
                except Exception as e2:
                    print(f"âŒ Browser backup test failed: {e2}")
                    await self._cleanup_browser_resources()
                    self.browser_initialized = False
                    self.use_browser = False
                    return False
                
        except Exception as e:
            import traceback
            error_details = traceback.format_exc()
            self.logger.error(f"Playwright initialization failed: {e}\n{error_details}")
            print(f"âŒ Playwright initialization failed: {e}")
            print("For detailed logs, check scraper_log.txt")
            
            # Clean up any resources
            await self._cleanup_browser_resources()
            
            # Set flags to indicate browser is not available
            self.browser_initialized = False
            self.use_browser = False
            return False
    
    async def _cleanup_browser_resources(self):
        """Clean up browser resources safely."""
        try:
            if self.page:
                await self.page.close()
                self.page = None
        except Exception as e:
            self.logger.warning(f"Error closing page: {e}")
        
        try:
            if self.browser_context:
                await self.browser_context.close()
                self.browser_context = None
        except Exception as e:
            self.logger.warning(f"Error closing browser context: {e}")
        
        try:
            if self.browser:
                await self.browser.close()
                self.browser = None
        except Exception as e:
            self.logger.warning(f"Error closing browser: {e}")
            
        try:
            if self.playwright:
                await self.playwright.stop()
                self.playwright = None
        except Exception as e:
            self.logger.warning(f"Error stopping playwright: {e}")
    
    async def _apply_stealth_settings(self):
        """Apply advanced stealth settings to avoid detection."""
        if not self.browser_context:
            return
            
        # Advanced stealth settings implemented through JavaScript
        await self.browser_context.add_init_script("""
        () => {
            // Overwrite the webdriver property
            Object.defineProperty(navigator, 'webdriver', {
                get: () => false
            });
            
            // Advanced WebDriver detection evasion
            const originalQuery = window.navigator.permissions.query;
            window.navigator.permissions.query = (parameters) => {
                if (parameters.name === 'notifications') {
                    return Promise.resolve({ state: Notification.permission });
                }
                return originalQuery(parameters);
            };
            
            // Disguise as real browser with plugins
            if (navigator.plugins.length === 0) {
                Object.defineProperty(navigator, 'plugins', {
                    get: () => [
                        { name: 'Chrome PDF Plugin', filename: 'internal-pdf-viewer', description: 'Portable Document Format' },
                        { name: 'Chrome PDF Viewer', filename: 'mhjfbmdgcfjbbpaeojofohoefgiehjai', description: 'Portable Document Format' },
                        { name: 'Native Client', filename: 'internal-nacl-plugin', description: 'Native Client Executable' }
                    ]
                });
            }
            
            // Hide automation by fixing properties used for fingerprinting
            const prototypeOverrides = [
                // Mock proper language list
                [navigator, 'languages', {
                    get: () => ['en-US', 'en', 'hi']
                }],
                
                // Make hardware concurrency realistic
                [navigator, 'hardwareConcurrency', { 
                    get: () => 8 
                }],
                
                // Hide headless flag
                [navigator, 'deviceMemory', { 
                    get: () => 8 
                }]
            ];
            
            prototypeOverrides.forEach(([obj, prop, descriptor]) => {
                if (obj[prop] === undefined) {
                    Object.defineProperty(obj, prop, descriptor);
                }
            });
            
            // Fix outerHeight and outerWidth
            if (window.outerWidth === 0 && window.outerHeight === 0) {
                Object.defineProperty(window, 'outerWidth', { get: () => window.innerWidth });
                Object.defineProperty(window, 'outerHeight', { get: () => window.innerHeight + 74 });
            }
            
            // Prevent canvas fingerprinting
            const originalGetImageData = CanvasRenderingContext2D.prototype.getImageData;
            CanvasRenderingContext2D.prototype.getImageData = function(x, y, w, h) {
                const imageData = originalGetImageData.call(this, x, y, w, h);
                
                // Add slight noise to canvas data to prevent fingerprinting
                if (imageData && imageData.data && imageData.data.length > 0) {
                    const data = imageData.data;
                    for (let i = 0; i < data.length; i += 4) {
                        // Only modify if certain conditions are met to avoid suspicion
                        if (Math.random() < 0.005) { // Very subtle changes
                            const offset = Math.floor(Math.random() * 2) - 1;
                            if (data[i] + offset >= 0 && data[i] + offset <= 255) data[i] += offset;
                            if (data[i+1] + offset >= 0 && data[i+1] + offset <= 255) data[i+1] += offset;
                            if (data[i+2] + offset >= 0 && data[i+2] + offset <= 255) data[i+2] += offset;
                        }
                    }
                }
                return imageData;
            };
        }
        """)
        
        # Set geolocation for India
        await self.browser_context.grant_permissions(['geolocation'])
        await self.browser_context.set_geolocation({"latitude": 28.6139, "longitude": 77.2090}) # New Delhi coordinates
        
    async def close_browser(self):
        """Close Playwright browser if it's open."""
        async with self.browser_lock:
            await self._cleanup_browser_resources()
            self.browser_initialized = False
                
    async def browser_get_page(self, url, max_retries=2):
        """Use Playwright browser automation to get a page."""
        # Ensure browser is initialized
        browser_ready = await self.initialize_browser()
        if not browser_ready:
            self.logger.warning("Browser not available. Falling back to requests.")
            return None
            
        domain = urlparse(url).netloc
        
        # Check for rate limiting on domain
        self._check_domain_rate_limit(domain)
        
        # Track this request
        self._track_domain_access(domain)
        
        retries = 0
        while retries < max_retries:
            try:
                self.logger.info(f"Browser requesting URL: {url}")
                
                async with self.browser_lock:
                    # Set shorter timeout for faster browsing
                    try:
                        # Navigate to the URL with reduced timeout
                        await self.page.goto(
                            url, 
                            wait_until="domcontentloaded", 
                            timeout=20000
                        )
                        
                        # Check if page loaded properly
                        page_content = await self.page.content()
                        content_length = len(page_content)
                        
                        self.logger.info(f"Initial page content length: {content_length}")
                        
                        if content_length < 100:  # Very short content might indicate a problem
                            self.logger.warning(f"Very short content ({content_length} bytes) received from {url}")
                            # Print the content for debugging
                            self.logger.debug(f"Full content: {page_content}")
                            retries += 1
                            if retries < max_retries:
                                # Wait a bit and try again
                                await asyncio.sleep(random.uniform(2, 4))
                                continue
                        
                        # Shorter wait time
                        await asyncio.sleep(random.uniform(1, 2))
                        
                        # Check for CAPTCHA presence
                        if await self._detect_captcha_in_browser():
                            self.logger.warning(f"CAPTCHA detected on {domain}. Adding to blocked domains.")
                            self.captcha_detected_domains.add(domain)
                            retries += 1
                            
                            # Try to refresh and wait
                            if retries < max_retries:
                                self.logger.info("Trying to bypass with longer wait...")
                                await asyncio.sleep(random.uniform(5, 10))
                                await self.page.reload()
                                continue
                        
                        # Check if content loaded properly (more efficient check)
                        page_content = await self.page.content()
                        if len(page_content) > 300:  # Smaller min size for valid page
                            # Extract cookies from the browser and save them to our session
                            browser_cookies = await self.browser_context.cookies()
                            for cookie in browser_cookies:
                                self.cookies[cookie['name']] = cookie['value']
                            
                            # Minimal scroll to make page more realistic but faster
                            await self.page.evaluate("window.scrollBy(0, 300)")
                            
                            # Add debugging to confirm content is valid
                            self.logger.info(f"Successfully retrieved content for {url}, length: {len(page_content)}")
                            
                            # Print a small sample of the content for debugging
                            content_sample = page_content[:200] if len(page_content) > 200 else page_content
                            self.logger.debug(f"Content sample: {content_sample}...")
                            
                            return page_content
                        else:
                            self.logger.warning(f"Page content too short ({len(page_content)} bytes) for {url}")
                            retries += 1
                    except PlaywrightTimeoutError:
                        self.logger.warning(f"Timeout loading {url}. Retrying...")
                        retries += 1
                        continue
            
            except PlaywrightTimeoutError:
                self.logger.warning(f"Timeout loading {url}. Retrying...")
                retries += 1
                await asyncio.sleep(random.uniform(1, 3))
                
            except Exception as e:
                self.logger.error(f"Browser error for {url}: {e}")
                retries += 1
                
                # If it's a fatal error, try reinitializing the browser
                if "context already closed" in str(e) or "browser closed" in str(e):
                    self.browser_initialized = False
                    browser_ready = await self.initialize_browser()
                    if not browser_ready:
                        break
                await asyncio.sleep(random.uniform(1, 3))
        
        self.logger.warning(f"Failed to get content from {url} after {max_retries} attempts")
        return None
                
    async def _detect_captcha_in_browser(self):
        """Detect if the current page has a CAPTCHA using Playwright."""
        try:
            # Get page content to check for CAPTCHA indicators
            content = await self.page.content()
            content_lower = content.lower()
            
            captcha_indicators = [
                'captcha', 'robot', 'human verification', 'security check',
                'prove you are human', 'are you a robot', 'recaptcha',
                'verify your identity', 'bot check', 'cloudflare'
            ]
            
            for indicator in captcha_indicators:
                if indicator in content_lower:
                    return True
                    
            # Check for specific CAPTCHA elements
            captcha_elements = await self.page.query_selector_all(
                "[class*='captcha'], [id*='captcha'], iframe[src*='captcha'], iframe[src*='recaptcha']"
            )
            
            if captcha_elements and len(captcha_elements) > 0:
                return True
                
            # Check for reCAPTCHA iframe
            recaptcha_iframe = await self.page.query_selector("iframe[src*='recaptcha']")
            if recaptcha_iframe:
                return True
                
            return False
            
        except Exception as e:
            self.logger.error(f"Error in CAPTCHA detection: {e}")
            return False
            
    async def _simulate_human_browsing(self):
        """Simulate sophisticated human browsing behavior in Playwright to appear more natural."""
        try:
            # Get viewport size
            viewport_size = await self.page.evaluate("""
                () => {
                    return {
                        width: window.innerWidth,
                        height: window.innerHeight
                    }
                }
            """)
            
            width = viewport_size['width']
            height = viewport_size['height']
            
            # Randomize scroll behavior
            scroll_style = random.choice(['smooth', 'stepped', 'quick'])
            
            if scroll_style == 'smooth':
                # Smooth scrolling with random pauses
                total_scroll = random.randint(300, min(3000, height))
                steps = random.randint(5, 10)
                per_step = total_scroll / steps
                
                for i in range(steps):
                    await self.page.evaluate(f"window.scrollBy(0, {per_step})")
                    await asyncio.sleep(random.uniform(0.3, 0.7))
                    
                # Sometimes scroll back up a bit
                if random.random() > 0.6:
                    await asyncio.sleep(random.uniform(0.5, 1.5))
                    await self.page.evaluate(f"window.scrollBy(0, -{random.randint(100, 300)})")
                    await asyncio.sleep(random.uniform(0.3, 0.7))
                    
            elif scroll_style == 'stepped':
                # Step scrolling with longer pauses
                for _ in range(random.randint(2, 4)):
                    scroll_amount = random.randint(300, 700)
                    await self.page.evaluate(f"window.scrollBy(0, {scroll_amount})")
                    await asyncio.sleep(random.uniform(1.0, 2.5))
                    
                    # Sometimes move mouse after scrolling
                    if random.random() > 0.5:
                        await self.page.mouse.move(
                            random.randint(100, width - 100),
                            random.randint(100, 500)
                        )
            else:
                # Quick scroll to random position
                position = random.randint(500, 1500)
                await self.page.evaluate(f"window.scrollTo(0, {position})")
                await asyncio.sleep(random.uniform(1.0, 3.0))
            
            # Sometimes hover over links or images
            if random.random() > 0.7:
                elements_to_try = ['a', 'img', 'button', 'h2', 'h3']
                element_type = random.choice(elements_to_try)
                
                elements = await self.page.query_selector_all(element_type)
                if elements and len(elements) > 0:
                    # Hover over a random element
                    random_element = elements[random.randint(0, len(elements) - 1)]
                    try:
                        await random_element.hover()
                        await asyncio.sleep(random.uniform(0.3, 1.0))
                    except:
                        pass
            
        except Exception as e:
            self.logger.warning(f"Error simulating human browsing: {e}")
    
    def get_random_user_agent(self) -> str:
        """Return a random user agent with preference for fake_useragent if available."""
        if self.ua:
            try:
                # Try to get a random real browser user agent
                browser_type = random.choice(['chrome', 'firefox', 'safari', 'edge'])
                return self.ua[browser_type]
            except Exception:
                # Fallback to random from our list
                pass
        
        return random.choice(self.user_agents)
    
    def get_random_proxy(self):
        """Return a random proxy configuration from the list."""
        return random.choice(self.proxy_list)
    
    def validate_indian_phone(self, phone: str, source: str = "unknown") -> Optional[Dict]:
        """Validate and format Indian phone numbers with enhanced validation.
        
        Args:
            phone: The phone number to validate
            source: String indicating where the number was found
            
        Returns:
            Dictionary with phone number metadata if valid, otherwise None
        """
        # Use our improved validator with source tracking
        return validate_indian_phone(phone, source)
    
    def make_request(self, url, max_retries=3):
        """Make an HTTP request with retry logic and proxy rotation."""
        # Extract domain for rate limiting
        domain = urlparse(url).netloc
        
        # Skip known blocked domains
        if domain in self.blocked_domains:
            self.logger.warning(f"Skipping known blocked domain: {domain}")
            return None
            
        # Check for CAPTCHA detection
        if domain in self.captcha_detected_domains:
            self.logger.info(f"Domain {domain} previously showed CAPTCHA. Trying with browser.")
            
            # Create a new event loop for browser operations
            if self.use_browser:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                try:
                    # First initialize the browser
                    browser_initialized = loop.run_until_complete(self.initialize_browser())
                    if browser_initialized:
                        # Then get the page
                        return loop.run_until_complete(self.browser_get_page(url, max_retries=1))
                except Exception as e:
                    self.logger.error(f"Browser request failed: {e}")
                finally:
                    loop.close()
            return None
        
        # Check if we've exceeded max requests for this domain
        if self.domain_request_count.get(domain, 0) >= self.max_requests_per_domain:
            self.logger.warning(f"Maximum request limit reached for domain {domain}")
            # Try with browser as fallback
            if self.use_browser:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                try:
                    browser_initialized = loop.run_until_complete(self.initialize_browser())
                    if browser_initialized:
                        return loop.run_until_complete(self.browser_get_page(url, max_retries=1))
                except Exception as e:
                    self.logger.error(f"Browser fallback request failed: {e}")
                finally:
                    loop.close()
            return None
            
        # Check and enforce rate limiting
        self._check_domain_rate_limit(domain)
        
        # Track this request
        self._track_domain_access(domain)
        
        retries = 0
        headers = self._get_realistic_headers(url)
        
        while retries < max_retries:
            try:
                # Add a shorter random delay before request
                delay = random.uniform(0.5, 2.0)
                self.logger.info(f"Waiting {delay:.2f} seconds before requesting {url}")
                time.sleep(delay)
                
                # Get a random proxy for this attempt
                proxies = self.get_random_proxy()
                
                # Use shorter timeout
                timeout = 15 if proxies else 10
                
                # Use our session with cookies
                response = self.session.get(
                    url,
                    headers=headers,
                    proxies=proxies,
                    timeout=timeout,
                    verify=True,
                    cookies=self.cookies
                )
                
                # Store cookies from this response
                if response.cookies:
                    self.cookies.update(dict(response.cookies))
                
                # Check response status
                if response.status_code == 200:
                    # Check for CAPTCHA or bot detection in response - simplified check
                    content_lower = response.text.lower()
                    if 'captcha' in content_lower or 'robot' in content_lower or 'automated' in content_lower:
                        self.logger.warning(f"CAPTCHA detected at {url}. Adding to CAPTCHA domains list.")
                        self.captcha_detected_domains.add(domain)
                        
                        # Fall back to browser automation if needed
                        if self.use_browser and retries >= 1:
                            loop = asyncio.new_event_loop()
                            asyncio.set_event_loop(loop)
                            try:
                                browser_initialized = loop.run_until_complete(self.initialize_browser())
                                if browser_initialized:
                                    return loop.run_until_complete(self.browser_get_page(url, max_retries=1))
                            except Exception as e:
                                self.logger.error(f"Browser fallback request failed: {e}")
                            finally:
                                loop.close()
                        
                        retries += 1
                        time.sleep(random.uniform(1, 3))  # Shorter cooldown
                    else:
                        return response
                        
                elif response.status_code == 403 or response.status_code == 429:
                    self.logger.warning(f"Request blocked or rate limited (status {response.status_code}). Rotating proxy and retrying...")
                    
                    # Add to blocked domains if consistently getting blocked
                    if retries >= 1:
                        self.blocked_domains.add(domain)
                        
                    # Try browser automation as a fallback
                    if retries >= 1 and self.use_browser:
                        loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(loop)
                        try:
                            browser_initialized = loop.run_until_complete(self.initialize_browser())
                            if browser_initialized:
                                return loop.run_until_complete(self.browser_get_page(url, max_retries=1))
                        except Exception as e:
                            self.logger.error(f"Browser fallback request failed: {e}")
                        finally:
                            loop.close()
                        
                    retries += 1
                    # Shorter backoff
                    time.sleep(random.uniform(1 * (retries + 1), 3 * (retries + 1)))
                    
                else:
                    self.logger.warning(f"Request failed with status code: {response.status_code}")
                    retries += 1
                    time.sleep(random.uniform(1, 2))
                    
            except (requests.exceptions.ProxyError, requests.exceptions.SSLError) as e:
                self.logger.warning(f"Proxy error: {e}. Trying different proxy...")
                retries += 1
                time.sleep(random.uniform(0.5, 1))
                
            except (requests.exceptions.ConnectionError, requests.exceptions.Timeout, 
                   requests.exceptions.ReadTimeout, socket.timeout) as e:
                self.logger.warning(f"Connection error: {e}. Retrying...")
                retries += 1
                time.sleep(random.uniform(0.5, 1))
                
            except Exception as e:
                self.logger.error(f"Unexpected error: {e}")
                retries += 1
                time.sleep(random.uniform(0.5, 1))
        
        # If all retries failed with regular requests, try browser automation
        if self.use_browser:
            self.logger.info(f"All request retries failed for {url}. Trying with browser automation...")
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                browser_initialized = loop.run_until_complete(self.initialize_browser())
                if browser_initialized:
                    return loop.run_until_complete(self.browser_get_page(url, max_retries=1))
            except Exception as e:
                self.logger.error(f"Final browser fallback failed: {e}")
            finally:
                loop.close()
            
        # If all retries failed, return None
        self.logger.error(f"All retries failed for URL: {url}")
        return None
    
    def _get_realistic_headers(self, url):
        """Generate realistic HTTP headers that vary between requests."""
        domain = urlparse(url).netloc
        user_agent = self.get_random_user_agent()
        
        # Base headers that most browsers send
        headers = {
            'User-Agent': user_agent,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': random.choice([
                'en-US,en;q=0.9', 
                'en-GB,en;q=0.9', 
                'en-CA,en;q=0.9',
                'en,en-US;q=0.9,fr;q=0.8'
            ]),
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',  # Do Not Track
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0'
        }
        
        # Sometimes include a referer to look more natural
        if random.random() > 0.5:
            # Pick a referer that makes sense - either Google, Bing, or a related site
            referers = [
                'https://www.google.com/',
                'https://www.bing.com/',
                'https://duckduckgo.com/',
                f'https://{domain}/'
            ]
            headers['Referer'] = random.choice(referers)
        
        # Add random non-standard but common headers with certain probability
        if random.random() > 0.7:
            headers['Sec-GPC'] = '1'  # Global Privacy Control
            
        # Add viewport dimensions for more realism
        viewports = [
            '1920x1080', '1366x768', '1536x864', '1440x900',
            '1280x720', '1600x900', '2560x1440', '3840x2160'
        ]
        if random.random() > 0.6:
            headers['Viewport-Width'] = random.choice(viewports).split('x')[0]
        
        return headers
        
    def _detect_captcha(self, response):
        """Detect if a response contains a CAPTCHA or bot detection."""
        try:
            content_lower = response.text.lower()
            
            # List of patterns that suggest CAPTCHA or bot detection
            bot_indicators = [
                'captcha', 'robot', 'human verification', 'security check',
                'prove you are human', 'are you a robot', 'recaptcha',
                'verify your identity', 'bot check', 'automated access', 
                'cloudflare', 'ddos protection'
            ]
            
            for indicator in bot_indicators:
                if indicator in content_lower:
                    return True
                    
            # Check for low content that might indicate blocking
            if len(response.text) < 500 and ("access denied" in content_lower or 
                                           "blocked" in content_lower or
                                           "detected unusual traffic" in content_lower):
                return True
                
            return False
            
        except Exception as e:
            self.logger.error(f"Error in CAPTCHA detection: {e}")
            return False
    
    def search_google(self, keyword: str, num_results: int = 10) -> List[str]:
        """Search Google and return a list of result URLs."""
        # Sanitize and validate the keyword
        keyword = keyword.strip()
        if not keyword:
            self.logger.warning("Empty search keyword provided")
            return []
            
        self.logger.info(f"Starting Google search for: '{keyword}'")
        urls = []
        page = 0
        max_pages = min(10, (num_results // 10) + 3)  # Increased max pages for more thorough search
        total_delay = 0  # Track total delay for this search
        
        # Use browser for Google searches to handle JavaScript and anti-bot measures
        if self.use_browser:
            self.logger.info("Using browser automation for Google search")
            
            # Create a new event loop for this synchronous method
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                # First initialize the browser
                browser_initialized = loop.run_until_complete(self.initialize_browser())
                if browser_initialized:
                    # Then run the search function - increased result count for better chance of meeting goal
                    browser_target = min(num_results * 2, 50)  # Try to get more results than needed
                    results = loop.run_until_complete(self._search_google_with_browser(keyword, browser_target))
                    if results:
                        self.logger.info(f"Browser-based Google search found {len(results)} results")
                        return results
                    else:
                        self.logger.warning("Browser-based Google search returned no results or was blocked")
                        # Immediately fall back to Bing search
                        self.logger.info("Immediately falling back to Bing search")
                        return self.search_bing(keyword, num_results)
                else:
                    self.logger.warning("Failed to initialize browser for Google search")
            except Exception as e:
                self.logger.warning(f"Browser-based search failed: {e}, falling back to Bing search")
                return self.search_bing(keyword, num_results)
            finally:
                # Close the loop when done
                loop.close()
        
        while len(urls) < num_results and page < max_pages:
            try:
                # Add a location parameter that rotates between searches
                geo_params = ''
                if random.random() > 0.3:
                    countries = ['in', 'us', 'uk', 'ca', 'au']
                    geo_params = f'&gl={random.choice(countries)}'
                
                # Add num parameter to request more results per page (Google may ignore this)
                results_per_page = min(100, num_results)
                
                # Vary the search query slightly to get different results
                search_variant = ''
                if page > 0 and random.random() > 0.7:
                    variants = [' company', ' business', ' contact', ' website', ' official']
                    search_variant = random.choice(variants)
                
                # Use different start parameter for pagination
                start_param = page * 10
                search_url = f"https://www.google.com/search?q={quote(keyword)}{search_variant}&num={results_per_page}&start={start_param}{geo_params}"
                
                # Add variable delay between pages to avoid detection
                if page > 0:
                    delay = random.uniform(5, 15)  # Longer delay between pages
                    self.logger.info(f"Adding page delay of {delay:.2f} seconds")
                    time.sleep(delay)
                    total_delay += delay
                
                # Check if we've been searching too long
                if total_delay > 120:  # 2 minutes max for entire search
                    self.logger.warning("Search time limit exceeded. Switching to Bing.")
                    # Immediate fallback to Bing search
                    bing_results = self.search_bing(keyword, num_results - len(urls))
                    urls.extend(bing_results)
                    break
                
                response = self.make_request(search_url)
                
                if response:
                    self.logger.info(f"Google search page {page+1} response status: {response.status_code}")
                    
                    # Check if we're being blocked immediately
                    if "unusual traffic" in response.text or "captcha" in response.text.lower():
                        self.logger.warning("Google search detected as bot. Switching to Bing immediately.")
                        # Immediate fallback to Bing search
                        bing_results = self.search_bing(keyword, num_results - len(urls))
                        urls.extend(bing_results)
                        break
                    
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # Try multiple selector patterns as Google changes its HTML structure
                    result_links = []
                    
                    # Try different selectors used by Google
                    selectors = [
                        'div.tF2Cxc a', 'div.yuRUbf a', 'div.g a', 'h3.LC20lb',
                        'div.g div.yuRUbf > a', 'div.egMi0 a', 'div.g a:not([class])',
                        'div.tF2Cxc > a', '.DKV0Md', 'a[href^="http"][ping]',
                        'a[href^="http"][data-jsarwt]', '.v5yQqb a[href^="http"]',
                        '.ftSUBd a[href^="http"]', '.N54PNb a[href^="http"]',
                        'a[href^="http"][jsname]', 'a[href^="/url?"]'
                    ]
                    
                    # Try each selector
                    for selector in selectors:
                        if selector == 'h3.LC20lb':
                            # For title elements, get parent anchor
                            for title in soup.select(selector):
                                parent_a = title.find_parent('a')
                                if parent_a and parent_a.get('href'):
                                    result_links.append(parent_a)
                        else:
                            links = soup.select(selector)
                            if links:
                                result_links.extend(links)
                                
                    # If we found links, extract and process URLs
                    if result_links:
                        self.logger.info(f"Found {len(result_links)} result links on page {page+1}")
                        
                        for link in result_links:
                            href = link.get('href')
                            if not href:
                                continue
                                
                            # Process Google redirect URLs
                            if href.startswith('/url?'):
                                match = re.search(r'/url\?q=([^&]+)', href)
                                if match:
                                    href = unquote(match.group(1))
                            
                            # Only include external links (not Google internal links)
                            if href.startswith('http') and 'google' not in href:
                                # Filter out known excluded domains
                                parsed_url = urlparse(href)
                                domain = parsed_url.netloc
                                if any(excluded in domain for excluded in self.excluded_domains):
                                    self.logger.info(f"Excluding domain from results: {domain}")
                                    continue
                                
                                # Add to results if not already included
                                if href not in urls:
                                    urls.append(href)
                    else:
                        # Fallback to all links if structured extraction fails
                        self.logger.warning("Could not find structured results, falling back to all links")
                        all_links = soup.find_all('a')
                        for link in all_links:
                            href = link.get('href')
                            if href and href.startswith('http') and 'google' not in href:
                                # Check for excluded domains
                                parsed_url = urlparse(href)
                                domain = parsed_url.netloc
                                if not any(excluded in domain for excluded in self.excluded_domains):
                                    urls.append(href)
                    
                    # Check if we need more results
                    if len(urls) >= num_results:
                        self.logger.info(f"Found sufficient results ({len(urls)}) for Google search")
                        break
                    
                    # Check if we've reached the end of results
                    if "did not match any documents" in response.text or "No results found for" in response.text:
                        self.logger.info("Reached end of Google search results")
                        break
                    
                    # Look for "next page" link to determine if there are more results
                    next_page = soup.select_one('#pnnext, a.pn')
                    if not next_page:
                        self.logger.info("No next page link found in Google search")
                        break
                    
                    page += 1
                else:
                    self.logger.warning("Google search failed, trying Bing instead...")
                    # Immediate fallback to Bing search
                    bing_results = self.search_bing(keyword, num_results - len(urls))
                    urls.extend(bing_results)
                    break
                    
            except Exception as e:
                self.logger.error(f"Error in Google search: {e}")
                time.sleep(random.uniform(1, 3))
                page += 1  # Move to next page despite error
                
        # If we still don't have enough results, try Bing to complement
        if len(urls) < num_results:
            self.logger.info(f"Google search found only {len(urls)} URLs, complementing with Bing search")
            missing_results = num_results - len(urls)
            bing_results = self.search_bing(keyword, missing_results)
            # Only add non-duplicate URLs
            for url in bing_results:
                if url not in urls:
                    urls.append(url)
        
        self.logger.info(f"Google search completed. Found {len(set(urls))} unique URLs")
        return list(set(urls))[:num_results]  # Remove duplicates and limit to requested number
    
    def search_bing(self, keyword: str, num_results: int = 10) -> List[str]:
        """Search Bing and return a list of result URLs."""
        urls = []
        page = 1
        max_pages = min(10, (num_results // 10) + 2)  # Limit page crawling
        
        # Use browser for Bing searches
        if self.use_browser:
            self.logger.info("Using browser automation for Bing search")
            
            # Create a new event loop for this synchronous method
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                # Initialize the browser
                browser_initialized = loop.run_until_complete(self.initialize_browser())
                if browser_initialized:
                    # Run the search function
                    results = loop.run_until_complete(self._search_bing_with_browser(keyword, num_results))
                    if results:
                        self.logger.info(f"Browser-based Bing search found {len(results)} results")
                        return results
                    else:
                        self.logger.warning("Browser-based Bing search returned no results or was blocked")
                        # Immediately fall back to alternative search methods
                        self.logger.info("Falling back to alternative search methods")
                        return self.extract_urls_from_search_query(keyword, num_results)
                else:
                    self.logger.warning("Failed to initialize browser for Bing search")
            except Exception as e:
                self.logger.warning(f"Browser-based Bing search failed: {e}, falling back to alternative search methods")
                return self.extract_urls_from_search_query(keyword, num_results)
            finally:
                loop.close()
        
        # Continue with regular request-based search if browser not available
        while len(urls) < num_results and page <= max_pages:
            try:
                first_result = ((page-1) * 10) + 1
                
                # Add randomized parameters to appear more natural
                country_codes = ['US', 'GB', 'CA', 'IN', 'AU']
                country_param = f"&cc={random.choice(country_codes)}" if random.random() > 0.5 else ""
                
                # Add language parameter occasionally
                lang_param = ""
                if random.random() > 0.7:
                    languages = ['en-US', 'en-IN', 'en-GB', 'en-CA', 'en-AU']
                    lang_param = f"&setlang={random.choice(languages)}"
                
                search_url = f"https://www.bing.com/search?q={quote(keyword)}&first={first_result}{country_param}{lang_param}&count=30"
                
                # Get a fresh set of headers for each request
                headers = self._get_realistic_headers(search_url)
                
                # Add variable delay between pages
                if page > 1:
                    delay = random.uniform(4, 8)
                    self.logger.info(f"Adding page delay of {delay:.2f} seconds")
                    time.sleep(delay)
                
                # Try to use a different proxy for each page
                proxy = self.get_random_proxy() if random.random() > 0.3 else None
                
                # Make direct request with special timeout handling
                try:
                    response = requests.get(
                        search_url,
                        headers=headers,
                        proxies=proxy,
                        timeout=20,
                        allow_redirects=True
                    )
                except (requests.exceptions.ConnectTimeout, requests.exceptions.ReadTimeout):
                    # If timeout, try once more with a different proxy
                    proxy = self.get_random_proxy()
                    try:
                        response = requests.get(
                            search_url,
                            headers=headers,
                            proxies=proxy,
                            timeout=25,  # Longer timeout for retry
                            allow_redirects=True
                        )
                    except Exception:
                        # If still fails, use our make_request method which has retry logic
                        response = self.make_request(search_url)
                except Exception:
                    # For any other exception, use our make_request method which has retry logic
                    response = self.make_request(search_url)
                
                if response and response.status_code == 200:
                    self.logger.info(f"Bing search page {page} response status: {response.status_code}")
                    
                    # Check if we're being blocked
                    if "captcha" in response.text.lower() or "unusual activity" in response.text.lower():
                        self.logger.warning("Bing search detected as bot. Switching to alternative method.")
                        # Immediate fallback to alternative search methods
                        alt_results = self.extract_urls_from_search_query(keyword, num_results - len(urls))
                        urls.extend(alt_results)
                        break
                    
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # Try multiple selectors for 2024 Bing layout
                    result_links = []
                    
                    # Updated Bing selectors for 2024
                    selectors = [
                        'li.b_algo h2 a',                # Standard format
                        'li.b_algo a.b_title',           # Common variant 
                        '.b_algo .b_title a',            # Alternative format
                        'li.b_algo a[href^="http"]',     # Generic format
                        'ol#b_results > li h2 a',        # Another variation
                        '.b_title a[href^="http"]',      # More generic title links
                        '.b_caption a[href^="http"]',    # Caption links
                        'a[h="ID=SERP"]',                # Links with SERP attribute
                        '#b_results a[href^="http"]',    # All links in results
                        'a[href^="http"][data-ts]'       # Links with data-ts attribute
                    ]
                    
                    # Try each selector
                    for selector in selectors:
                        try:
                            links = soup.select(selector)
                            if links:
                                result_links.extend(links)
                        except Exception as e:
                            self.logger.warning(f"Error with selector {selector}: {e}")
                            continue
                    
                    # Process found links
                    if result_links:
                        self.logger.info(f"Found {len(result_links)} result links on page {page}")
                        
                        for link in result_links:
                            href = link.get('href')
                            if href and href.startswith('http') and 'bing' not in href and 'microsoft' not in href:
                                # Check for excluded domains
                                parsed_url = urlparse(href)
                                domain = parsed_url.netloc
                                if not any(excluded in domain for excluded in self.excluded_domains):
                                    if href not in urls:
                                        urls.append(href)
                    else:
                        # Fallback to all links
                        self.logger.warning("Could not find structured results, falling back to all links")
                        all_links = soup.find_all('a')
                        for link in all_links:
                            href = link.get('href')
                            if href and href.startswith('http') and 'bing' not in href and 'microsoft' not in href:
                                # Check for excluded domains
                                parsed_url = urlparse(href)
                                domain = parsed_url.netloc
                                if not any(excluded in domain for excluded in self.excluded_domains):
                                    if href not in urls:
                                        urls.append(href)
                    
                    # Last resort - direct regex extraction of URLs
                    if len(urls) == 0:
                        self.logger.warning("Still no URLs found, trying direct URL extraction")
                        url_pattern = re.compile(r'(https?://(?!(?:www\.)?(?:bing|microsoft)\.)[-a-zA-Z0-9@:%._\+~#=]{1,256}\.[a-zA-Z0-9()]{1,6}\b(?:[-a-zA-Z0-9()@:%_\+.~#?&//=]*))')
                        found_urls = url_pattern.findall(response.text)
                        for url in found_urls:
                            if url not in urls:
                                urls.append(url)
                    
                    # Check if we've hit our target
                    if len(urls) >= num_results:
                        self.logger.info(f"Found sufficient results ({len(urls)}) for Bing search")
                        break
                    
                    # Check if we've hit the last page
                    if len(result_links) == 0 or page >= max_pages:
                        self.logger.info("Reached end of Bing search results or max pages")
                        break
                        
                    # Check for "next page" link
                    next_page = soup.select_one('a.sb_pagN, a.b_widePag_next')
                    if not next_page:
                        self.logger.info("No next page link found in Bing search")
                        break
                        
                    page += 1
                else:
                    self.logger.warning(f"Bing search failed or blocked: {response.status_code if response else 'No response'}")
                    # Immediate fallback to alternative search methods
                    alt_results = self.extract_urls_from_search_query(keyword, num_results - len(urls))
                    urls.extend(alt_results)
                    break
                    
            except Exception as e:
                self.logger.error(f"Error in Bing search: {e}")
                time.sleep(random.uniform(1, 3))
                page += 1  # Move to next page despite error
        
        # If we still don't have enough results, try alternative search
        if len(urls) < num_results:
            self.logger.info(f"Bing search found only {len(urls)} URLs, complementing with alternative search")
            missing_results = num_results - len(urls)
            alt_results = self.extract_urls_from_search_query(keyword, missing_results)
            # Only add non-duplicate URLs
            for url in alt_results:
                if url not in urls:
                    urls.append(url)
        
        self.logger.info(f"Bing search completed. Found {len(set(urls))} unique URLs")
        return list(set(urls))[:num_results]  # Remove duplicates and limit to requested number
    
    def search_duckduckgo(self, keyword: str, num_results: int = 10) -> List[str]:
        """Search DuckDuckGo as a backup search engine."""
        urls = []
        
        # Use browser for DuckDuckGo which is better at handling their JavaScript
        if self.use_browser:
            self.logger.info("Using browser automation for DuckDuckGo search")
            
            # Create a new event loop for this synchronous method
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                # First initialize the browser
                browser_initialized = loop.run_until_complete(self.initialize_browser())
                if browser_initialized:
                    # Then run the search function
                    results = loop.run_until_complete(self._search_duckduckgo_with_browser(keyword, num_results))
                    return results
                else:
                    self.logger.warning("Failed to initialize browser for DuckDuckGo search")
            except Exception as e:
                self.logger.warning(f"Browser-based DuckDuckGo search failed: {e}, falling back to regular search")
            finally:
                # Close the loop when done
                loop.close()
        
        try:
            # DuckDuckGo's HTML frontend is more scraping-friendly
            search_url = f"https://html.duckduckgo.com/html/?q={quote(keyword)}"
            response = self.make_request(search_url)
            
            if response:
                soup = BeautifulSoup(response.text, 'html.parser')
                results = soup.find_all('a', {'class': 'result__a'})
                
                for result in results:
                    if result.get('href'):
                        href = result.get('href')
                        if 'duckduckgo.com' in href:
                            # Extract actual URL from DuckDuckGo's redirect
                            href = href.split('uddg=')[1].split('&')[0] if 'uddg=' in href else None
                        if href and href.startswith('http'):
                            urls.append(href)
                
        except Exception as e:
            self.logger.error(f"Error in DuckDuckGo search: {e}")
        
        return list(set(urls))[:num_results]
    
    async def _search_google_with_browser(self, keyword: str, num_results: int = 10) -> List[str]:
        """Search Google using stealth browser automation to handle anti-bot measures."""
        urls = []
        page = 0
        max_pages = min(10, (num_results // 10) + 2)
        
        if not self.browser_initialized or not self.page:
            browser_ready = await self.initialize_browser()
            if not browser_ready:
                self.logger.error("Failed to initialize browser for Google search")
                return urls
        
        print(f"ðŸ” Using stealth browser for Google search: '{keyword}'")
        
        while len(urls) < num_results and page < max_pages:
            try:
                start_param = page * 10
                
                # Add randomized parameters for more natural appearance
                country_param = random.choice(['us', 'uk', 'in', 'ca', 'au'])
                lang_param = random.choice(['en', 'en-US', 'en-GB', 'en-IN'])
                
                # Create search URL with natural parameters
                search_url = f"https://www.google.com/search?q={quote(keyword)}&start={start_param}&gl={country_param}&hl={lang_param}"
                
                # Add a longer delay between pages to look natural
                if page > 0:
                    delay = random.uniform(8, 15) 
                    print(f"â±ï¸ Waiting {delay:.1f}s between search pages...")
                    await asyncio.sleep(delay)
                
                print(f"ðŸŒ Loading search results page {page+1}...")
                # Get the search results page
                await self.page.goto(search_url, wait_until="domcontentloaded")
                
                # Wait for results to load
                try:
                    await self.page.wait_for_selector("div.g", timeout=15000)
                except Exception:
                    # Try alternative selectors if the standard one fails
                    try:
                        await self.page.wait_for_selector("div.yuRUbf", timeout=5000)
                    except Exception:
                        # Last resort - just wait for any links
                        await self.page.wait_for_selector("a[href^='http']", timeout=5000)
                
                # Simulate human behavior - scroll a bit before extracting
                await self._simulate_human_browsing()
                
                # Check if CAPTCHA is present
                if await self._detect_captcha_in_browser():
                    self.logger.warning("CAPTCHA detected in Google search. Switching to Bing.")
                    print("ðŸ›‘ CAPTCHA detected! Switching to Bing search...")
                    return []
                
                # Extract the links using JavaScript for more robust extraction
                print("ðŸ” Extracting search result links...")
                links = await self.page.evaluate("""
                    () => {
                        // Helper to extract the actual destination URL from Google's redirect URL
                        function extractUrl(href) {
                            if (href.startsWith('/url?')) {
                                const match = href.match(/url\\?q=([^&]+)/);
                                if (match) return decodeURIComponent(match[1]);
                            }
                            return href;
                        }
                        
                        // Try various selectors Google uses (they change over time)
                        const selectors = [
                            // Modern Google selectors (2023-2024)
                            '[data-header-feature="0"] a',       // Modern organic results
                            'div[data-sokoban-container] a',     // Another modern format
                            'div.g div.yuRUbf > a',              // Still used in some views
                            'h3.LC20lb',                         // Result titles (parent has link)
                            'div.egMi0 a',                       // Another possible results container
                            'div.g a:not([class])',              // Alternative format
                            'div.tF2Cxc > a',                    // Another variation 
                            '.DKV0Md',                           // Yet another variation
                            'a[href^="http"][ping]',             // Links with ping attribute (usually results)
                            'a[href^="http"][data-jsarwt]',      // Another Google link pattern
                            '.v5yQqb a[href^="http"]',           // More specific container
                            '.ftSUBd a[href^="http"]',           // Newer format (2024)
                            '.N54PNb a[href^="http"]',           // Main content links
                            'a[href^="http"][jsname]'            // Links with jsname attribute
                        ];
                        
                        let results = [];
                        
                        // Try each selector
                        for (const selector of selectors) {
                            const elements = document.querySelectorAll(selector);
                            if (elements.length > 0) {
                                for (const element of elements) {
                                    // For title elements, we need to get the parent or ancestor with href
                                    let href;
                                    if (element.tagName === 'A') {
                                        href = element.href;
                                    } else {
                                        // Find closest anchor tag
                                        const anchor = element.closest('a');
                                        if (anchor) href = anchor.href;
                                    }
                                    
                                    if (href && href.startsWith('http') && 
                                        !href.includes('google.com/') && 
                                        !href.includes('youtube.com/') &&
                                        !href.includes('gmail.com/') &&
                                        !href.includes('gstatic.com/') &&
                                        !href.includes('accounts.google') &&
                                        !href.includes('support.google')) {
                                        
                                        // Clean the URL by removing tracking parameters
                                        try {
                                            const url = new URL(href);
                                            // Remove common tracking params
                                            ['utm_source', 'utm_medium', 'utm_campaign', 'gclid', 'fbclid'].forEach(param => {
                                                url.searchParams.delete(param);
                                            });
                                            results.push(url.toString());
                                        } catch {
                                            results.push(href);
                                        }
                                    } else if (href && href.startsWith('/url?')) {
                                        // Extract URL from Google's redirect
                                        const actualUrl = extractUrl(href);
                                        if (actualUrl && actualUrl.startsWith('http') && 
                                            !actualUrl.includes('google.com/') && 
                                            !actualUrl.includes('youtube.com/')) {
                                            results.push(actualUrl);
                                        }
                                    }
                                }
                                // If we found results with this selector, stop trying others
                                if (results.length > 0) break;
                            }
                        }
                        
                        // If still no results, fall back to all links that might be results
                        if (results.length === 0) {
                            const allLinks = document.querySelectorAll('a[href]');
                            for (const link of allLinks) {
                                let href = link.href;
                                if (href.startsWith('/url?')) {
                                    href = extractUrl(href);
                                }
                                
                                if (href && href.startsWith('http') && 
                                    !href.includes('google.com/') &&
                                    !href.includes('youtube.com/') &&
                                    !href.includes('accounts.google') &&
                                    !href.includes('support.google')) {
                                    results.push(href);
                                }
                            }
                        }
                        
                        // Remove duplicates and return
                        return [...new Set(results)];
                    }
                """)
                
                if links and len(links) > 0:
                    print(f"âœ… Found {len(links)} links on page {page+1}")
                    for link in links:
                        if link not in urls:
                            urls.append(link)
                            # Print each discovered URL in real-time
                            print(f"  ðŸ”— {link}")
                else:
                    print(f"âš ï¸ No links found on page {page+1}")
                
                # Check if we have enough results
                if len(urls) >= num_results:
                    break
                
                # Check if we've reached the end
                next_button = await self.page.query_selector("#pnnext, a.pn")
                if not next_button:
                    print("ðŸ›‘ No more search result pages available")
                    break
                    
                page += 1
                
            except Exception as e:
                self.logger.error(f"Error in Google browser search: {e}")
                print(f"âŒ Error in Google search: {str(e)[:100]}")
                break
                
        # Make the list unique and limit to requested number
        result_urls = list(set(urls))[:num_results]
        print(f"âœ… Google search completed. Found {len(result_urls)} unique URLs")
        return result_urls
    
    async def _search_bing_with_browser(self, keyword: str, num_results: int = 10) -> List[str]:
        """Search Bing using stealth browser automation."""
        urls = []
        page = 1
        max_pages = min(10, (num_results // 10) + 2)
        
        if not self.browser_initialized or not self.page:
            browser_ready = await self.initialize_browser()
            if not browser_ready:
                self.logger.error("Failed to initialize browser for Bing search")
                return urls
        
        print(f"ðŸ” Using stealth browser for Bing search: '{keyword}'")
        
        while len(urls) < num_results and page <= max_pages:
            try:
                first_result = ((page-1) * 10) + 1
                
                # Add randomized parameters for more natural appearance
                country_param = random.choice(['US', 'GB', 'IN', 'CA', 'AU'])
                lang_param = random.choice(['en-US', 'en-GB', 'en-IN'])
                
                search_url = f"https://www.bing.com/search?q={quote(keyword)}&first={first_result}&cc={country_param}&setlang={lang_param}"
                
                # Add longer delay between pages
                if page > 1:
                    delay = random.uniform(6, 12)
                    print(f"â±ï¸ Waiting {delay:.1f}s between search pages...")
                    await asyncio.sleep(delay)
                
                print(f"ðŸŒ Loading search results page {page}...")
                await self.page.goto(search_url, wait_until="domcontentloaded")
                
                # Wait for results to load
                try:
                    await self.page.wait_for_selector("li.b_algo", timeout=15000)
                except Exception:
                    # Try alternative selector
                    try:
                        await self.page.wait_for_selector("ol#b_results > li", timeout=5000)
                    except Exception:
                        # Try newer Bing selectors
                        try:
                            await self.page.wait_for_selector(".b_title", timeout=5000)
                        except Exception:
                            # Last resort - just wait for any links
                            await self.page.wait_for_selector("a[href^='http']", timeout=5000)
                
                # Simulate human behavior - scroll a bit before extracting
                await self._simulate_human_browsing()
                
                # Extract links using JavaScript for better reliability
                print("ðŸ” Extracting search result links...")
                links = await self.page.evaluate("""
                    () => {
                        // Try various Bing selectors including newest formats
                        const selectors = [
                            // Modern Bing selectors (2023-2024)
                            '.b_title h2 a',                 // Current standard format
                            'li.b_algo h2 a',                // Standard format
                            'li.b_algo a.b_title',           // Common variant 
                            '.b_algo .b_title a',            // Alternative format
                            'li.b_algo a[href^="http"]',     // Generic format
                            'ol#b_results > li h2 a',        // Another variation
                            '.b_title a[href^="http"]',      // More generic title links
                            '.b_caption a[href^="http"]',    // Caption links
                            'a[h="ID=SERP"]',                // Links with SERP attribute
                            '#b_results cite',               // URL citations (less reliable)
                            'a[href^="http"][data-ts]'       // Links with data-ts attribute (newer Bing)
                        ];
                        
                        let results = [];
                        
                        // Try each selector
                        for (const selector of selectors) {
                            const elements = document.querySelectorAll(selector);
                            if (elements.length > 0) {
                                for (const element of elements) {
                                    let href;
                                    if (element.tagName === 'A') {
                                        href = element.href;
                                    } else if (element.tagName === 'CITE') {
                                        // For citations, get the parent's link if available
                                        const parent = element.closest('li.b_algo');
                                        if (parent) {
                                            const anchor = parent.querySelector('a');
                                            if (anchor) href = anchor.href;
                                        }
                                    }
                                    
                                    if (href && href.startsWith('http') && 
                                        !href.includes('bing.com') && 
                                        !href.includes('microsoft.com') &&
                                        !href.includes('msn.com') &&
                                        !href.includes('live.com')) {
                                        
                                        // Clean the URL by removing tracking parameters
                                        try {
                                            const url = new URL(href);
                                            // Remove common tracking params
                                            ['utm_source', 'utm_medium', 'utm_campaign', 'msclkid', 'fbclid'].forEach(param => {
                                                url.searchParams.delete(param);
                                            });
                                            results.push(url.toString());
                                        } catch {
                                            results.push(href);
                                        }
                                    }
                                }
                                // If we found results with this selector, stop trying others
                                if (results.length > 0) break;
                            }
                        }
                        
                        // If still no results, fall back to all links that could be results
                        if (results.length === 0) {
                            const allLinks = document.querySelectorAll('a[href^="http"]');
                            for (const link of allLinks) {
                                let href = link.href;
                                
                                if (href && 
                                    !href.includes('bing.com') && 
                                    !href.includes('microsoft.com') &&
                                    !href.includes('msn.com') &&
                                    !href.includes('live.com') &&
                                    !href.includes('windows.com')) {
                                    results.push(href);
                                }
                            }
                        }
                        
                        // Remove duplicates
                        return [...new Set(results)];
                    }
                """)
                
                if links and len(links) > 0:
                    print(f"âœ… Found {len(links)} links on page {page}")
                    for link in links:
                        if link not in urls:
                            urls.append(link)
                            # Print each discovered URL in real-time
                            print(f"  ðŸ”— {link}")
                else:
                    print(f"âš ï¸ No links found on page {page}")
                
                # Check if we have enough results
                if len(urls) >= num_results:
                    break
                
                # Look for the next page button
                next_button = await self.page.query_selector("a.sb_pagN, a.b_widePag_next")
                if not next_button:
                    print("ðŸ›‘ No more search result pages available")
                    break
                    
                page += 1
                
            except Exception as e:
                self.logger.error(f"Error in Bing browser search: {e}")
                print(f"âŒ Error in Bing search: {str(e)[:100]}")
                break
                
        result_urls = list(set(urls))[:num_results]
        print(f"âœ… Bing search completed. Found {len(result_urls)} unique URLs")
        return result_urls
    
    async def _search_duckduckgo_with_browser(self, keyword: str, num_results: int = 10) -> List[str]:
        """Search DuckDuckGo using stealth browser automation."""
        urls = []
        
        if not self.browser_initialized or not self.page:
            browser_ready = await self.initialize_browser()
            if not browser_ready:
                self.logger.error("Failed to initialize browser for DuckDuckGo search")
                return urls
        
        print(f"ðŸ” Using stealth browser for DuckDuckGo search: '{keyword}'")
        
        try:
            # Use the HTML version which is more reliable
            search_url = f"https://html.duckduckgo.com/html/?q={quote(keyword)}"
            print(f"ðŸŒ Loading DuckDuckGo search page...")
            await self.page.goto(search_url, wait_until="domcontentloaded")
            
            # Wait for results to load
            try:
                await self.page.wait_for_selector("a.result__a", timeout=15000)
            except Exception:
                # Try alternative selector
                try:
                    await self.page.wait_for_selector(".results_links", timeout=5000)
                except Exception:
                    # Last resort
                    await self.page.wait_for_selector("a[href]", timeout=5000)
            
            # Simulate human behavior - scroll a bit before extracting
            await self._simulate_human_browsing()
            
            # Initial extraction
            print("ðŸ” Extracting search result links...")
            links = await self.page.evaluate("""
                () => {
                    const results = [];
                    // DuckDuckGo's HTML links have a special format
                    const links = document.querySelectorAll('a.result__a');
                    for (const link of links) {
                        const href = link.getAttribute('href');
                        if (href) {
                            // Extract the actual URL from DuckDuckGo's redirect
                            if (href.includes('/uddg=')) {
                                try {
                                    const decoded = decodeURIComponent(href.split('uddg=')[1].split('&')[0]);
                                    if (decoded.startsWith('http')) {
                                        results.push(decoded);
                                    }
                                } catch (e) {
                                    // If decoding fails, just use the original
                                    results.push(href);
                                }
                            } else if (href.startsWith('http')) {
                                results.push(href);
                            }
                        }
                    }
                    return [...new Set(results)];
                }
            """)
            
            if links and len(links) > 0:
                print(f"âœ… Found {len(links)} links on initial page")
                for link in links:
                    if link not in urls:
                        urls.append(link)
                        # Print each discovered URL in real-time
                        print(f"  ðŸ”— {link}")
            else:
                print("âš ï¸ No links found on initial page")
            
            # If we need more results, try to load more
            more_attempts = 0
            while len(urls) < num_results and more_attempts < 3:
                # Look for "More Results" button
                try:
                    more_button = await self.page.query_selector("input.result--more__btn")
                    if more_button:
                        print("ðŸ”„ Loading more results...")
                        await more_button.click()
                        
                        # Wait for new results to load
                        await asyncio.sleep(random.uniform(3, 6))
                        
                        # Extract new links
                        new_links = await self.page.evaluate("""
                            () => {
                                const results = [];
                                const links = document.querySelectorAll('a.result__a');
                                for (const link of links) {
                                    const href = link.getAttribute('href');
                                    if (href) {
                                        if (href.includes('/uddg=')) {
                                            try {
                                                const decoded = decodeURIComponent(href.split('uddg=')[1].split('&')[0]);
                                                if (decoded.startsWith('http')) {
                                                    results.push(decoded);
                                                }
                                            } catch (e) {
                                                results.push(href);
                                            }
                                        } else if (href.startsWith('http')) {
                                            results.push(href);
                                        }
                                    }
                                }
                                return [...new Set(results)];
                            }
                        """)
                        
                        if new_links and len(new_links) > 0:
                            print(f"âœ… Found {len(new_links)} additional links")
                            for link in new_links:
                                if link not in urls:
                                    urls.append(link)
                                    # Print each discovered URL in real-time
                                    print(f"  ðŸ”— {link}")
                        
                    more_attempts += 1
                except Exception as e:
                    self.logger.error(f"Error loading more DuckDuckGo results: {e}")
                    print(f"âš ï¸ Error loading more results: {str(e)[:100]}")
                    break
                    
        except Exception as e:
            self.logger.error(f"Error in DuckDuckGo browser search: {e}")
            print(f"âŒ Error in DuckDuckGo search: {str(e)[:100]}")
            
        result_urls = list(set(urls))[:num_results]
        print(f"âœ… DuckDuckGo search completed. Found {len(result_urls)} unique URLs")
        return result_urls
    
    def extract_contacts_from_url(self, url: str) -> Tuple[Set[str], Set[Union[str, Dict]]]:
        """Extract contact information from a URL with enhanced validation.
        
        Args:
            url: The URL to extract contacts from
            
        Returns:
            Tuple of (set of emails, set of phone dictionaries)
        """
        # Make sure we're using a universal format for URLs
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url
        
        try:
            # Extract the domain for rate limiting
            parsed_url = urlparse(url)
            domain = parsed_url.netloc
            
            # Check if domain is in excluded domains list
            if any(excluded in domain for excluded in self.excluded_domains):
                self.logger.warning(f"Skipping excluded domain: {domain}")
                return set(), set()
            
            # Check rate limits
            self._check_domain_rate_limit(domain)
            
            self.logger.info(f"Extracting contacts from: {url}")
            
            # First, try to directly scrape the main page
            response = self.make_request(url)
            if not response:
                self.logger.warning(f"Failed to get response from {url}")
                return set(), set()
            
            # Extract contacts from the main page
            html_content = response.text
            emails, phones = self._extract_contacts_comprehensive(html_content, url, domain)
            
            # Check if we already have enough contacts to meet our target
            # This prevents unnecessary processing of more pages
            if len(self.found_emails) + len(emails) >= self.target_results and len(self.found_phones) + len(phones) >= self.target_results:
                # Update our tracking sets
                self.found_emails.update(emails)
                self.found_phones.update(phones)
                return emails, phones
            
            # Track access
            self._track_domain_access(domain)
            
            # Look for dedicated contact pages
            contact_urls = self._find_contact_urls(html_content, url, domain)
            for contact_url in contact_urls:
                if len(contact_urls) > 2:
                    # If there are many potential contact pages, only check the most likely ones
                    lower_url = contact_url.lower()
                    if not ('contact' in lower_url or 'about' in lower_url):
                        continue
                
                    try:
                        # Don't check URLs we've already seen
                        if contact_url == url:
                            continue
                            
                        self.logger.info(f"Checking contact page: {contact_url}")
                        
                        # Add delay between requests
                        time.sleep(random.uniform(1, 3))
                        
                        # Extract from the contact page
                        contact_emails, contact_phones = self._extract_from_contact_page(contact_url, domain)
                        
                        # Update our results with contact page findings
                        emails.update(contact_emails)
                        phones.update(contact_phones)
                        
                        # Update our global tracking
                        self.found_emails.update(emails)
                        self.found_phones.update(phones)
                        
                        # If we found good contact info, we can stop checking more pages
                        if len(contact_emails) >= 2 or len(contact_phones) >= 2:
                            break
                            
                        # Check if we've reached our target in total
                        if len(self.found_emails) >= self.target_results and len(self.found_phones) >= self.target_results:
                            break
                            
                    except Exception as e:
                        self.logger.error(f"Error extracting from contact page {contact_url}: {e}")
            
            return emails, phones
        except Exception as e:
            self.logger.error(f"Error extracting contacts from {url}: {e}")
            return set(), set()
    
    def _extract_from_contact_page(self, url: str, domain: str) -> Tuple[Set[str], Set[Union[str, Dict]]]:
        """Extract contact information from a dedicated contact page."""
        try:
            # Check rate limits
            self._check_domain_rate_limit(domain)
            
            # Get the contact page
            response = self.make_request(url)
            if not response:
                return set(), set()
            
            # Track this access
            self._track_domain_access(domain)
            
            # Extract all contacts from the page
            html_content = response.text
            emails, phones = self._extract_contacts_comprehensive(html_content, url, domain)
            
            return emails, phones
        except Exception as e:
            self.logger.error(f"Error in contact page extraction for {url}: {e}")
            return set(), set()
    
    def _extract_contacts_comprehensive(self, html_content: str, url: str, domain: str) -> Tuple[Set[str], Set[Dict]]:
        """Extract all contact information from HTML with enhanced extraction and filtering.
        
        Args:
            html_content: The HTML content to extract contacts from
            url: The URL of the page
            domain: The domain of the website
            
        Returns:
            Tuple of (set of emails, set of phone dictionaries)
        """
        # Initialize result sets
        emails = set()
        phones = set()
        
        # Use Beautiful Soup for structured parsing
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Extract all emails from the page text first
        text_content = soup.get_text()
        emails.update(self._extract_emails_from_text(text_content))
        
        # Tag the main parts of the page for better source tracking
        sources = {
            'homepage': url.rstrip('/') == domain.rstrip('/'),
            'contact_page': any(term in url.lower() for term in ['contact', 'about', 'reach', 'connect']),
            'footer': False,
            'header': False
        }
        
        # ENHANCED: Preprocess the HTML to better identify phone numbers
        # Replace common phone pattern delimiters with more consistent separators
        html_content = re.sub(r'(\d)\s*[-â€“â€”Â·â€¢|:/]\s*(\d)', r'\1-\2', html_content)
        
        # Try to identify footer and header sections
        footer_sections = soup.select('footer, .footer, [id*="footer"], [class*="footer"]')
        header_sections = soup.select('header, .header, [id*="header"], [class*="header"]')
        
        # Process footers with specific source tag
        for footer in footer_sections:
            footer_text = footer.get_text()
            footer_phones = self._extract_phones_from_text(footer_text, 'footer')
            phones.update(footer_phones)
            footer_emails = self._extract_emails_from_text(footer_text)
            emails.update(footer_emails)
            sources['footer'] = True
        
        # Process headers with specific source tag
        for header in header_sections:
            header_text = header.get_text()
            header_phones = self._extract_phones_from_text(header_text, 'header')
            phones.update(header_phones)
            header_emails = self._extract_emails_from_text(header_text)
            emails.update(header_emails)
            sources['header'] = True
        
        # Identify contact sections - expanded list of contact section identifiers
        contact_sections = []
        contact_areas = soup.select('div[id*="contact"], div[class*="contact"], section[id*="contact"], section[class*="contact"], '
                                   'div[id*="phone"], div[class*="phone"], div[id*="call"], div[class*="call"], '
                                   '.contact-info, .contact-details, .phone-number, .tel, .contact-number, '
                                   'div[id*="address"], div[class*="address"], .address-details')
        for area in contact_areas:
            contact_sections.append(area.get_text())
        
        # Look for phones in contact sections specifically
        for section in contact_sections:
            section_phones = self._extract_phones_from_text(section, 'contact_section')
            phones.update(section_phones)
        
        # Look for phones in main content with appropriate source tag
        source_tag = 'homepage' if sources['homepage'] else 'contact_page' if sources['contact_page'] else 'other_page'
        main_phones = self._extract_phones_from_text(text_content, source_tag)
        phones.update(main_phones)
        
        # ENHANCED: Look for elements with phone-related attributes or class names
        # Common attributes and class names for phone numbers
        phone_attributes = ['tel', 'phone', 'mobile', 'contact', 'call']
        phone_class_patterns = ['phone', 'tel', 'contact', 'call', 'mobile', 'number', 'dial', 'support']
        
        # Find elements with tel: links (most reliable source of phone numbers)
        tel_links = soup.select('a[href^="tel:"]')
        for link in tel_links:
            href = link.get('href', '')
            if href.startswith('tel:'):
                phone_number = href[4:].strip()
                valid_phone = self.validate_indian_phone(phone_number, 'tel_link')
                if valid_phone:
                    phones.add(valid_phone)
        
        # Find elements with phone-related attributes
        for attr in phone_attributes:
            elements = soup.select(f'[{attr}]')
            for elem in elements:
                attr_value = elem.get(attr, '')
                if attr_value and re.search(r'\d', attr_value):  # Must contain at least one digit
                    valid_phone = self.validate_indian_phone(attr_value, f'attr_{attr}')
                    if valid_phone:
                        phones.add(valid_phone)
        
        # Find elements with phone-related classes
        for pattern in phone_class_patterns:
            elements = soup.select(f'.{pattern}, [class*="{pattern}"]')
            for elem in elements:
                elem_text = elem.get_text()
                # Detect if the element might contain multiple phone numbers
                if elem_text.count('+91') > 1 or re.findall(r'\d{9,}', elem_text.replace(' ', '')):
                    # Split by common separators to handle multiple phone numbers in the same element
                    split_parts = re.split(r'[,;/|]|\s+(?:and|or|&amp;|&)\s+', elem_text)
                    for part in split_parts:
                        if len(part.strip()) >= 8:  # Only process parts long enough to be a phone number
                            part_phones = self._extract_phones_from_text(part, f'multi_phone_{pattern}')
                            phones.update(part_phones)
                else:
                    # Regular processing for single phone
                    phone_numbers = self._extract_phones_from_text(elem_text, f'class_{pattern}')
                    phones.update(phone_numbers)
        
        # Look for phone numbers in special containers often used in business listings
        business_containers = soup.select('.business-info, .company-info, .organization-info, .contact-card, .contact-box, '
                                         '.profile-card, .branch-info, .address-card, .office-info, .location-info')
        for container in business_containers:
            container_phones = self._extract_phones_from_text(container.get_text(), 'business_container')
            phones.update(container_phones)
        
        # Find phone numbers in elements with phone-related labels
        phone_labels = ['Phone:', 'Tel:', 'Mobile:', 'Contact:', 'Call:', 'Telephone:', 'Ph:', 'Cell:', 'Landline:', 'Office:']
        for label in phone_labels:
            label_elements = [element for element in soup.find_all(string=re.compile(label, re.IGNORECASE)) 
                              if not isinstance(element, Comment)]
            
            for element in label_elements:
                # Check if the phone number is in the same tag
                parent = element.parent
                if parent:
                    # Look for phone numbers in the parent's text
                    parent_text = parent.get_text()
                    label_phones = self._extract_phones_from_text(parent_text, f'label_{label.lower().replace(":", "")}')
                    phones.update(label_phones)
                    
                    # Check up to 3 siblings to find phone numbers (they might be in adjacent elements)
                    curr_element = parent
                    checked = 0
                    while checked < 3 and curr_element.next_sibling:
                        checked += 1
                        next_sibling = curr_element.next_sibling
                        if isinstance(next_sibling, str):
                            sibling_text = next_sibling
                        else:
                            sibling_text = next_sibling.get_text() if hasattr(next_sibling, 'get_text') else str(next_sibling)
                        
                        if re.search(r'\d', sibling_text):  # Check if sibling has any digits
                            sibling_phones = self._extract_phones_from_text(sibling_text, f'sibling{checked}_of_label_{label.lower().replace(":", "")}')
                            phones.update(sibling_phones)
                            if sibling_phones:  # If we found phones, no need to check more siblings
                                break
                        
                        curr_element = next_sibling
                        
                    # Also check parent's parent for context (commonly used structure)
                    if parent.parent:
                        parent_parent_text = parent.parent.get_text()
                        if len(parent_parent_text) - len(parent_text) > 5:  # Only if parent's parent has additional text
                            parent_phones = self._extract_phones_from_text(parent_parent_text, f'parent_of_label_{label.lower().replace(":", "")}')
                            phones.update(parent_phones)
        
        # Check structured data for contacts (JSON-LD)
        json_ld_scripts = soup.select('script[type="application/ld+json"]')
        for script in json_ld_scripts:
            try:
                json_data = json.loads(script.string)
                self._extract_from_json(json_data, emails, phones)
            except (json.JSONDecodeError, TypeError, AttributeError):
                pass
        
        # Look for vcard/hcard microformats
        vcards = soup.select('.vcard, .h-card')
        for vcard in vcards:
            vcard_text = vcard.get_text()
            vcard_phones = self._extract_phones_from_text(vcard_text, 'vcard')
            phones.update(vcard_phones)
            vcard_emails = self._extract_emails_from_text(vcard_text)
            emails.update(vcard_emails)
        
        # Look for tel: links
        tel_links = soup.select('a[href^="tel:"]')
        for link in tel_links:
            href = link.get('href', '')
            if href.startswith('tel:'):
                phone_number = href[4:].strip()
                valid_phone = self.validate_indian_phone(phone_number, 'tel_link')
                if valid_phone:
                    phones.add(valid_phone)
        
        # Look for mailto: links
        mailto_links = soup.select('a[href^="mailto:"]')
        for link in mailto_links:
            href = link.get('href', '')
            if href.startswith('mailto:'):
                email = href[7:].strip()
                if self._validate_email(email):
                    emails.add(email)
        
        # Check for meta tags with contact information
        meta_tags = soup.select('meta[name*="contact"], meta[property*="contact"], '
                               'meta[name*="phone"], meta[property*="phone"], '
                               'meta[name*="email"], meta[property*="email"]')
        for tag in meta_tags:
            content = tag.get('content', '')
            if '@' in content:
                emails.update(self._extract_emails_from_text(content))
            elif re.search(r'\d', content):
                phones.update(self._extract_phones_from_text(content, 'meta_tag'))
                
        # Check for structured contact data in tables
        contact_tables = soup.select('table.contact, table.address, table.contact-info, table.details, table:has(th:contains("Contact"))')
        for table in contact_tables:
            table_phones = self._extract_phones_from_text(table.get_text(), 'contact_table')
            phones.update(table_phones)
            
            # Check individual cells for phone numbers
            cells = table.select('td, th')
            for cell in cells:
                cell_text = cell.get_text()
                if any(label.lower() in cell_text.lower() for label in phone_labels):
                    # Check this cell and adjacent cells for phones
                    cell_phones = self._extract_phones_from_text(cell_text, 'table_cell_with_label')
                    phones.update(cell_phones)
                    
                    # Check next cell (if we found a label, the phone might be in adjacent cell)
                    next_cell = cell.find_next('td')
                    if next_cell:
                        next_cell_phones = self._extract_phones_from_text(next_cell.get_text(), 'table_adjacent_cell')
                        phones.update(next_cell_phones)
        
        # Check <dl> definition lists which are often used for contact info
        dl_elements = soup.select('dl, .definition-list')
        for dl in dl_elements:
            # Check for dt/dd pairs
            dt_elements = dl.select('dt')
            for dt in dt_elements:
                dt_text = dt.get_text().lower()
                if any(label.lower().replace(':', '') in dt_text for label in phone_labels):
                    # If dt contains phone label, check the following dd
                    dd = dt.find_next('dd')
                    if dd:
                        dd_phones = self._extract_phones_from_text(dd.get_text(), 'dl_definition')
                        phones.update(dd_phones)
        
        # Check <address> elements which often contain contact information
        address_elements = soup.select('address, .address')
        for address in address_elements:
            address_phones = self._extract_phones_from_text(address.get_text(), 'address_element')
            phones.update(address_phones)
            address_emails = self._extract_emails_from_text(address.get_text())
            emails.update(address_emails)
        
        # Filter and normalize emails
        filtered_emails = {email for email in emails if self._validate_email(email)}
        
        # Return the filtered results
        return filtered_emails, phones
    
    def _extract_from_json(self, json_data: Dict, emails: Set[str], phones: Set[Dict]):
        """Extract contact information from JSON-LD or structured data."""
        # Email extraction
        email_keys = ['email', 'emailAddress', 'contactEmail', 'contactPoint', 'contactInfo']
        for key in email_keys:
            if key in json_data:
                value = json_data[key]
                if isinstance(value, str) and '@' in value:
                    emails.update(self._extract_emails_from_text(value))
                elif isinstance(value, dict) and 'email' in value:
                    emails.update(self._extract_emails_from_text(value['email']))
                elif isinstance(value, list):
                    for item in value:
                        if isinstance(item, dict) and 'email' in item:
                            emails.update(self._extract_emails_from_text(item['email']))
        
        # Phone extraction
        phone_keys = ['telephone', 'phone', 'phoneNumber', 'contactNumber', 'contactPhone', 'contactPoint']
        for key in phone_keys:
            if key in json_data:
                value = json_data[key]
                if isinstance(value, str):
                    phones.update(self._extract_phones_from_text(value, f"json_data_{key}"))
                elif isinstance(value, dict) and 'telephone' in value:
                    phones.update(self._extract_phones_from_text(value['telephone'], f"json_data_{key}_telephone"))
                elif isinstance(value, list):
                    for item in value:
                        if isinstance(item, dict) and 'telephone' in item:
                            phones.update(self._extract_phones_from_text(item['telephone'], f"json_data_{key}_list_item_telephone"))
        
        # Recursively check nested dictionaries
        for key, value in json_data.items():
            if isinstance(value, dict):
                self._extract_from_json(value, emails, phones)
            elif isinstance(value, list):
                for item in value:
                    if isinstance(item, dict):
                        self._extract_from_json(item, emails, phones)
    
    def _find_contact_urls(self, html_content: str, base_url: str, domain: str) -> List[str]:
        """Find contact page URLs from the main page."""
        contact_urls = []
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Common patterns for contact page links
        contact_patterns = [
            'contact', 'contact-us', 'reach-us', 'connect', 'get-in-touch',
            'about-us', 'feedback', 'support', 'help', 'contactus'
        ]
        
        # Look for links with contact-related text or URLs
        for a in soup.find_all('a', href=True):
            href = a['href']
            text = a.get_text().lower().strip()
            
            # Check if the link text contains contact-related keywords
            is_contact_text = any(pattern in text for pattern in contact_patterns)
            
            # Check if the URL contains contact-related keywords
            is_contact_url = any(pattern in href.lower() for pattern in contact_patterns)
            
            if is_contact_text or is_contact_url:
                # Build absolute URL if relative
                if href.startswith('/'):
                    contact_url = urljoin(base_url, href)
                elif href.startswith('http'):
                    # Only include links to the same domain
                    if domain in href:
                        contact_url = href
                    else:
                        continue
                else:
                    contact_url = urljoin(base_url, href)
                
                # Add to contact URLs if not already there and not the same as base URL
                if contact_url != base_url and contact_url not in contact_urls:
                    contact_urls.append(contact_url)
        
        return contact_urls
    
    def _extract_emails_from_text(self, text: str) -> Set[str]:
        """Extract and validate email addresses from text."""
        emails = set()
        
        # Standard email pattern
        email_matches = self.email_pattern.findall(text)
        for email in email_matches:
            # Basic validation to filter out false positives
            if self._validate_email(email):
                emails.add(email.lower())
        
        # Obfuscated email pattern (e.g., "user at domain dot com")
        obfuscated_matches = self.obfuscated_email_pattern.findall(text)
        for match in obfuscated_matches:
            if len(match) == 3:  # Should have 3 parts: username, domain, TLD
                reconstructed_email = f"{match[0]}@{match[1]}.{match[2]}"
                if self._validate_email(reconstructed_email):
                    emails.add(reconstructed_email.lower())
        
        return emails
    
    def _validate_email(self, email: str) -> bool:
        """Validate an email address with enhanced filtering for HTML/CSS artifacts."""
        # Use our improved validator
        return validate_email(email)
    
    def _extract_phones_from_text(self, text: str, source: str = "unknown") -> Set[Union[str, Dict]]:
        """Extract and validate phone numbers from text.
        
        Args:
            text: The text to extract phone numbers from
            source: Where the text was found (e.g., 'homepage', 'contact_page')
            
        Returns:
            Set of validated phone numbers in E.164 format or as dictionaries with metadata
        """
        phones = set()
        
        if not text or len(text) < 5:
            return phones
            
        # Preprocessing to make common phone formats more recognizable
        # Replace common phone format indicators
        text = re.sub(r'(?i)(phone|mobile|telephone|contact|call|ph|tel|mob)(\s*)(:|at|us|on|no|number|#)(\s*)', ' ', text)
        
        # Add space around specific separators to help identify boundaries
        text = re.sub(r'([()])', r' \1 ', text)
        
        # Clean multiple spaces and special characters that might interfere with pattern matching
        text = re.sub(r'\s+', ' ', text)
        
        # Replace common unicode symbols that might be used as separators
        text = text.replace('â€¢', ' ').replace('|', ' ').replace('/', ' ').replace('\\', ' ')
        
        # Some sites use "dot" text to separate groups of numbers
        text = re.sub(r'(\d+)\s*dot\s*(\d+)', r'\1.\2', text, flags=re.IGNORECASE)
        
        # Look for context indicators to better target phone extraction
        # Phrases commonly preceding phone numbers
        phone_indicators = ['call', 'phone', 'contact', 'mobile', 'dial', 'telephone', 'cell', 'ph:', 'tel:']
        
        # Extract text segments more likely to contain phone numbers
        segments = []
        
        # Split by common delimiters
        for segment in re.split(r'[;,\n\r]', text):
            # Check if segment has any of the phone indicators
            if any(indicator in segment.lower() for indicator in phone_indicators):
                segments.append(segment)
            # Or if it contains a pattern that looks like a phone number
            elif re.search(r'\d{3}[-.\s]?\d{3}[-.\s]?\d{4}', segment):
                segments.append(segment)
            # Check for parentheses which are commonly used in phone numbers
            elif '(' in segment and ')' in segment and re.search(r'\d', segment):
                segments.append(segment)
            # Check for segments with +91 which is Indian country code
            elif '+91' in segment or '+91-' in segment:
                segments.append(segment)
            # Check for segments with a high density of digits
            elif len(re.findall(r'\d', segment)) >= 8:
                segments.append(segment)
        
        # Add the full text as a fallback
        segments.append(text)
        
        # Process all segments including the full text
        for segment in segments:
            # Use our main pattern for Indian phone numbers
            phone_matches = self.phone_pattern.findall(segment)
            for phone_match in phone_matches:
                # Handle different match types
                if isinstance(phone_match, tuple):
                    for group in phone_match:
                        if group and len(group.strip()) >= 5:  # Only process non-empty groups with sufficient length
                            valid_phone = self.validate_indian_phone(group, source)
                            if valid_phone:
                                phones.add(valid_phone)
                else:
                    if phone_match and len(phone_match.strip()) >= 5:
                        valid_phone = self.validate_indian_phone(phone_match, source)
                        if valid_phone:
                            phones.add(valid_phone)
            
            # Try alternate pattern for more formats
            alt_matches = self.phone_pattern_alt.findall(segment)
            for phone_match in alt_matches:
                if isinstance(phone_match, tuple):
                    for group in phone_match:
                        if group and len(group.strip()) >= 5:
                            valid_phone = self.validate_indian_phone(group, source)
                            if valid_phone:
                                phones.add(valid_phone)
                else:
                    if phone_match and len(phone_match.strip()) >= 5:
                        valid_phone = self.validate_indian_phone(phone_match, source)
                        if valid_phone:
                            phones.add(valid_phone)
                            
        # Additional extraction for special cases:
        # Some phone numbers might not be matched by the regex due to unusual formatting
        # This part tries to find raw digit sequences and validate them
        # Look for sequences of digits that are 8, 10, or 11 digits long (common in Indian phone numbers)
        raw_digit_matches = re.findall(r'\b\d{8}\b|\b\d{10}\b|\b\d{11}\b', text)
        for digits in raw_digit_matches:
            valid_phone = self.validate_indian_phone(digits, f"{source}_raw_digits")
            if valid_phone:
                phones.add(valid_phone)
        
        # Look for numbers with +91 prefix in raw form
        plus91_matches = re.findall(r'\+91\s*\d{10}', text)
        for match in plus91_matches:
            valid_phone = self.validate_indian_phone(match, f"{source}_plus91")
            if valid_phone:
                phones.add(valid_phone)
                
        # Look for numbers with parentheses around STD codes
        std_matches = re.findall(r'\(\s*0?\d{2,4}\s*\)\s*\d{6,8}', text)
        for match in std_matches:
            valid_phone = self.validate_indian_phone(match, f"{source}_std_code")
            if valid_phone:
                phones.add(valid_phone)
        
        return phones
    
    def test_extraction(self, test_urls=None):
        """Test the extraction functionality with sample URLs."""
        if test_urls is None:
            # Default test URLs with known contact information
            test_urls = [
                "https://www.digitalmarketingdelhi.in/",
                "https://www.socialbeat.in/", 
                "https://digitalready.co/",
                "https://www.webchutney.com/contact",
                "https://www.techmagnate.com/contact-us.html"
            ]
        
        print("\n=== CONTACT EXTRACTION TEST ===")
        all_emails = set()
        all_phones = set()
        
        for url in test_urls:
            print(f"\nTesting URL: {url}")
            try:
                emails, phones = self.extract_contacts_from_url(url)
                
                if emails:
                    print("Emails found:")
                    for email in emails:
                        print(f"  - {email}")
                        all_emails.add(email)
                else:
                    print("No emails found")
                    
                if phones:
                    print("Phones found:")
                    for phone in phones:
                        print(f"  - {phone}")
                        all_phones.add(phone)
                else:
                    print("No phones found")
                    
            except Exception as e:
                print(f"Error: {e}")
        
        print("\nTest Summary:")
        print(f"Total unique emails found: {len(all_emails)}")
        print(f"Total unique phones found: {len(all_phones)}")
        return all_emails, all_phones
    
    def _check_domain_rate_limit(self, domain):
        """Check if we should rate limit a domain access and wait if needed."""
        current_time = time.time()
        
        # If we've accessed this domain recently, enforce a delay
        if domain in self.domain_access_times:
            last_access = self.domain_access_times[domain]
            elapsed = current_time - last_access
            
            if elapsed < self.domain_min_interval:
                wait_time = self.domain_min_interval - elapsed + random.uniform(1, 5)
                self.logger.info(f"Rate limiting domain {domain}. Waiting {wait_time:.2f} seconds")
                time.sleep(wait_time)
    
    def _track_domain_access(self, domain):
        """Track when we accessed a domain and how many times."""
        self.domain_access_times[domain] = time.time()
        
        if domain in self.domain_request_count:
            self.domain_request_count[domain] += 1
        else:
            self.domain_request_count[domain] = 1
            
        # Add to recent domains set for rate limiting
        self.recent_domains.add(domain)
        
    async def _check_domain_rate_limit_async(self, domain):
        """Async version of domain rate limiting."""
        current_time = time.time()
        
        # If we've accessed this domain recently, enforce a delay
        if domain in self.domain_access_times:
            last_access = self.domain_access_times[domain]
            elapsed = current_time - last_access
            
            if elapsed < self.domain_min_interval:
                wait_time = self.domain_min_interval - elapsed + random.uniform(1, 5)
                self.logger.info(f"Rate limiting domain {domain}. Waiting {wait_time:.2f} seconds")
                await asyncio.sleep(wait_time)
                
    def is_indian_domain(self, url):
        """Check if a domain is likely to be Indian based on TLD or content."""
        # First check TLD for .in domains
        domain = urlparse(url).netloc
        if self.indian_domain_pattern.search(domain):
            return True
            
        # Check for common Indian domain names
        indian_terms = ['india', 'bharat', 'desi', 'hindustan', 'bharatiya', 'sarkari']
        for term in indian_terms:
            if term in domain.lower():
                return True
                
        # Use TLD extract to check if the site is from India
        extract_result = tldextract.extract(url)
        if extract_result.suffix == 'in':
            return True
            
        return False
        
    def __enter__(self):
        """Support for 'with' context manager."""
        return self
        
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Clean up resources when exiting 'with' context."""
        # Create a new event loop for cleanup
        try:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            # Run the cleanup
            try:
                loop.run_until_complete(self.close_browser())
            except Exception as e:
                self.logger.warning(f"Error during browser cleanup in context exit: {e}")
                
            # Clean up the loop
            try:
                loop.close()
            except Exception as e:
                self.logger.warning(f"Error closing loop in context exit: {e}")
        except Exception as e:
            self.logger.warning(f"Error in context exit: {e}")
        
        return False  # Don't suppress exceptions
        
    def __del__(self):
        """Destructor to ensure proper cleanup."""
        try:
            # Attempt to safely clean up browser resources
            if hasattr(self, 'browser') and self.browser:
                # Create a new event loop for cleanup
                try:
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                    
                    # Run the cleanup
                    try:
                        loop.run_until_complete(self.close_browser())
                    except Exception:
                        # Silently ignore errors in cleanup
                        pass
                    
                    # Clean up the loop
                    try:
                        loop.close()
                    except Exception:
                        pass
                except Exception:
                    # Last resort - set everything to None
                    if hasattr(self, 'browser'):
                        self.browser = None
                    if hasattr(self, 'browser_context'):
                        self.browser_context = None
                    if hasattr(self, 'page'):
                        self.page = None
                    if hasattr(self, 'playwright'):
                        self.playwright = None
        except Exception:
            # Suppress any errors in destructor
            pass

    def test_regex(self, sample_html=None):
        """Unit test function to validate email and phone extraction patterns."""
        if sample_html is None:
            sample_html = """
            <p>Contact us at: contact@example.com, support@company.co.in</p>
            <p>Call us: +91 9876543210, 8765432109, 07654321098</p>
            <p>Email: info@domain.in or marketing@site.com</p>
            """
        
        print("=== REGEX PATTERN TEST ===")
        soup = BeautifulSoup(sample_html, 'html.parser')
        text = soup.get_text()
        
        print("Text sample:", text.strip()[:100] + "..." if len(text) > 100 else text.strip())
        print("\nEmail pattern:", self.email_pattern.pattern)
        emails = self.email_pattern.findall(text)
        print("Emails found:", emails)
        
        print("\nPhone pattern:", self.phone_pattern.pattern)
        phone_matches = self.phone_pattern.findall(text)
        print("Phone matches:", phone_matches)
        
        # Test the phone validation with source information
        print("\nTesting phone validation:")
        for phone_match in phone_matches:
            if isinstance(phone_match, tuple):
                for group in phone_match:
                    if group:
                        print(f"\nTesting: {group}")
                        valid_phone = self.validate_indian_phone(phone_match, "test_sample")
                        if valid_phone:
                            print(f"  âœ“ Valid: {valid_phone['phone']} (Source: {valid_phone['source']})")
                        else:
                            print(f"  âœ— Invalid")
            else:
                print(f"\nTesting: {phone_match}")
                valid_phone = self.validate_indian_phone(phone_match, "test_sample")
                if valid_phone:
                    print(f"  âœ“ Valid: {valid_phone['phone']} (Source: {valid_phone['source']})")
                else:
                    print(f"  âœ— Invalid")
        
        # Test using the improved validator directly
        print("\nTesting improved validator directly:")
        for phone_match in ["+91 9876543210", "8765432109", "07654321098"]:
            print(f"\nDirect test: {phone_match}")
            result = validate_indian_phone(phone_match, "test_sample")
            if result:
                print(f"  âœ“ Valid: {result['phone']} (Original: {result['original']}, Source: {result['source']})")
            else:
                print(f"  âœ— Invalid")

    def save_detailed_results_to_csv(self, keyword: str, results_by_url: List[Dict]):
        """Save detailed extraction results to a CSV file, including per-URL findings."""
        # Create directory if it doesn't exist
        os.makedirs('scraped_data', exist_ok=True)
        
        # Create a safe filename
        safe_keyword = re.sub(r'[^\w\s-]', '', keyword).strip().replace(' ', '_')
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        filename = f"scraped_data/{safe_keyword}_{timestamp}_detailed.csv"
        
        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
            fieldnames = ['url', 'domain', 'emails', 'phones', 'phone_sources', 'error']
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            
            for result in results_by_url:
                # Process phone numbers to extract source information
                phones_list = []
                sources_list = []
                
                for phone in result.get('phones', []):
                    if isinstance(phone, dict):
                        # If phone is in the new dictionary format
                        phones_list.append(phone.get('phone', ''))
                        sources_list.append(f"{phone.get('phone', '')}: {phone.get('source', 'unknown')}")
                    else:
                        # If phone is a string (legacy format)
                        phones_list.append(phone)
                        sources_list.append(f"{phone}: unknown")
                
                # Convert list fields to comma-separated strings
                row = {
                    'url': result.get('url', ''),
                    'domain': result.get('domain', ''),
                    'emails': ','.join(result.get('emails', [])),
                    'phones': ','.join(phones_list),
                    'phone_sources': '; '.join(sources_list),
                    'error': result.get('error', '')
                }
                writer.writerow(row)
            
        self.logger.info(f"Detailed results saved to {filename}")
        return filename
        
    def save_results_to_csv(self, keyword: str, emails: Set[str], phones: Set[str]):
        """Save extracted contacts to a CSV file with enhanced phone information."""
        # Create directory if it doesn't exist
        os.makedirs('scraped_data', exist_ok=True)
        
        # Create a safe filename
        safe_keyword = re.sub(r'[^\w\s-]', '', keyword).strip().replace(' ', '_')
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        filename = f"scraped_data/{safe_keyword}_{timestamp}_contacts.csv"
        
        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.writer(csvfile)
            # Updated header with source information
            writer.writerow(['Type', 'Contact', 'Original', 'Source'])
            
            # Write emails
            for email in emails:
                writer.writerow(['Email', email, '', ''])
            
            # Write phones with original format and source if available
            for phone in phones:
                if isinstance(phone, dict):
                    # If phone is already in the new dictionary format
                    writer.writerow(['Phone', phone.get('phone', ''), 
                                    phone.get('original', ''), 
                                    phone.get('source', 'unknown')])
                else:
                    # If phone is in the legacy string format
                    writer.writerow(['Phone', phone, '', ''])
                
        self.logger.info(f"Results saved to {filename}")
        return filename

    def scrape(self, keyword: str, num_results: int = 50, max_runtime_minutes: int = 15):
        """Main function to scrape contacts based on keyword with enforced runtime limits and strict data count limits.
        
        Args:
            keyword: Search keyword to find websites
            num_results: Target number of unique emails and phones to find
            max_runtime_minutes: Maximum runtime in minutes
            
        Returns:
            Dictionary with extracted contacts and metadata
        """
        # Sanitize and validate input
        keyword = keyword.strip()
        if not keyword:
            self.logger.error("Empty keyword provided")
            return {"error": "Empty keyword provided", "emails": [], "phones": []}
            
        # Validate num_results
        try:
            num_results = int(num_results)
            num_results = max(10, min(100, num_results))  # Limit between 10 and 100
        except (ValueError, TypeError):
            self.logger.warning(f"Invalid num_results: {num_results}, using default of 50")
            num_results = 50
            
        # Validate max_runtime_minutes
        try:
            max_runtime_minutes = int(max_runtime_minutes)
            max_runtime_minutes = max(5, min(60, max_runtime_minutes))  # Limit between 5 and 60
        except (ValueError, TypeError):
            self.logger.warning(f"Invalid max_runtime_minutes: {max_runtime_minutes}, using default of 15")
            max_runtime_minutes = 15
        
        start_time = time.time()
        max_runtime_seconds = max_runtime_minutes * 60
        
        # Set the target number of results to find
        self.target_results = num_results
        
        # Reset tracking sets for this scraping session
        self.found_emails = set()
        self.found_phones = set()
        
        self.logger.info(f"Starting scraping for keyword: '{keyword}', target results: {num_results}")
        print(f"\n{'='*60}\nStarting scraping for: '{keyword}'\n{'='*60}")
        print(f"Target: {num_results} unique emails and phone numbers")
        
        # Track all discovered items for live display and final results
        all_emails = set()
        all_phones = set()
        processed_urls = set()
        
        try:
            # Try to improve internet access if possible
            if platform.system() != 'Windows':  # Skip on Windows as it often fails
                self._setup_proton_vpn_connection()
            
            # Initialize browser if needed
            browser_initialized = False
            if self.use_browser:
                # We need to create a new event loop since this is a synchronous method
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                try:
                    print("ðŸŒ Initializing Playwright browser...")
                    browser_initialized = loop.run_until_complete(self.initialize_browser())
                    if not browser_initialized:
                        self.logger.warning("Browser initialization failed. Continuing without browser automation.")
                        print("âš ï¸ Browser initialization failed. Continuing without browser automation.")
                        self.use_browser = False
                except Exception as e:
                    self.logger.error(f"Browser initialization error: {e}")
                    print(f"âš ï¸ Browser initialization error. Continuing without browser.")
                    self.use_browser = False
                finally:
                    # Close the loop here to ensure resources are released
                    try:
                        loop.close()
                    except Exception as e:
                        self.logger.error(f"Error closing loop: {e}")
            
            # Break down long keywords into better search queries
            search_queries = [keyword]
            if len(keyword.split()) > 4:
                # For long queries, create additional, more specific sub-queries
                words = keyword.split()
                if len(words) > 6:
                    search_queries.append(" ".join(words[:4]))
                    search_queries.append(" ".join(words[-4:]))
                    
            all_urls = []
            search_engines_used = {'google': False, 'bing': False, 'duckduckgo': False}
            
            # Calculate how many URLs we should try to fetch based on target results
            initial_url_target = min(num_results * 3, 60)  # Start with 3x target, max 60 URLs
            
            for query in search_queries:
                self.logger.info(f"Searching for query: '{query}'")
                
                # Try to get enough URLs from Google first
                print(f"\nðŸ” Searching Google for: '{query}'...")
                try:
                    google_urls = self.search_google(query, min(initial_url_target, 30))
                    search_engines_used['google'] = True
                    
                    # Print discovered URLs in real-time
                    if google_urls:
                        print(f"\nâœ… Found {len(google_urls)} URLs from Google:")
                        for i, url in enumerate(google_urls, 1):
                            print(f"  {i}. {url}")
                    else:
                        print("âŒ No URLs found from Google")
                except Exception as e:
                    self.logger.error(f"Google search error: {e}")
                    print(f"âŒ Google search error: {str(e)[:100]}")
                    google_urls = []
                
                self.logger.info(f"Found {len(google_urls)} URLs from Google")
                all_urls.extend(google_urls)
                
                # Check if we've reached our initial URL target
                if len(all_urls) >= initial_url_target:
                    print(f"âœ… Found sufficient number of URLs ({len(all_urls)}) from Google")
                    break
                
                # Check if we're taking too long and need to move on
                if time.time() - start_time > max_runtime_seconds * 0.3:  # Use 30% of time max for initial search
                    self.logger.warning("30% of maximum runtime reached for Google search. Moving to next source.")
                    print("â±ï¸ 30% of runtime used for Google search. Moving to next search engine.")
                else:
                    # Try Bing to get more results
                    print(f"\nðŸ” Searching Bing for: '{query}'...")
                    try:
                        # Adjust number to fetch based on how many we already have
                        bing_target = initial_url_target - len(all_urls)
                        bing_urls = self.search_bing(query, min(bing_target, 30))
                        search_engines_used['bing'] = True
                        
                        # Print discovered URLs in real-time
                        if bing_urls:
                            print(f"\nâœ… Found {len(bing_urls)} URLs from Bing:")
                            for i, url in enumerate(bing_urls, 1):
                                print(f"  {i}. {url}")
                        else:
                            print("âŒ No URLs found from Bing")
                            
                        all_urls.extend(bing_urls)
                    except Exception as e:
                        self.logger.error(f"Bing search error: {e}")
                        print(f"âŒ Bing search error: {str(e)[:100]}")
                        bing_urls = []
                    
                    self.logger.info(f"Found {len(bing_urls)} URLs from Bing")
                    
                    # Check if we have enough URLs from Google and Bing combined
                    if len(all_urls) >= initial_url_target:
                        print(f"âœ… Found sufficient number of URLs ({len(all_urls)}) from Google and Bing")
                        break
                    
                    # If still need more, try DuckDuckGo
                    if len(all_urls) < initial_url_target and time.time() - start_time < max_runtime_seconds * 0.4:
                        print(f"\nðŸ” Searching DuckDuckGo for: '{query}'...")
                        try:
                            # Adjust number to fetch based on how many we already have
                            duckduckgo_target = initial_url_target - len(all_urls)
                            duckduckgo_urls = self.search_duckduckgo(query, min(duckduckgo_target, 20))
                            search_engines_used['duckduckgo'] = True
                            
                            # Print discovered URLs in real-time
                            if duckduckgo_urls:
                                print(f"\nâœ… Found {len(duckduckgo_urls)} URLs from DuckDuckGo:")
                                for i, url in enumerate(duckduckgo_urls, 1):
                                    print(f"  {i}. {url}")
                            else:
                                print("âŒ No URLs found from DuckDuckGo")
                                
                            all_urls.extend(duckduckgo_urls)
                        except Exception as e:
                            self.logger.error(f"DuckDuckGo search error: {e}")
                            print(f"âŒ DuckDuckGo search error: {str(e)[:100]}")
                            duckduckgo_urls = []
                        
                        self.logger.info(f"Found {len(duckduckgo_urls)} URLs from DuckDuckGo")
            
            # If still need more URLs, try a modified version of the query
            if len(all_urls) < initial_url_target * 0.5 and time.time() - start_time < max_runtime_seconds * 0.5:
                print(f"\nâš ï¸ Not enough URLs found. Trying with modified search queries...")
                
                # Create modified queries by adding relevant keywords
                modified_queries = []
                # Add industry-specific terms for better results
                industry_terms = ["company", "business", "services", "provider", "contact"]
                for term in industry_terms:
                    # Only add term if it's not already in the original query
                    if term.lower() not in keyword.lower():
                        modified_queries.append(f"{keyword} {term}")
                
                # Try each modified query with search engines we haven't fully used yet
                for mod_query in modified_queries[:2]:  # Limit to first 2 to save time
                    print(f"\nðŸ” Trying modified query: '{mod_query}'...")
                    
                    # Select which search engine to use for this query
                    # Prefer engines we haven't used yet or used less
                    if not search_engines_used['google']:
                        try:
                            print(f"ðŸ” Searching Google with modified query...")
                            mod_google_urls = self.search_google(mod_query, 15)
                            all_urls.extend(mod_google_urls)
                            search_engines_used['google'] = True
                            print(f"âœ… Found {len(mod_google_urls)} additional URLs from Google")
                        except Exception as e:
                            print(f"âŒ Modified Google search error: {str(e)[:100]}")
                    
                    # Only try Bing if we still need more URLs
                    if len(all_urls) < initial_url_target and not search_engines_used['bing']:
                        try:
                            print(f"ðŸ” Searching Bing with modified query...")
                            mod_bing_urls = self.search_bing(mod_query, 15)
                            all_urls.extend(mod_bing_urls)
                            search_engines_used['bing'] = True
                            print(f"âœ… Found {len(mod_bing_urls)} additional URLs from Bing")
                        except Exception as e:
                            print(f"âŒ Modified Bing search error: {str(e)[:100]}")
                    
                    # Check if we have enough URLs after modified queries
                    if len(all_urls) >= initial_url_target:
                        print(f"âœ… Found sufficient URLs with modified queries")
                        break
            
            # If we didn't find any URLs after all attempts, try the fallback method
            if not all_urls:
                print(f"\nâš ï¸ Search engines returned no results. Using alternative search method.")
                # Use direct requests to search engines instead of the existing methods which might be failing
                alternative_urls = self.extract_urls_from_search_query(keyword, initial_url_target)
                if alternative_urls:
                    print(f"âœ… Found {len(alternative_urls)} URLs using alternative search method.")
                    all_urls.extend(alternative_urls)
                else:
                    print("âŒ Failed to find any URLs for your search query.")
                    return {
                        'emails': [],
                        'phones': [],
                        'error': "No search results found. Please try a different search query.",
                        'runtime_minutes': (time.time() - start_time) / 60.0
                    }
            
            # Remove duplicates and filter out excluded domains
            unique_urls = []
            for url in set(all_urls):
                parsed_url = urlparse(url)
                domain = parsed_url.netloc
                # Skip if domain matches any of the excluded domains
                if not any(excluded in domain for excluded in self.excluded_domains):
                    unique_urls.append(url)
                else:
                    self.logger.info(f"Filtered out excluded domain: {domain}")
            
            self.logger.info(f"Total unique URLs to process (excluded domains filtered): {len(unique_urls)}")
            print(f"\nðŸŒ Total unique URLs to process (after filtering): {len(unique_urls)}")
            
            # Score and prioritize URLs by likelihood of containing contacts
            print("ðŸ“Š Analyzing URLs for contact potential...")
            scored_urls = []
            for url in unique_urls:
                likelihood = self.predict_contact_likelihood(url)
                scored_urls.append((url, likelihood))
            
            # Sort URLs by contact likelihood (highest first)
            scored_urls.sort(key=lambda x: x[1], reverse=True)
            prioritized_urls = [url for url, score in scored_urls]
            
            if len(prioritized_urls) > 0:
                print(f"âœ… Top 5 most promising URLs:")
                for i, (url, score) in enumerate(scored_urls[:5], 1):
                    print(f"  {i}. {url} (Score: {score:.2f})")
            
            # Check if we're over time limit
            elapsed_time = time.time() - start_time
            url_limit = len(prioritized_urls)  # Start with all URLs
            
            if elapsed_time > max_runtime_seconds * 0.5:
                # If over half the time is used in search, limit URLs to process
                url_limit = min(30, len(prioritized_urls))
                self.logger.warning(f"Runtime limit reached. Processing only {url_limit} URLs.")
                print(f"â±ï¸ Runtime limit reached. Processing only {url_limit} URLs.")
            else:
                # Otherwise, process a reasonable number based on time available
                remaining_time = max(0, max_runtime_seconds - elapsed_time)
                # Estimate how many URLs we can process with remaining time
                # (assume ~5 seconds per URL to be conservative)
                estimated_urls = max(20, int(remaining_time / 5))
                url_limit = min(estimated_urls, len(prioritized_urls))
                print(f"â±ï¸ Based on remaining time, processing up to {url_limit} URLs.")
            
            # Calculate remaining time and adjust processing parameters
            remaining_time = max(0, max_runtime_seconds - elapsed_time)
            if remaining_time < 120:  # Less than two minutes left
                self.min_delay = 0.5
                self.max_delay = 2.0
                self.logger.warning("Time constraints detected. Using faster processing.")
                print("â±ï¸ Time constraints detected. Using faster processing.")
                # Further limit URLs if time is very short
                url_limit = min(15, url_limit)
            
            # Prepare the prioritized URLs for processing, limited by url_limit
            prioritized_urls = prioritized_urls[:url_limit]
            
            # Extract contacts from URLs
            print("\nðŸ” Extracting contacts from URLs (this may take a few minutes)...")
            print(f"{'='*60}\n")
            
            # Process in smaller batches for more responsive progress updates
            batch_size = 5
            all_results = []
            processed_count = 0
            
            # Pre-extract the domains from the URLs for faster checking
            url_domains = {}
            for url in prioritized_urls:
                try:
                    domain = urlparse(url).netloc
                    url_domains[url] = domain
                except:
                    url_domains[url] = url
            
            # Create a copy of the list to iterate over
            urls_to_process = prioritized_urls.copy()
            
            # Continue processing until we meet targets or run out of URLs
            while urls_to_process and (len(all_emails) < num_results or len(all_phones) < num_results):
                # Take a batch of URLs
                batch_urls = urls_to_process[:batch_size]
                urls_to_process = urls_to_process[batch_size:]
                
                # Process each URL in the batch and display findings in real-time
                for url in batch_urls:
                    if url in processed_urls:
                        continue
                    
                    # Check if we've already reached the target number of results
                    if len(all_emails) >= num_results and len(all_phones) >= num_results:
                        self.logger.info(f"âœ… Target number of results reached: {num_results} emails and phones")
                        print(f"\nâœ… TARGET REACHED: Found {len(all_emails)} emails and {len(all_phones)} phones")
                        break
                    
                    # Check if we've reached our time limit
                    if time.time() - start_time > max_runtime_seconds * 0.9:
                        self.logger.warning("90% of maximum runtime reached. Stopping processing.")
                        print("â±ï¸ 90% of time limit reached. Stopping further processing.")
                        break
                    
                    # Skip URLs from excluded domains
                    domain = url_domains.get(url, urlparse(url).netloc)
                    if any(excluded in domain for excluded in self.excluded_domains):
                        self.logger.info(f"Skipping excluded domain: {domain}")
                        continue
                    
                    processed_urls.add(url)
                    
                    processed_count += 1
                    print(f"\nðŸ”— Processing URL {processed_count}/{len(prioritized_urls)}: {url}")
                    
                    # Extract contacts from this URL
                    try:
                        emails_found, phones_found = self.extract_contacts_from_url(url)
                        
                        # Log results for this URL
                        if emails_found or phones_found:
                            print(f"âœ… Found on {domain}:")
                            
                            # Display emails with clear formatting
                            if emails_found:
                                print("  ðŸ“§ EMAILS:")
                                for email in emails_found:
                                    print(f"    â€¢ {email}")
                                    all_emails.add(email)
                                    self.found_emails.add(email)
                            
                            # Display phones with clear formatting
                            if phones_found:
                                print("  ðŸ“± PHONES:")
                                for phone in phones_found:
                                    print(f"    â€¢ {phone}")
                                    all_phones.add(phone)
                                    self.found_phones.add(phone)
                        else:
                            print(f"â„¹ï¸ No contacts found on {domain}")
                        
                        # Add result to structured results
                        all_results.append({
                            'url': url,
                            'domain': domain,
                            'emails': list(emails_found),
                            'phones': list(phones_found)
                        })
                        
                        # Check if we've reached target after processing this URL
                        if len(all_emails) >= num_results and len(all_phones) >= num_results:
                            self.logger.info(f"âœ… Target number of results reached after processing {domain}")
                            print(f"âœ… Target reached after processing {domain}")
                            break
                            
                    except Exception as e:
                        print(f"âŒ Error processing {domain}: {str(e)}")
                        self.logger.error(f"Error processing {url}: {e}")
                        
                        # Add error result to keep track
                        all_results.append({
                            'url': url,
                            'domain': domain,
                            'error': str(e),
                            'emails': [],
                            'phones': []
                        })
                
                # Show progress summary after each batch
                print(f"\n{'='*60}")
                print(f"â±ï¸ PROGRESS: {processed_count}/{len(prioritized_urls)} URLs processed")
                print(f"ðŸ“Š TOTAL FOUND: {len(all_emails)} emails, {len(all_phones)} phones")
                if num_results > 0:
                    email_progress = min(100, int(len(all_emails) / num_results * 100))
                    phone_progress = min(100, int(len(all_phones) / num_results * 100))
                    print(f"ðŸ“ˆ TARGET PROGRESS: Emails: {email_progress}%, Phones: {phone_progress}%")
                print(f"{'='*60}\n")
                
                # Check if we've reached the target number of results
                if len(all_emails) >= num_results and len(all_phones) >= num_results:
                    self.logger.info(f"âœ… Target number of results reached: {num_results} emails and phones")
                    print(f"\nâœ… TARGET REACHED: Found {len(all_emails)} emails and {len(all_phones)} phones")
                    break
                
                # Check if we're running out of time
                if time.time() - start_time > max_runtime_seconds * 0.9:
                    print("â±ï¸ Time limit nearly reached. Stopping early.")
                    break
                
                # If we're about to run out of URLs and haven't reached targets, get more URLs
                if (not urls_to_process or len(urls_to_process) < 5) and len(all_emails) < num_results and len(all_phones) < num_results:
                    # Only get more URLs if we still have at least 35% of our runtime left
                    if time.time() - start_time < max_runtime_seconds * 0.65:
                        print("\nðŸ” Getting more URLs to reach target...")
                        
                        # Try a different search engine or variation of the query
                        additional_urls = []
                        
                        # Track which search engines we've already used
                        if not search_engines_used.get('duckduckgo', False):
                            print("ðŸ” Trying DuckDuckGo for additional URLs...")
                            try:
                                duck_urls = self.search_duckduckgo(keyword, 20)
                                additional_urls.extend(duck_urls)
                                search_engines_used['duckduckgo'] = True
                            except Exception as e:
                                self.logger.error(f"Error getting additional URLs from DuckDuckGo: {e}")
                        
                        # Try with a variation of the query
                        varied_query = f"{keyword} contact information"
                        if not search_engines_used.get('google', False):
                            print(f"ðŸ” Trying Google with varied query: '{varied_query}'...")
                            try:
                                google_urls = self.search_google(varied_query, 20)
                                additional_urls.extend(google_urls)
                                search_engines_used['google'] = True
                            except Exception as e:
                                self.logger.error(f"Error getting additional URLs from Google: {e}")
                        elif not search_engines_used.get('bing', False):
                            print(f"ðŸ” Trying Bing with varied query: '{varied_query}'...")
                            try:
                                bing_urls = self.search_bing(varied_query, 20)
                                additional_urls.extend(bing_urls)
                                search_engines_used['bing'] = True
                            except Exception as e:
                                self.logger.error(f"Error getting additional URLs from Bing: {e}")
                        
                        # Filter and add the new URLs
                        if additional_urls:
                            print(f"âœ… Found {len(additional_urls)} additional URLs")
                            # Remove duplicates and already processed URLs
                            filtered_urls = [
                                url for url in additional_urls 
                                if url not in processed_urls and not any(
                                    excluded in urlparse(url).netloc for excluded in self.excluded_domains
                                )
                            ]
                            
                            # Score and add to processing queue
                            if filtered_urls:
                                print(f"âœ… Adding {len(filtered_urls)} new URLs to processing queue")
                                new_scored_urls = []
                                for url in filtered_urls:
                                    new_scored_urls.append((url, self.predict_contact_likelihood(url)))
                                
                                # Sort by likelihood
                                new_scored_urls.sort(key=lambda x: x[1], reverse=True)
                                new_urls = [url for url, _ in new_scored_urls]
                                
                                # Add top 15 to our processing queue
                                top_new_urls = new_urls[:15]
                                urls_to_process.extend(top_new_urls)
                                
                                # Add domains to tracking dict
                                for url in top_new_urls:
                                    try:
                                        domain = urlparse(url).netloc
                                        url_domains[url] = domain
                                    except:
                                        url_domains[url] = url
                                
                                print(f"â±ï¸ Continuing processing with {len(urls_to_process)} URLs in queue")
            
            self.logger.info(f"Scraping completed. Found {len(all_emails)} unique emails and {len(all_phones)} unique Indian phone numbers")
            print(f"\n{'='*60}")
            print(f"âœ… SCRAPING COMPLETED")
            print(f"ðŸ“Š FOUND: {len(all_emails)} unique emails and {len(all_phones)} unique Indian phone numbers")
            print(f"{'='*60}")
            
            # Clean up resources
            if self.use_browser:
                print("â™»ï¸ Cleaning up browser resources...")
                # Create a new event loop for cleanup and make sure it gets closed properly
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                try:
                    loop.run_until_complete(self.close_browser())
                except Exception as e:
                    self.logger.error(f"Error during browser cleanup: {e}")
                finally:
                    try:
                        # Make sure to close the loop to release resources
                        loop.close()
                    except Exception as e:
                        self.logger.error(f"Error closing cleanup loop: {e}")
            
            # Save results
            print("ðŸ’¾ Saving results...")
            result_file = self.save_results_to_csv(keyword, all_emails, all_phones)
            
            # Also save detailed results with per-URL information
            detailed_file = self.save_detailed_results_to_csv(keyword, all_results)
            print(f"ðŸ“„ Summary results saved to: {result_file}")
            print(f"ðŸ“Š Detailed results saved to: {detailed_file}")
            
            # Calculate total runtime
            total_runtime = (time.time() - start_time) / 60.0
            self.logger.info(f"Total runtime: {total_runtime:.2f} minutes")
            print(f"â±ï¸ Total runtime: {total_runtime:.2f} minutes")
            
            return {
                'emails': list(all_emails),
                'phones': list(all_phones),
                'results_by_url': all_results,
                'result_file': result_file,
                'detailed_file': detailed_file,
                'runtime_minutes': total_runtime
            }
            
        except Exception as e:
            import traceback
            error_details = traceback.format_exc()
            self.logger.error(f"Error during scraping process: {e}\n{error_details}")
            print(f"âŒ Error during scraping: {e}")
            print("For more details, check the log file: scraper_log.txt")
            
            # Make sure we clean up even on error
            if self.use_browser:
                # Create a new event loop for cleanup
                cleanup_loop = asyncio.new_event_loop()
                asyncio.set_event_loop(cleanup_loop)
                try:
                    cleanup_loop.run_until_complete(self.close_browser())
                except Exception as cleanup_error:
                    self.logger.error(f"Error during cleanup: {cleanup_error}")
                finally:
                    try:
                        # Make sure to close the loop to release resources
                        cleanup_loop.close()
                    except Exception as loop_error:
                        self.logger.error(f"Error closing cleanup loop: {loop_error}")
            
            # Return whatever we found so far
            return {
                'emails': list(all_emails),
                'phones': list(all_phones),
                'error': str(e),
                'error_details': error_details
            }

    def extract_urls_from_search_query(self, keyword: str, num_results: int = 10) -> List[str]:
        """Alternative direct search method when the primary search methods fail.
        This method uses direct requests to search engines without browser automation.
        """
        print(f"ðŸ”„ Using alternative search method for '{keyword}'")
        urls = []
        
        try:
            # Try multiple search engines with direct requests
            search_engines = [
                {
                    "name": "Google",
                    "url": f"https://www.google.com/search?q={quote(keyword)}&num=30",
                    "headers": {
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
                        'Accept-Language': 'en-US,en;q=0.9,hi;q=0.8',
                        'Accept-Encoding': 'gzip, deflate, br',
                        'Connection': 'keep-alive',
                        'Referer': 'https://www.google.com/',
                        'Upgrade-Insecure-Requests': '1',
                        'sec-ch-ua': '"Not A(Brand";v="99", "Google Chrome";v="121", "Chromium";v="121"',
                        'sec-ch-ua-mobile': '?0',
                        'sec-ch-ua-platform': '"Windows"'
                    },
                    "link_patterns": [
                        {'tag': 'a', 'class_': None, 'attr': 'href', 'pattern': r'^https?://(?!www\.google\.).*$'},
                        {'tag': 'a', 'class_': None, 'attr': 'href', 'pattern': r'^/url\?q=(https?://.*?)&'}
                    ]
                },
                {
                    "name": "Bing",
                    "url": f"https://www.bing.com/search?q={quote(keyword)}&count=30",
                    "headers": {
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0',
                        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
                        'Accept-Language': 'en-US,en;q=0.9',
                        'Connection': 'keep-alive',
                        'Referer': 'https://www.bing.com/',
                        'sec-ch-ua': '"Microsoft Edge";v="120", "Chromium";v="120"',
                        'sec-ch-ua-mobile': '?0',
                        'sec-ch-ua-platform': '"Windows"'
                    },
                    "link_patterns": [
                        {'tag': 'a', 'class_': 'b_algo', 'attr': 'href', 'pattern': r'^https?://(?!www\.bing\.).*$'},
                        {'tag': 'li', 'class_': 'b_algo', 'attr': None, 'pattern': None}  # Look for a tags within this
                    ]
                },
                {
                    "name": "DuckDuckGo",
                    "url": f"https://html.duckduckgo.com/html/?q={quote(keyword)}",
                    "headers": {
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
                        'Accept-Language': 'en-US,en;q=0.9',
                        'Connection': 'keep-alive'
                    },
                    "link_patterns": [
                        {'tag': 'a', 'class_': 'result__a', 'attr': 'href', 'pattern': r'.*'},
                        {'tag': 'a', 'class_': 'result__url', 'attr': 'href', 'pattern': r'.*'}
                    ]
                }
            ]
            
            # Try each search engine
            for engine in search_engines:
                try:
                    print(f"ðŸ” Trying {engine['name']} direct search...")
                    
                    # Make the request with random proxy and retries
                    success = False
                    max_retries = 4
                    for attempt in range(max_retries):
                        try:
                            # Select a random proxy
                            proxy = self.get_random_proxy() if random.random() > 0.3 else None
                            
                            # Use different user agent for each attempt
                            if 'User-Agent' in engine['headers']:
                                engine['headers']['User-Agent'] = self.get_random_user_agent()
                            
                            # Make the request
                            response = requests.get(
                                engine["url"],
                                headers=engine["headers"],
                                proxies=proxy,
                                timeout=25,
                                allow_redirects=True
                            )
                            
                            if response.status_code == 200:
                                success = True
                                break
                            else:
                                print(f"âš ï¸ {engine['name']} returned status code {response.status_code}, trying again...")
                                time.sleep(random.uniform(1, 3))
                        except Exception as e:
                            print(f"âš ï¸ Error accessing {engine['name']}: {str(e)[:100]}")
                            time.sleep(random.uniform(1, 3))
                    
                    if not success:
                        print(f"âŒ Failed to get results from {engine['name']} after {max_retries} attempts")
                        continue
                        
                    # Parse the response
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # Extract links based on patterns
                    engine_urls = set()
                    
                    # Step 1: Try to extract based on defined patterns
                    for pattern in engine["link_patterns"]:
                        if pattern['tag'] and pattern['attr']:
                            # Direct extraction based on tag and optional class
                            elements = soup.find_all(pattern['tag'], class_=pattern['class_']) if pattern['class_'] else soup.find_all(pattern['tag'])
                            
                            for element in elements:
                                href = element.get(pattern['attr'])
                                if href:
                                    # Special case for Google redirect URLs
                                    if href.startswith('/url?q='):
                                        try:
                                            match = re.search(r'/url\?q=(https?://.*?)&', href)
                                            if match:
                                                href = match.group(1)
                                        except:
                                            continue
                                    # Special case for Yahoo redirect URLs
                                    elif '/RU=' in href:
                                        try:
                                            match = re.search(r'/RU=([^/]+)/', href)
                                            if match:
                                                href = match.group(1)
                                                href = unquote(href)
                                        except:
                                            continue
                                    # Special case for DuckDuckGo redirect URLs
                                    elif 'uddg=' in href:
                                        try:
                                            href = href.split('uddg=')[1].split('&')[0]
                                            href = unquote(href)
                                        except:
                                            continue
                                            
                                    # Ensure it's a genuine URL, not a search engine internal link
                                    if href.startswith('http') and not any(domain in href.lower() for domain in ['google.', 'bing.', 'yahoo.', 'duckduckgo.', 'microsoft.']):
                                        # Verify it's a proper URL
                                        try:
                                            parsed_url = urlparse(href)
                                            if parsed_url.netloc and '.' in parsed_url.netloc:
                                                # Check against excluded domains
                                                if not any(excluded in parsed_url.netloc for excluded in self.excluded_domains):
                                                    engine_urls.add(href)
                                        except:
                                            continue
                        elif pattern['tag'] and not pattern['attr']:
                            # Look for links within the container elements
                            containers = soup.find_all(pattern['tag'], class_=pattern['class_']) if pattern['class_'] else soup.find_all(pattern['tag'])
                            for container in containers:
                                links = container.find_all('a')
                                for link in links:
                                    href = link.get('href')
                                    if href and href.startswith('http') and not any(domain in href.lower() for domain in ['google.', 'bing.', 'yahoo.', 'duckduckgo.', 'microsoft.']):
                                        # Verify it's a proper URL
                                        try:
                                            parsed_url = urlparse(href)
                                            if parsed_url.netloc and '.' in parsed_url.netloc:
                                                # Check against excluded domains
                                                if not any(excluded in parsed_url.netloc for excluded in self.excluded_domains):
                                                    engine_urls.add(href)
                                        except:
                                            continue
                    
                    # Step 2: If specific patterns didn't yield results, try general link extraction
                    if not engine_urls:
                        print(f"âš ï¸ No URLs found using specific patterns, trying general extraction...")
                        all_links = soup.find_all('a')
                        for link in all_links:
                            href = link.get('href')
                            if href:
                                # Process redirects
                                if href.startswith('/url?') and 'q=' in href:
                                    try:
                                        match = re.search(r'/url\?q=(https?://.*?)&', href)
                                        if match:
                                            href = match.group(1)
                                    except:
                                        continue
                                elif 'uddg=' in href:
                                    try:
                                        href = href.split('uddg=')[1].split('&')[0]
                                        href = unquote(href)
                                    except:
                                        continue
                                        
                                # Ensure it's a genuine URL with proper domain
                                if href.startswith('http'):
                                    try:
                                        parsed_url = urlparse(href)
                                        if parsed_url.netloc and '.' in parsed_url.netloc:
                                            # Skip search engine domains
                                            if not any(domain in parsed_url.netloc.lower() for domain in ['google.', 'bing.', 'yahoo.', 'duckduckgo.', 'microsoft.']):
                                                # Check against excluded domains
                                                if not any(excluded in parsed_url.netloc for excluded in self.excluded_domains):
                                                    engine_urls.add(href)
                                    except:
                                        continue
                    
                    # Add the new URLs to our combined list
                    if engine_urls:
                        new_urls = list(engine_urls)
                        urls.extend(new_urls)
                        print(f"âœ… Found {len(new_urls)} URLs from {engine['name']}")
                        
                        # Print some examples of URLs found
                        if len(new_urls) > 0:
                            print("Examples of URLs found:")
                            for example_url in new_urls[:3]:
                                print(f"  â€¢ {example_url}")
                    else:
                        print(f"âš ï¸ No URLs found from {engine['name']}")
                    
                    # If we have enough URLs, we can stop
                    if len(urls) >= num_results * 2:  # Get extra URLs for better filtering
                        break
                        
                except Exception as e:
                    self.logger.error(f"Error searching with {engine['name']}: {e}")
                    print(f"âŒ Error with {engine['name']}: {str(e)[:100]}")
            
            # Make sure we have unique URLs
            unique_urls = list(set(urls))
            
            # Filter URLs based on relevance to the keyword
            if len(unique_urls) > num_results:
                filtered_urls = self._filter_urls_by_relevance(unique_urls, keyword)
                print(f"âœ… Filtered to {len(filtered_urls)} relevant URLs from alternative search methods")
                return filtered_urls[:num_results]
            else:
                print(f"âœ… Returning all {len(unique_urls)} URLs from alternative search methods")
                return unique_urls
                
        except Exception as e:
            self.logger.error(f"Error in alternative URL extraction: {e}")
            print(f"âŒ Error in alternative URL extraction: {str(e)[:100]}")
            return []
    
    # NOTE: The URL pattern generation methods have been removed to ensure only genuine URLs from search engines are used.

    def _filter_urls_by_relevance(self, urls: List[str], keyword: str) -> List[str]:
        """Filter URLs based on their relevance to the keyword."""
        if not urls:
            return []
            
        # Convert keyword to lowercase and split into words
        keyword_lower = keyword.lower()
        keyword_words = set(keyword_lower.split())
        
        # Remove common words
        common_words = {'in', 'at', 'of', 'the', 'and', 'or', 'for', 'with', 'to', 'from'}
        keyword_words = keyword_words - common_words
        
        # Store URLs with their relevance score
        url_scores = []
        
        for url in urls:
            try:
                score = 0
                parsed_url = urlparse(url)
                domain = parsed_url.netloc.lower()
                path = parsed_url.path.lower()
                
                # Award points for keywords in domain
                for word in keyword_words:
                    if word in domain:
                        score += 3  # High relevance for domain match
                
                # Award points for keywords in path
                for word in keyword_words:
                    if word in path:
                        score += 1  # Medium relevance for path match
                
                # Award points for business/contact-related patterns in URL
                business_patterns = ['contact', 'about', 'team', 'company', 'business', 
                                    'services', 'portfolio', 'clients']
                for pattern in business_patterns:
                    if pattern in domain or pattern in path:
                        score += 2  # Higher relevance for business-related pages
                
                # Add to scored list
                url_scores.append((url, score))
                
            except Exception:
                # Skip URLs that can't be parsed
                continue
        
        # Sort by score (descending)
        url_scores.sort(key=lambda x: x[1], reverse=True)
        
        # Return the URLs in order of relevance
        return [url for url, score in url_scores]
    
    # NOTE: The URL pattern generation methods have been removed to ensure only genuine URLs from search engines are used.

    def predict_contact_likelihood(self, url: str) -> float:
        """Predict the likelihood of finding contact info on a URL based on patterns.
        
        Returns:
            Float between 0.0 and 1.0 indicating likelihood
        """
        try:
            likelihood = 0.5  # Start with neutral value
            parsed_url = urlparse(url)
            domain = parsed_url.netloc.lower()
            path = parsed_url.path.lower()
            
            # Check for known contact patterns in URLs
            contact_patterns = ['contact', 'about', 'team', 'staff', 'people', 'company', 
                               'connect', 'reach', 'touch', 'info', 'support']
            
            # Increase likelihood for contact pages
            for pattern in contact_patterns:
                if pattern in path:
                    likelihood += 0.2
                    break  # One match is enough
            
            # Check domain for business-like patterns
            business_patterns = ['solutions', 'services', 'consulting', 'digital', 
                                'tech', 'software', 'agency', 'consultancy', 'firm']
            
            for pattern in business_patterns:
                if pattern in domain:
                    likelihood += 0.1
                    break  # One match is enough
            
            # Check for very short domains (often personal sites with less contact info)
            if len(domain.split('.')[0]) <= 4:  # Excluding www and TLD
                likelihood -= 0.1
            
            # Check for blog platforms (less likely to have business contact info)
            blog_platforms = ['blog', 'wordpress', 'medium', 'blogger', 'blogspot', 'tumblr']
            for platform in blog_platforms:
                if platform in domain:
                    likelihood -= 0.2
                    break
            
            # Check for social networks (unlikely to find direct contacts)
            social_networks = ['facebook', 'linkedin', 'twitter', 'instagram', 'youtube']
            for network in social_networks:
                if network in domain:
                    likelihood -= 0.3
                    break
                    
            # Prefer root or contact pages, not deep links
            path_segments = [s for s in path.split('/') if s]
            if len(path_segments) > 2:
                likelihood -= 0.1 * (len(path_segments) - 2)
            
            # Cap between 0.0 and 1.0
            return max(0.0, min(1.0, likelihood))
            
        except Exception:
            # Default to medium likelihood for unparseable URLs
            return 0.5


def main():
    """Command line interface for the scraper."""
    try:
        # Try to import colorama for cross-platform color support
        from colorama import init, Fore, Style
        init()  # Initialize colorama
        
        # Define color functions
        def green(text): return f"{Fore.GREEN}{text}{Style.RESET_ALL}"
        def red(text): return f"{Fore.RED}{text}{Style.RESET_ALL}"
        def yellow(text): return f"{Fore.YELLOW}{text}{Style.RESET_ALL}"
        def blue(text): return f"{Fore.BLUE}{text}{Style.RESET_ALL}"
        def cyan(text): return f"{Fore.CYAN}{text}{Style.RESET_ALL}"
        def magenta(text): return f"{Fore.MAGENTA}{text}{Style.RESET_ALL}"
        def bold(text): return f"{Style.BRIGHT}{text}{Style.RESET_ALL}"
        
    except ImportError:
        # Fallback if colorama is not installed
        def green(text): return text
        def red(text): return text
        def yellow(text): return text
        def blue(text): return text
        def cyan(text): return text
        def magenta(text): return text
        def bold(text): return text
        print("For colored output, install colorama: pip install colorama")
    
    print(bold("=" * 75))
    print(bold(cyan("ðŸ“± ADVANCED CONTACT SCRAPER ðŸ“§")))
    print(bold(cyan("Extract Emails and Indian Phone Numbers from Search Results")))
    print(bold("=" * 75))
    
    # Add debugging option
    run_debug = input(yellow("Run in debug mode? (y/n): ")).lower() == 'y'
    
    if run_debug:
        print("\n" + bold(cyan("ðŸ§ª Running in debug mode with predefined test sites...")))
        # Create a scraper instance
        scraper = ContactScraper(use_browser=True, debug_mode=True)
        
        # Test with direct URLs
        test_urls = [
            "https://www.digitalmarketingdelhi.in/",
            "https://www.socialbeat.in/", 
            "https://digitalready.co/",
            "https://www.webchutney.com/contact",
            "https://www.techmagnate.com/contact-us.html"
        ]
        
        print(bold(cyan("ðŸ” TESTING WITH KNOWN URLS")))
        
        all_emails = set()
        all_phones = set()
        
        for url in test_urls:
            try:
                print(f"\n{bold(yellow('TESTING URL:'))} {url}")
                emails, phones = scraper.extract_contacts_from_url(url)
                
                if emails or phones:
                    if emails:
                        print(f"{green('âœ… Emails found:')}")
                        for email in emails:
                            print(f"  ðŸ“§ {email}")
                            all_emails.add(email)
                            
                    if phones:
                        print(f"{green('âœ… Phones found:')}")
                        for phone in phones:
                            print(f"  ðŸ“± {phone}")
                            all_phones.add(phone)
                else:
                    print(f"{yellow('âš ï¸ No contacts found')}")
            except Exception as e:
                print(f"{red(f'âŒ Error: {str(e)[:100]}')}")
        
        print(f"\n{bold(cyan('FINAL RESULTS:'))}")
        print(f"{green(f'Total emails found: {len(all_emails)}')}")
        print(f"{green(f'Total phones found: {len(all_phones)}')}")
        
        # Test regex patterns directly
        print("\n" + bold(cyan("ðŸ§ª TESTING REGEX PATTERNS")))
        
        # Test email regex
        test_emails = [
            "contact@example.com",
            "support@company.co.in",
            "info@domain.in",
            "user (at) domain.com",
            "name@domain.com.ph",
            "email-with+symbol@domain.org",
            "first.last@subdomain.domain.co.in"
        ]
        
        print(f"\n{bold('Testing Email Pattern:')}")
        for test_email in test_emails:
            emails = scraper._extract_emails_from_text(test_email)
            if emails:
                print(f"  âœ… {test_email} => {', '.join(emails)}")
            else:
                print(f"  âŒ {test_email} => Not matched")
        
        # Test phone regex
        test_phones = [
            "+91 9876543210",
            "9876543210",
            "09876543210",
            "+91-98765-43210",
            "98765 43210",
            "0987-654-3210",
            "(+91) 98765 43210",
            "9876 543 210"
        ]
        
        print(f"\n{bold('Testing Phone Pattern:')}")
        for test_phone in test_phones:
            phones = scraper._extract_phones_from_text(test_phone)
            if phones:
                print(f"  âœ… {test_phone} => {', '.join(phones)}")
            else:
                print(f"  âŒ {test_phone} => Not matched")
        
        # Save results
        if all_emails or all_phones:
            result_file = scraper.save_results_to_csv("debug_test", all_emails, all_phones)
            print(f"{blue(f'Results saved to: {result_file}')}")
        
        return # Exit here if in debug mode
    
    # Ask if user wants to run tests first
    run_test = input(yellow("Run tests first? (y/n): ")).lower() == 'y'
    
    if run_test:
        print("\n" + bold(cyan("ðŸ§ª Running tests...")))
        with ContactScraper(use_browser=True) as scraper:
            # Test regex patterns
            scraper.test_regex()
            
            # Ask user if they want to run test extraction
            test_extraction = input("\n" + yellow("Run test extraction from sample URLs? (y/n): ")).lower() == 'y'
            if test_extraction:
                # Let user provide test URLs
                custom_urls = input(yellow("Enter comma-separated test URLs (or press Enter for default): "))
                if custom_urls.strip():
                    test_urls = [url.strip() for url in custom_urls.split(",")]
                else:
                    test_urls = None
                    
                scraper.test_extraction(test_urls)
            
            print("\n" + green("âœ… Tests completed"))
    
    # Main scraping process
    run_scraper = input("\n" + yellow("Run main scraper? (y/n): ")).lower() == 'y'
    if not run_scraper:
        print(blue("Exiting..."))
        return
        
    keyword = input(yellow("Enter search keyword: "))
    
    try:
        num_results = int(input(yellow("Enter number of results to scrape (10-100): ")))
        num_results = max(10, min(100, num_results))  # Limit between 10 and 100
    except ValueError:
        print(red("Invalid number, using default of 50"))
        num_results = 50
    
    # Ask if browser automation should be used
    use_browser = input(yellow("Use browser automation for better results? (y/n): ")).lower() == 'y'
    
    # Ask for maximum runtime
    try:
        max_runtime = int(input(yellow("Maximum runtime in minutes (5-60): ")))
        max_runtime = max(5, min(60, max_runtime))  # Limit between 5 and 60 minutes
    except ValueError:
        print(red("Invalid runtime, using default of 15 minutes"))
        max_runtime = 15
    
    print("\n" + bold(cyan("ðŸš€ Starting scraper...")))
    print(bold("=" * 75))
    print(blue(f"âš™ï¸ SETTINGS"))
    print(blue(f"ðŸ” Search query: ") + bold(cyan(f"'{keyword}'")))
    print(blue(f"ðŸ”¢ Max results: ") + bold(cyan(f"{num_results}")))
    print(blue(f"â±ï¸ Max runtime: ") + bold(cyan(f"{max_runtime} minutes")))
    print(blue(f"ðŸŒ Browser automation: ") + bold(green("Enabled") if use_browser else bold(red("Disabled"))))
    print(bold("=" * 75))
    print(magenta("ðŸ“Š You'll see progress updates as the scraper runs."))
    print()
    
    # Use context manager to ensure proper cleanup
    with ContactScraper(use_browser=use_browser) as scraper:
        try:
            # Run the synchronous scrape method
            results = scraper.scrape(keyword, num_results, max_runtime)
            
            print("\n" + bold(cyan("ðŸ“Š SCRAPING SUMMARY")))
            print(bold("=" * 75))
            
            if 'error' in results:
                print(red(f"âŒ Error during scraping: {results['error']}"))
            
            emails_count = len(results.get('emails', []))
            phones_count = len(results.get('phones', []))
            
            print(green(f"âœ… Emails found: {bold(str(emails_count))}"))
            print(green(f"âœ… Indian phone numbers found: {bold(str(phones_count))}"))
            
            if 'result_file' in results:
                print(blue(f"ðŸ“„ Results saved to: {cyan(results['result_file'])}"))
            
            if 'runtime_minutes' in results:
                runtime = f"{results['runtime_minutes']:.2f} minutes"
                print(blue(f"â±ï¸ Total runtime: {cyan(runtime)}"))
                
            # Show sample of results if available
            if results.get('emails'):
                print("\n" + bold(cyan("ðŸ“§ SAMPLE EMAILS FOUND:")))
                for email in list(results['emails'])[:5]:
                    print(f"  â€¢ {green(email)}")
                if len(results['emails']) > 5:
                    print(f"  {yellow('... and ' + str(len(results['emails']) - 5) + ' more')}")
                    
            if results.get('phones'):
                print("\n" + bold(cyan("ðŸ“± SAMPLE PHONE NUMBERS FOUND:")))
                for phone in list(results['phones'])[:5]:
                    print(f"  â€¢ {green(phone)}")
                if len(results['phones']) > 5:
                    print(f"  {yellow('... and ' + str(len(results['phones']) - 5) + ' more')}")
            
            print(bold("=" * 75))
                    
        except KeyboardInterrupt:
            print("\n" + yellow("âš ï¸ Scraping interrupted by user."))
            print(blue("ðŸ’¾ Saving any results found so far..."))
            try:
                # Try to save partial results
                emails = getattr(scraper, 'all_emails', set())
                phones = getattr(scraper, 'all_phones', set())
                if emails or phones:
                    result_file = scraper.save_results_to_csv(keyword, emails, phones)
                    print(green(f"âœ… Partial results saved to: {result_file}"))
                else:
                    print(yellow("âš ï¸ No results to save."))
            except Exception as e:
                print(red(f"âŒ Could not save partial results: {e}"))
                
        except Exception as e:
            print("\n" + red(f"âŒ Error during scraping: {e}"))
            
    print("\n" + bold(green("âœ… Process completed.")))


if __name__ == "__main__":
    main()

