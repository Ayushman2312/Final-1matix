import requests
from datetime import datetime, timedelta
from bs4 import BeautifulSoup, Comment
import re
import random
import time
import csv
from urllib.parse import quote, urlparse, urljoin, unquote
import os
import socket
import threading
from typing import List, Dict, Tuple, Optional, Set, Union
import logging
from fake_useragent import UserAgent
import json
from playwright.async_api import Browser, Page, Playwright, TimeoutError as PlaywrightTimeoutError
import tldextract
import warnings
import asyncio
import sys
import platform
import traceback
from collections import defaultdict, deque
# Import our improved validation functions
try:
    # Try relative import first (when used as a package)
    from .improved_validators import validate_email, validate_indian_phone
except ImportError:
    # Fall back to absolute import (when run as a script)
    from improved_validators import validate_email, validate_indian_phone

# Fix for Windows asyncio pipe ResourceWarning issues
if platform.system() == 'Windows':
    # Silence the resource warnings that occur in Windows with asyncio
    import warnings
    warnings.filterwarnings("ignore", category=ResourceWarning, message="unclosed.*")
    
    # Replace the default event loop for Windows
    if sys.version_info >= (3, 8):
        # For Python 3.8+, use WindowsProactorEventLoopPolicy
        # This provides better compatibility with Playwright and subprocess operations
        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())
        # Create a new proactor event loop
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
    else:
        # For older Python versions
        asyncio.set_event_loop(asyncio.ProactorEventLoop())

# Global event loop management to prevent event loop conflicts
_GLOBAL_EVENT_LOOP = None

# Advanced rate limiting and request throttling
class RateLimiter:
    """
    Advanced rate limiter to prevent getting blocked by domains
    Implements dynamic delay, exponential backoff, and success rate monitoring
    """
    
    def __init__(self):
        # Track request times per domain
        self.request_times = {}
        # Track recent success/failure per domain
        self.success_record = {}
        # Default minimum delay between requests to the same domain (seconds)
        self.default_min_delay = 2.0
        # Special domains with specific rate limits
        self.domain_specific_delays = {
            'google.com': 30.0,  # Strict limit for Google
            'google.co.in': 30.0,
            'bing.com': 15.0,
            'duckduckgo.com': 10.0,
            'linkedin.com': 60.0,
            'instagram.com': 45.0,
            'facebook.com': 45.0
        }
        # Track 429 responses per domain
        self.rate_limited = {}
        # Maximum backoff in seconds
        self.max_backoff = 300  # 5 minutes
        # Track errors per domain for exponential backoff
        self.error_count = {}
        # Jitter range (seconds) to add to delays
        self.jitter_range = (0.5, 3.0)
        # Maximum parallel requests to the same domain
        self.max_parallel_requests = 1
        # Currently active requests per domain
        self.active_requests = {}
        # Lock for thread safety
        self.lock = threading.RLock()

    def should_delay_request(self, domain):
        """
        Determines if a request should be delayed based on previous requests
        
        Args:
            domain (str): The domain to check
            
        Returns:
            tuple: (should_delay, recommended_delay_seconds)
        """
        with self.lock:
            # Extract base domain
            base_domain = self._extract_base_domain(domain)
            
            # Check if domain is rate limited
            if base_domain in self.rate_limited:
                last_limited, backoff_time = self.rate_limited[base_domain]
                # If we received a 429 within the last backoff period, enforce delay
                if datetime.now() < last_limited + timedelta(seconds=backoff_time):
                    remaining_time = (last_limited + timedelta(seconds=backoff_time) - datetime.now()).total_seconds()
                    return True, max(remaining_time, 30)  # At least 30 seconds
            
            # Check if we've exceeded parallel request limit
            if self.active_requests.get(base_domain, 0) >= self.max_parallel_requests:
                return True, 5.0  # 5 second delay when too many parallel requests
            
            # Calculate required delay based on previous requests
            required_delay = self.calculate_required_delay(base_domain)
            
            # Check if we need to delay
            last_request_time = self.request_times.get(base_domain, [datetime.now() - timedelta(hours=1)])[-1]
            time_since_last_request = (datetime.now() - last_request_time).total_seconds()
            
            if time_since_last_request < required_delay:
                return True, required_delay - time_since_last_request
            
            return False, 0

    def calculate_required_delay(self, domain):
        """
        Calculate the required delay for a domain based on recent history
        
        Args:
            domain (str): Domain to calculate delay for
            
        Returns:
            float: Recommended delay in seconds
        """
        base_domain = self._extract_base_domain(domain)
        
        # Start with the default or domain-specific delay
        base_delay = self.domain_specific_delays.get(base_domain, self.default_min_delay)
        
        # Apply exponential backoff based on error count
        error_count = self.error_count.get(base_domain, 0)
        if error_count > 0:
            # Exponential backoff: base_delay * 2^error_count
            backoff_multiplier = min(2 ** error_count, self.max_backoff / base_delay)
            base_delay *= backoff_multiplier
        
        # Add jitter to prevent synchronized patterns
        jitter = random.uniform(self.jitter_range[0], self.jitter_range[1])
        
        # Adjust delay based on success rate
        success_rate = self.get_success_rate(base_domain)
        if success_rate < 0.5:  # Below 50% success rate
            rate_multiplier = 1 + (0.5 - success_rate) * 4  # Up to 3x slower when success rate is low
            base_delay *= rate_multiplier
        
        # Cap the maximum delay
        return min(base_delay + jitter, self.max_backoff)
    
    def _extract_base_domain(self, domain):
        """Extract the base domain from a full domain name"""
        if not domain:
            return "unknown"
        
        # Handle common search engines specially to ensure proper rate limiting
        if 'google' in domain:
            return 'google.com'
        if 'bing' in domain:
            return 'bing.com'
        if 'duckduckgo' in domain:
            return 'duckduckgo.com'
        
        # Extract base domain using tldextract
        try:
            extracted = tldextract.extract(domain)
            return f"{extracted.domain}.{extracted.suffix}"
        except:
            # Fallback for parsing errors
            parts = domain.split('.')
            if len(parts) >= 2:
                return '.'.join(parts[-2:])
            return domain

    def record_request(self, domain):
        """Record that a request is being made to a domain"""
        with self.lock:
            base_domain = self._extract_base_domain(domain)
            
            # Record request time
            if base_domain not in self.request_times:
                self.request_times[base_domain] = []
            
            # Keep only recent request times (last 24 hours)
            cutoff = datetime.now() - timedelta(hours=24)
            self.request_times[base_domain] = [
                t for t in self.request_times[base_domain] if t > cutoff
            ]
            
            # Add current request time
            self.request_times[base_domain].append(datetime.now())
            
            # Increment active requests counter
            self.active_requests[base_domain] = self.active_requests.get(base_domain, 0) + 1

    def record_success(self, domain):
        """Record a successful request to a domain"""
        with self.lock:
            base_domain = self._extract_base_domain(domain)
            
            # Update success record
            if base_domain not in self.success_record:
                self.success_record[base_domain] = {'success': 0, 'failure': 0}
            
            self.success_record[base_domain]['success'] += 1
            
            # Reduce error count when successful
            if base_domain in self.error_count and self.error_count[base_domain] > 0:
                self.error_count[base_domain] = max(0, self.error_count[base_domain] - 0.5)
            
            # Remove rate limit marking if present
            if base_domain in self.rate_limited:
                del self.rate_limited[base_domain]
            
            # Decrement active requests counter
            if base_domain in self.active_requests:
                self.active_requests[base_domain] = max(0, self.active_requests[base_domain] - 1)

    def record_error(self, domain, status_code=None):
        """
        Record a failed request to a domain
        
        Args:
            domain (str): The domain that returned an error
            status_code (int, optional): HTTP status code if available
        """
        with self.lock:
            base_domain = self._extract_base_domain(domain)
            
            # Update failure record
            if base_domain not in self.success_record:
                self.success_record[base_domain] = {'success': 0, 'failure': 0}
            
            self.success_record[base_domain]['failure'] += 1
            
            # Increment error count for backoff calculation
            if base_domain not in self.error_count:
                self.error_count[base_domain] = 0
            
            # More severe increment for rate limiting
            if status_code == 429:  # Too Many Requests
                self.error_count[base_domain] += 2
                
                # Record rate limiting
                backoff_time = min(60 * (2 ** self.error_count[base_domain]), self.max_backoff)
                self.rate_limited[base_domain] = (datetime.now(), backoff_time)
                
                logging.warning(f"âš ï¸ Rate limited by {base_domain} (429 Too Many Requests). Backing off for {backoff_time:.1f} seconds")
            elif status_code and status_code >= 400:
                # Less severe for other error codes
                self.error_count[base_domain] += 1
            else:
                # Generic error (no status code)
                self.error_count[base_domain] += 0.5
            
            # Decrement active requests counter
            if base_domain in self.active_requests:
                self.active_requests[base_domain] = max(0, self.active_requests[base_domain] - 1)

    def get_success_rate(self, domain=None):
        """
        Get the success rate for a domain or overall
        
        Args:
            domain (str, optional): Domain to get success rate for
            
        Returns:
            float: Success rate between 0 and 1
        """
        with self.lock:
            if domain:
                base_domain = self._extract_base_domain(domain)
                record = self.success_record.get(base_domain, {'success': 0, 'failure': 0})
                total = record['success'] + record['failure']
                return record['success'] / total if total > 0 else 1.0
            else:
                # Calculate overall success rate
                total_success = sum(r['success'] for r in self.success_record.values())
                total_failure = sum(r['failure'] for r in self.success_record.values())
                total = total_success + total_failure
                return total_success / total if total > 0 else 1.0

    def adaptive_delay(self, domain, importance=1.0):
        """
        Apply an adaptive delay for the domain based on history
        
        Args:
            domain (str): Domain to delay for
            importance (float): Importance factor (lower means longer delay is acceptable)
        """
        should_delay, delay_time = self.should_delay_request(domain)
        
        if should_delay:
            # Adjust delay based on importance (lower importance can wait longer)
            adjusted_delay = delay_time / max(0.1, importance)
            
            # Add some randomness
            adjusted_delay *= random.uniform(0.8, 1.2)
            
            # Apply the delay
            time.sleep(adjusted_delay)
        
        # Record this request
        self.record_request(domain)

def get_or_create_event_loop():
    """Get the current event loop or create a new one if needed.
    
    This function ensures we reuse the same event loop where possible
    to avoid issues with asyncio.
    
    Returns:
        asyncio.AbstractEventLoop: The event loop to use
    """
    global _GLOBAL_EVENT_LOOP
    
    # If we already have a global event loop that's still usable, return it
    if _GLOBAL_EVENT_LOOP is not None and not _GLOBAL_EVENT_LOOP.is_closed():
        return _GLOBAL_EVENT_LOOP
    
    # If we're on Windows, always set the ProactorEventLoop policy
    if platform.system() == 'Windows':
        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())
    
    try:
        # Try to get the current event loop
        loop = asyncio.get_event_loop()
        if loop.is_closed():
            raise RuntimeError("Event loop is closed")
        _GLOBAL_EVENT_LOOP = loop
        return loop
    except RuntimeError:
        # Create a new event loop if needed
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        _GLOBAL_EVENT_LOOP = loop
        return loop

def extract_json_ld(html_content: str) -> List[Dict]:
    """
    Extract structured data from JSON-LD script tags in the HTML content.
    
    Args:
        html_content: HTML content to parse
        
    Returns:
        List of dictionaries containing the JSON-LD data
    """
    results = []
    try:
        soup = BeautifulSoup(html_content, 'html.parser')
        json_ld_scripts = soup.find_all('script', type='application/ld+json')
        
        for script in json_ld_scripts:
            try:
                # Get the script content
                json_text = script.string
                if not json_text:
                    continue
                    
                # Parse the JSON
                data = json.loads(json_text)
                
                # Handle case where the JSON-LD is a list
                if isinstance(data, list):
                    results.extend(data)
                # Handle case where the JSON-LD is a single object
                else:
                    results.append(data)
            except json.JSONDecodeError:
                continue
            except Exception:
                continue
    except Exception:
        pass
        
    return results


class ContactScraper:
    def __init__(self, use_browser=True, debug_mode=False):
        # Set up logging
        self.logger = logging.getLogger('ContactScraper')
        self.logger.setLevel(logging.DEBUG if debug_mode else logging.INFO)
        
        # Add a file handler
        if not self.logger.handlers:
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            
            # Add console handler
            ch = logging.StreamHandler()
            ch.setFormatter(formatter)
            ch.setLevel(logging.WARNING)  # Only warnings and above to console
            self.logger.addHandler(ch)
            
            # Add file handler
            try:
                # Create logs directory if it doesn't exist
                os.makedirs('scraper_logs', exist_ok=True)
                # Create a timestamped log file
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                fh = logging.FileHandler(f'scraper_logs/scraper_{timestamp}.log')
                fh.setFormatter(formatter)
                self.logger.addHandler(fh)
            except Exception as e:
                print(f"Warning: Could not set up file logging: {e}")
        
        self.debug_mode = debug_mode
        self.use_browser = use_browser
        
        # Initialize rate limiter for advanced request throttling
        self.rate_limiter = RateLimiter()
        
        # Initialize counters
        self.captcha_detected_domains = set()
        self.blocked_domains = set()
        self.successful_domains = set()
        
        # Initialize browser components
        self.browser = None
        self.context = None
        self.page = None
        self.browser_initialized = False
        
        # Initialize default proxy configuration
        self.proxy_list = [None]  # Start with direct connection
        self.current_proxy_index = 0
        
        # Expanded list of domains that should not be scraped (for ethical considerations)
        self.excluded_domains = [
            # Marketplaces and B2B platforms
            'indiamart.com', 'm.indiamart.com', 'dir.indiamart.com',
            'tradeindia.com', 'www.tradeindia.com',
            'exportersindia.com', 'www.exportersindia.com',
            'justdial.com', 'digitalinternationalintermesh.com',
            'wholesalebox.in', 'alibaba.com', 'thomasnet.com',
            'yellowpages.com', 'sulekha.com', 'shopify.com',
            'ebay.com', 'ebay.in', 'amazon.com', 'amazon.in', 'flipkart.com',
            'myntra.com', 'snapdeal.com', 'paytmmall.com', 
            
            # Social networks and sharing sites
            'facebook.com', 'instagram.com', 'twitter.com', 'linkedin.com',
            'youtube.com', 'pinterest.com', 'tumblr.com', 'reddit.com',
            'quora.com', 'medium.com', 'blogger.com', 'blogspot.com',
            'wordpress.com', 'tiktok.com', 'whatsapp.com', 't.me',
            'telegram.org', 'telegram.me', 'flickr.com', 'imgur.com',
            'slideshare.net', 
            
            # Search engines and news sites
            'google.com', 'google.co.in', 'bing.com', 'yahoo.com',
            'duckduckgo.com', 'baidu.com', 'yandex.com', 'ask.com',
            'timesofindia.indiatimes.com', 'ndtv.com', 'news18.com',
            'hindustantimes.com', 'thehindu.com', 'indianexpress.com',
            
            # Job sites
            'naukri.com', 'monster.com', 'indeed.com', 'glassdoor.com',
            'shine.com', 'timesjobs.com', 'linkedin.com/jobs',
            
            # Web services, cloud platforms, and analytics
            'googleapis.com', 'gstatic.com', 'googleusercontent.com',
            'ggpht.com', 'doubleclick.net', 'googleadservices.com',
            'google-analytics.com', 'googlesyndication.com', 'cloudflare.com',
            'cloudfront.net', 'amazonaws.com', 'akamaihd.net', 'analytics.google.com',
            'mailchimp.com', 'freshdesk.com', 'zendesk.com', 'azure.com',
            'salesforce.com', 'hubspot.com', 'github.com', 'gitlab.com',
            'cdn.com', 'jsdelivr.net', 'jquery.com', 'w3.org',
            
            # Government and educational domains
            'gov.in', 'edu', 'nic.in', 'ac.in', 'res.in', '.edu',
            'wikipedia.org', 'wikimedia.org', 'wikibooks.org',
            
            # File sharing and document sites
            'drive.google.com', 'docs.google.com', 'sheets.google.com',
            'forms.google.com', 'calendar.google.com', 'dropbox.com',
            'onedrive.live.com', 'box.com', 'mediafire.com', 'slideshare.net',
            'scribd.com', 'docstoc.com', 'issuu.com', 'academia.edu',
            'researchgate.net',
            
            # Payment gateways and banking
            'paytm.com', 'paypal.com', 'razorpay.com', 'instamojo.com', 
            'billdesk.com', 'phonepe.com', 'gpay.app', 'sbi.co.in',
            'icicibank.com', 'hdfcbank.com', 'axisbank.com', 'pnbindia.in',
            
            # Tracking and ad networks
            'taboola.com', 'outbrain.com', 'adnxs.com', 'moatads.com',
            'criteo.com', 'scorecardresearch.com', 'adjust.com', 'appsflyer.com',
            'branch.io', 'amplitude.com', 'segment.com', 'ga.js',
            'gtag.js', 'gtm.js', 'fbpixel', 'facebook.net', 'flurry.com',
            'omniture.com', 'adroll.com', 'adform.net', 'rubiconproject.com',
            'pubmatic.com', 'casalemedia.com'
        ]
        self.logger.info(f"Configured {len(self.excluded_domains)} excluded domains for ethical scraping")
        
        # Additional checks for URL filtering
        self.tracking_param_patterns = [
            'utm_', 'gclid', 'fbclid', 'msclkid', 'dclid', 'zanpid', 
            'ref', 'referrer', 'source', 'ref_src', 'ref_url', 'cmpid',
            'pf_rd_', '_hsenc', '_hsmi', 'vero_id', 'mc_eid', 'hsa_', 
            'oly_enc_id', 'oly_anon_id', '_openstat', 'wickedid', 
            'mkt_tok', 'trk', 'igshid'
        ]
        
        # File extension patterns that should be excluded
        self.excluded_file_extensions = [
            '.jpg', '.jpeg', '.png', '.gif', '.bmp', '.svg', '.webp', '.ico',
            '.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx',
            '.zip', '.rar', '.tar', '.gz', '.7z', '.exe', '.dll', '.apk',
            '.mp3', '.mp4', '.avi', '.mkv', '.mov', '.flv', '.wmv',
            '.css', '.js', '.xml', '.json', '.rss', '.atom'
        ]
        
        # Try to use fake_useragent for more realistic user agents
        try:
            self.ua = UserAgent()
            self.logger.info("Using fake-useragent for more realistic user agents")
        except Exception as e:
            self.logger.warning(f"Could not initialize fake-useragent: {e}. Using fallback user agents.")
            self.ua = None
        
        # Fallback user agents with Indian locale hints
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:97.0) Gecko/20100101 Firefox/97.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.3 Safari/605.1.15',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36 Edg/98.0.1108.56'
        ]
        
        # Cookie and session management
        self.session = requests.Session()
        self.cookies = {}
        
        # Playwright configuration
        self.use_browser = use_browser
        self.browser = None
        self.browser_context = None
        self.page = None
        self.browser_initialized = False
        self.playwright = None
        
        # Royal Residential Proxies configuration
        self.proxy_host = 'geo.iproyal.com'
        self.proxy_port = 11200
        self.proxy_username = 'vnkl9BGvMRlmvWfO'
        self.proxy_password = 'EjFoKHcjcchVYwZ9'
        
        # Configure proxies with a larger pool, focusing on Indian IPs
        self.proxy_list = []
        
        # Add primary proxy with Indian country targeting
        self.proxy_list.append({
            'http': f'http://{self.proxy_username}:{self.proxy_password}@{self.proxy_host}:{self.proxy_port}',
            'https': f'http://{self.proxy_username}:{self.proxy_password}@{self.proxy_host}:{self.proxy_port}'
        })
        
        # Add residential proxies with different country codes but prioritize India
        countries = ['in', 'in', 'in', 'us', 'uk', 'sg']  # Multiple 'in' entries to increase probability
        for country_code in countries:
            self.proxy_list.append({
                'http': f'http://{self.proxy_username}:{self.proxy_password}@{self.proxy_host}:{self.proxy_port}',
                'https': f'http://{self.proxy_username}:{self.proxy_password}@{self.proxy_host}:{self.proxy_port}'
            })
        
        # Add proxy with rotating IPs (no country specified)
        self.proxy_list.append({
            'http': f'http://{self.proxy_username}:{self.proxy_password}@{self.proxy_host}:{self.proxy_port}',
            'https': f'http://{self.proxy_username}:{self.proxy_password}@{self.proxy_host}:{self.proxy_port}'
        })
        
        # Add fallback to direct connection (no proxy)
        self.proxy_list.append(None)
        
        # Track proxy performance
        self.proxy_success_count = {}  # Count successful requests per proxy
        self.proxy_failure_count = {}  # Count failed requests per proxy
        self.consecutive_failures = 0  # Track consecutive failures for current proxy
        
        # Proxy rotation settings
        self.max_consecutive_failures = 3  # After this many failures, rotate proxy
        self.rotation_counter = 0  # Count how many times we've rotated
        
        # Regex patterns specifically for Indian phone numbers and emails
        # UPDATED: More comprehensive phone patterns to match various formats
        # Including number with country code, without country code, and with separators
        # This covers formats like:
        # +91 9876543210, +91-9876543210, 09876543210, 9876543210, 98765-43210, etc.
        self.phone_pattern = re.compile(r'''
            (?:
                # Format: +91 followed by 10 digits with optional separators
                (?:\+91[\s\-.]?)?(?:[6789]\d{9})
                |
                # Format: 10 digits with optional separators in between
                (?:[6789]\d{2,4}[\s\-.]?\d{2,4}[\s\-.]?\d{2,4})
                |
                # Format: Leading 0 followed by 10 digits with optional separators
                (?:0[6789]\d{1,2}[\s\-.]?\d{3,4}[\s\-.]?\d{3,4})
                |
                # Short format: 5 digits - 5 digits for some business numbers
                (?:[6789]\d{4}[\s\-.]?\d{5})
                |
                # STD Code followed by landline (e.g., 022-12345678)
                (?:0\d{2,4}[\s\-.]?\d{6,8})
                |
                # International format with country code and STD code
                (?:\+?91[\s\-.]?\d{2,4}[\s\-.]?\d{6,8})
                |
                # 8-digit landline without STD code
                (?:[2345]\d{7})
                |
                # Toll-free numbers
                (?:1(?:800|900|860)[\s\-.]?\d{3}[\s\-.]?\d{4})
                |
                # 5-digit special numbers (short codes)
                (?:\b\d{5}\b)
            )
        ''', re.VERBOSE)
        
        # Add a secondary pattern for more phone formats
        self.phone_pattern_alt = re.compile(r'''
            (?:
                # Common patterns with brackets
                (?:\(?\+91\)?[\s\-.]?)?(?:\(?\d{2,5}\)?[\s\-.]?\d{5,8})
                |
                # General 10-digit format (with or without country code)
                (?:(?:\+\d{1,2}[\s\-.]?)?\d{10})
                |
                # Parentheses format with STD code (e.g., (022) 12345678)
                (?:\(\d{2,4}\)[\s\-.]?\d{6,8})
                |
                # Formats with periods as separators
                (?:\+?91\.[\s\-]?\d{2,4}\.[\s\-]?\d{6,8})
                |
                # Formats with parentheses for country code
                (?:\(\+?91\)[\s\-.]?\d{2,4}[\s\-.]?\d{6,8})
                |
                # International format with + but without 91 (e.g., +22 12345678) - for businesses
                (?:\+\d{2}[\s\-.]?\d{8,10})
                |
                # Format with ISD+STD code commonly used by businesses 
                (?:00[\s\-.]?91[\s\-.]?\d{2,4}[\s\-.]?\d{6,8})
            )
        ''', re.VERBOSE)
        
        # UPDATED: More comprehensive email pattern
        # Matches common email formats while avoiding common false positives
        self.email_pattern = re.compile(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}')
        
        # Additional pattern for email extraction from obfuscated text
        self.obfuscated_email_pattern = re.compile(r'([a-zA-Z0-9._%+-]+)\s*(?:[\[\(]?\s*at\s*[\]\)]?|\[@\]|&#64;|@)\s*([a-zA-Z0-9.-]+)(?:[\[\(]?\s*(?:dot|\.)\s*[\]\)]?|\.|\s*\.)([a-zA-Z]{2,})')
        
        # Additional patterns for Indian domains and specific formats
        self.indian_domain_pattern = re.compile(r'\.in$|\.co\.in$|\.org\.in$|\.net\.in$')
        
        # Random request delay ranges
        self.min_delay = 2.0  # Minimum delay between requests in seconds
        self.max_delay = 10.0  # Maximum delay between requests in seconds
        
        # Anti-blocking measures
        self.recent_domains = set()  # Track recently accessed domains for rate limiting
        self.domain_access_times = {}  # Track access times per domain
        self.domain_min_interval = 20  # Minimum seconds between accessing same domain
        self.max_requests_per_domain = 3  # Max requests per domain in a session
        self.domain_request_count = {}  # Counter for requests per domain
        
        # If a domain blocks us, remember it
        self.blocked_domains = set()
        self.captcha_detected_domains = set()
        
        # Create directory for results if not exists
        os.makedirs('scraped_data', exist_ok=True)
        
        # Playwright lock to ensure only one browser operation at a time
        self.browser_lock = asyncio.Lock()
        
        # Debug mode flag
        self.debug_mode = debug_mode
        
        # Install required packages if missing
        self._ensure_dependencies()
        
        # Results tracking
        self.target_results = 0
        self.found_emails = set()
        self.found_phones = set()
        
    def _ensure_dependencies(self):
        """Ensure all required dependencies are installed."""
        try:
            print("Checking dependencies...")
            
            # Check for requests package
            try:
                import requests
                print("âœ… Requests library found")
            except ImportError:
                print("âŒ Requests library not found, attempting to install...")
                import subprocess
                import sys
                subprocess.run([sys.executable, "-m", "pip", "install", "requests"], check=True)
                print("âœ… Requests library installed")
            
            # Check for BeautifulSoup
            try:
                from bs4 import BeautifulSoup
                print("âœ… BeautifulSoup library found")
            except ImportError:
                print("âŒ BeautifulSoup library not found, attempting to install...")
                import subprocess
                import sys
                subprocess.run([sys.executable, "-m", "pip", "install", "beautifulsoup4"], check=True)
                print("âœ… BeautifulSoup library installed")
            
            # Check for Playwright
            try:
                # First check if module is importable
                import playwright
                try:
                    version = playwright.__version__
                    print(f"âœ… Playwright package found (version {version})")
                except AttributeError:
                    print("âœ… Playwright package found (version not available)")
                
                # Try importing the Playwright browser launch module
                try:
                    from playwright.async_api import async_playwright
                    print("âœ… Playwright async API available")
                except ImportError:
                    print("âŒ Playwright async API not available, reinstalling...")
                    import subprocess
                    import sys
                    subprocess.run([sys.executable, "-m", "pip", "install", "playwright"], check=True)
                    print("âœ… Playwright reinstalled")
                
                # Check if browser is installed
                import subprocess
                import sys
                
                print("Checking Playwright browser installation...")
                # Run the command to see if browsers are installed
                result = subprocess.run(
                    [sys.executable, "-m", "playwright", "install", "--help"],
                    capture_output=True,
                    text=True
                )
                
                if result.returncode != 0:
                    print("âŒ Playwright browser installation check failed")
                    print("Installing Playwright browsers...")
                    subprocess.run([sys.executable, "-m", "pip", "install", "playwright"], check=True)
                    subprocess.run([sys.executable, "-m", "playwright", "install"], check=True)
                    print("âœ… Playwright browsers installed")
                else:
                    print("âœ… Playwright browser installation available")
                    
                # Install the browser if not already
                print("Installing Playwright browsers...")
                try:
                    subprocess.run([sys.executable, "-m", "playwright", "install", "chromium"], check=True)
                    print("âœ… Chromium browser installed")
                except subprocess.CalledProcessError:
                    print("âš ï¸ Chromium browser installation failed, may already be installed")
                
                # Install dependencies
                print("Installing browser dependencies...")
                try:
                    subprocess.run([sys.executable, "-m", "playwright", "install-deps", "chromium"], check=True)
                    print("âœ… Browser dependencies installed")
                except subprocess.CalledProcessError:
                    print("âš ï¸ Browser dependencies installation failed")
                
                return True
                
            except ImportError:
                print("âŒ Playwright not found, attempting to install...")
                import subprocess
                import sys
                subprocess.run([sys.executable, "-m", "pip", "install", "playwright"], check=True)
                print("âœ… Playwright installed")
                # Also install the browser
                try:
                    subprocess.run([sys.executable, "-m", "playwright", "install", "chromium"], check=True)
                    print("âœ… Chromium browser installed")
                except subprocess.CalledProcessError:
                    print("âš ï¸ Failed to install Chromium browser")
                    return False
                    
                # Install dependencies
                try:
                    subprocess.run([sys.executable, "-m", "playwright", "install-deps", "chromium"], check=True)
                    print("âœ… Browser dependencies installed")
                except subprocess.CalledProcessError:
                    print("âš ï¸ Failed to install browser dependencies")
                    
                return True
            
            return True
            
        except Exception as e:
            print(f"âŒ Error ensuring dependencies: {e}")
            return False
    
    def _setup_proton_vpn_connection(self):
        """Setup ProtonVPN connection for more reliable access (if available)."""
        try:
            # Check if protonvpn-cli is installed
            import subprocess
            result = subprocess.run(['which', 'protonvpn-cli'], 
                                   stdout=subprocess.PIPE, 
                                   stderr=subprocess.PIPE)
            
            if result.returncode == 0:
                print("ProtonVPN CLI found, attempting to connect...")
                # Try to connect to ProtonVPN
                try:
                    # Connect to fastest server
                    connect_result = subprocess.run(['protonvpn-cli', 'connect', '-f'], 
                                                  stdout=subprocess.PIPE, 
                                                  stderr=subprocess.PIPE,
                                                  timeout=30)
                    
                    if connect_result.returncode == 0:
                        print("âœ… Connected to ProtonVPN")
                        return True
                    else:
                        print("âŒ Failed to connect to ProtonVPN")
                except Exception as e:
                    print(f"Error connecting to ProtonVPN: {e}")
        except Exception:
            # ProtonVPN not available, continue without it
            pass
        
        return False
            
    async def initialize_browser(self):
        """Initialize the Playwright browser with proper configuration for all operating systems."""
        if self.browser_initialized and self.browser and self.page:
            print("âœ… Browser already initialized")
            return True
            
        # Print detailed initialization status
        print("ðŸ”„ Initializing browser for web scraping...")
        
        # Track initialization time
        start_time = time.time()
        
        # Ensure dependencies are installed
        if not self._ensure_dependencies():
            print("âŒ Failed to ensure dependencies")
            return False
            
        try:
            # Import Playwright modules
            from playwright.async_api import async_playwright
            
            # Fix for Windows: Ensure we're using the proper event loop policy
            # This is handled by our global get_or_create_event_loop helper
            
            # Initialize Playwright
            self.playwright = await async_playwright().start()
            print("âœ… Playwright initialized")
            
            # Use a country-specific proxy for better results (especially for Indian searches)
            proxy_config = None
            try:
                proxy = self.get_random_proxy()
                if proxy and 'http' in proxy:
                    # Format proxy URL properly for Playwright
                    proxy_str = proxy['http']
                    if proxy_str.startswith('http://'):
                        proxy_str = proxy_str[7:]
                    
                    # Extract username/password if present
                    username = None
                    password = None
                    if '@' in proxy_str:
                        auth, proxy_str = proxy_str.split('@', 1)
                        if ':' in auth:
                            username, password = auth.split(':', 1)
                    
                    # Configure proxy
                    proxy_config = {
                        "server": f"http://{proxy_str}",
                    }
                    
                    # Add credentials if available
                    if username and password:
                        proxy_config["username"] = username
                        proxy_config["password"] = password
                    
                    print(f"ðŸŒ Using proxy: {proxy_str}")
            except Exception as e:
                print(f"âš ï¸ Proxy setup failed, continuing without proxy: {e}")
                proxy_config = None
            
            # Set basic browser parameters with minimal settings for stability
            browser_args = [
                '--no-sandbox',
                '--disable-dev-shm-usage'
            ]
            
            # Launch with minimal configuration for maximum compatibility
            try:
                print("ðŸš€ Launching browser...")
                launch_options = {
                    "headless": True,
                    "args": browser_args,
                    "timeout": 30000  # 30 second timeout
                }
                
                # Add proxy if available
                if proxy_config:
                    launch_options["proxy"] = proxy_config
                
                # Print launch options for debugging
                print(f"Launch options: {launch_options}")
                
                # Launch browser
                self.browser = await self.playwright.chromium.launch(**launch_options)
                print("âœ… Browser launched successfully")
            except Exception as e:
                print(f"âŒ Browser launch failed: {str(e)}")
                # Try with a truly minimal configuration as last resort
                try:
                    print("ðŸ”„ Trying with minimal browser configuration...")
                    self.browser = await self.playwright.chromium.launch(headless=True)
                    print("âœ… Browser launched with minimal configuration")
                except Exception as e2:
                    print(f"âŒ Minimal browser launch also failed: {str(e2)}")
                    await self._cleanup_browser_resources()
                    self.use_browser = False
                    return False
            
            # Create a browser context with minimal settings
            try:
                print("ðŸ” Creating browser context...")
                self.browser_context = await self.browser.new_context()
                print("âœ… Browser context created")
            except Exception as e:
                print(f"âŒ Failed to create browser context: {str(e)}")
                await self._cleanup_browser_resources()
                self.use_browser = False
                return False
            
            # Create a new page
            try:
                print("ðŸ“„ Creating browser page...")
                self.page = await self.browser_context.new_page()
                print("âœ… Browser page created")
            except Exception as e:
                print(f"âŒ Failed to create browser page: {str(e)}")
                await self._cleanup_browser_resources()
                self.use_browser = False
                return False
            
            # Test the browser with a simple navigation to make sure it works
            try:
                print("ðŸŒ Testing browser with navigation...")
                await self.page.goto("https://example.com", timeout=30000)
                print("âœ… Browser test completed successfully")
            except Exception as e:
                print(f"âŒ Browser test failed: {str(e)}")
                await self._cleanup_browser_resources()
                self.use_browser = False
                return False
            
            # Success - mark as initialized
            self.browser_initialized = True
            elapsed_time = time.time() - start_time
            print(f"âœ… Browser initialized successfully in {elapsed_time:.1f} seconds")
            return True
            
        except Exception as e:
            import traceback
            print(f"âŒ Failed to initialize browser: {str(e)}")
            print("Error details:")
            traceback.print_exc()
            
            # Try to clean up any resources that were created
            try:
                await self._cleanup_browser_resources()
            except Exception as cleanup_error:
                self.logger.error(f"Error cleaning up browser resources: {cleanup_error}")
            
            # If we get here, browser initialization failed
            self.browser_initialized = False
            self.use_browser = False
            
            # Disable browser for this session
            print("âŒ Browser initialization failed. Disabling browser for this session.")
            return False
    
    async def _cleanup_browser_resources(self):
        """Clean up browser resources safely with better error handling."""
        # Close the page
        if self.page:
            self.logger.info("Closing browser page...")
            try:
                await self.page.close()
                print("âœ… Browser page closed successfully")
            except Exception as e:
                self.logger.warning(f"Error closing page: {e}")
                print(f"âš ï¸ Error closing browser page: {e}")
            self.page = None
        
        # Close the browser context
        if self.browser_context:
            self.logger.info("Closing browser context...")
            try:
                await self.browser_context.close()
                print("âœ… Browser context closed successfully")
            except Exception as e:
                self.logger.warning(f"Error closing browser context: {e}")
                print(f"âš ï¸ Error closing browser context: {e}")
            self.browser_context = None
        
        # Close the browser
        if self.browser:
            self.logger.info("Closing browser...")
            try:
                await self.browser.close()
                print("âœ… Browser closed successfully")
            except Exception as e:
                self.logger.warning(f"Error closing browser: {e}")
                print(f"âš ï¸ Error closing browser: {e}")
            self.browser = None
        
        # Stop playwright
        if self.playwright:
            self.logger.info("Stopping playwright...")
            try:
                await self.playwright.stop()
                print("âœ… Playwright stopped successfully")
            except Exception as e:
                self.logger.warning(f"Error stopping playwright: {e}")
                print(f"âš ï¸ Error stopping playwright: {e}")
            self.playwright = None
        
        # Set initialization flag to false
        self.browser_initialized = False
        self.logger.info("Browser resources cleanup completed")
    
    async def _apply_stealth_settings(self, page=None):
        """Apply advanced stealth settings to avoid detection and prevent captchas.
        
        Args:
            page: Optional page object to apply stealth settings to.
                 If None, applies to browser_context.
        """
        # If page is provided, apply stealth settings to that page
        if page:
            # Apply stealth settings to the specific page
            await page.add_init_script("""
            () => {
                // Override navigator properties to hide automation
                Object.defineProperty(navigator, 'webdriver', {
                    get: () => false
                });
                
                // Disable the Automation object
                delete navigator.__proto__.webdriver;
                
                // Add random languages with Indian locale hints
                const languages = [
                    ['en-IN', 'en', 'hi'],
                    ['en-US', 'en', 'hi', 'te'],
                    ['en-IN', 'en', 'ta'],
                    ['en-GB', 'en', 'hi', 'mr']
                ];
                const randomLanguages = languages[Math.floor(Math.random() * languages.length)];
                Object.defineProperty(navigator, 'languages', {
                    get: () => randomLanguages
                });
                
                // Spoof permissions API - critical for avoiding detection
                if (navigator.permissions) {
                    const originalQuery = navigator.permissions.query;
                    navigator.permissions.query = parameters => {
                        if (parameters.name === 'notifications') {
                            return Promise.resolve({ state: Notification.permission });
                        }
                        
                        // Pretend all requested permissions are "prompt" for natural behavior
                        if (['clipboard-read', 'clipboard-write', 'camera', 'microphone'].includes(parameters.name)) {
                            return Promise.resolve({ state: 'prompt' });
                        }
                        
                        return originalQuery(parameters);
                    };
                }
            }
            """)
            return
        
        if not self.browser_context:
            return
            
        # Apply stealth settings to the browser context (original implementation)
        await self.browser_context.add_init_script("""
        () => {
            // ====== HARDWARE FINGERPRINT RANDOMIZATION ======
            // Randomize hardware concurrency (CPU cores) with a realistic value
            const cpuCores = [2, 4, 6, 8, 12, 16];
            const randomCores = cpuCores[Math.floor(Math.random() * cpuCores.length)];
            Object.defineProperty(navigator, 'hardwareConcurrency', {
                get: () => randomCores
            });
            
            // Randomize device memory with a realistic value (in GB)
            const deviceMemory = [2, 4, 8, 16];
            const randomMemory = deviceMemory[Math.floor(Math.random() * deviceMemory.length)];
            Object.defineProperty(navigator, 'deviceMemory', {
                get: () => randomMemory
            });
            
            // ====== BROWSER IDENTITY OBFUSCATION ======
            // Override navigator properties to hide automation
            Object.defineProperty(navigator, 'webdriver', {
                get: () => false
            });
            
            // Disable the Automation object
            delete navigator.__proto__.webdriver;
            
            // Make navigator.plugins and navigator.mimeTypes non-empty
            const makeFakePluginArray = () => {
                const plugins = [
                    { name: 'Chrome PDF Plugin', filename: 'internal-pdf-viewer', description: 'Portable Document Format' },
                    { name: 'Chrome PDF Viewer', filename: 'mhjfbmdgcfjbbpaeojofohoefgiehjai', description: 'Portable Document Format' },
                    { name: 'Native Client', filename: 'internal-nacl-plugin', description: 'Native Client Executable' },
                    { name: 'Microsoft Edge PDF Plugin', filename: 'internal-pdf-viewer', description: 'Portable Document Format' }
                ];
                
                const pluginArray = Object.create(PluginArray.prototype);
                Object.defineProperties(pluginArray, {
                    length: { value: plugins.length },
                    item: { value: index => plugins[index] },
                    namedItem: { value: name => plugins.find(p => p.name === name) },
                    refresh: { value: () => {} }
                });
                
                // Add numerical indices
                plugins.forEach((plugin, i) => {
                    pluginArray[i] = plugin;
                });
                
                return pluginArray;
            };
            
            if (navigator.plugins.length === 0) {
                Object.defineProperty(navigator, 'plugins', {
                    get: () => makeFakePluginArray()
                });
            }
            
            // Add random languages with Indian locale hints
            const languages = [
                ['en-IN', 'en', 'hi'],
                ['en-US', 'en', 'hi', 'te'],
                ['en-IN', 'en', 'ta'],
                ['en-GB', 'en', 'hi', 'mr']
            ];
            const randomLanguages = languages[Math.floor(Math.random() * languages.length)];
            Object.defineProperty(navigator, 'languages', {
                get: () => randomLanguages
            });
            
            // ====== PERMISSIONS API SPOOFING ======
            // Spoof permissions API - critical for avoiding detection
            if (navigator.permissions) {
                const originalQuery = navigator.permissions.query;
                navigator.permissions.query = parameters => {
                    if (parameters.name === 'notifications') {
                        return Promise.resolve({ state: Notification.permission });
                    }
                    
                    // Pretend all requested permissions are "prompt" for natural behavior
                    if (['clipboard-read', 'clipboard-write', 'camera', 'microphone'].includes(parameters.name)) {
                        return Promise.resolve({ state: 'prompt' });
                    }
                    
                    return originalQuery(parameters);
                };
            }
            
            // ====== PREVENT CANVAS FINGERPRINTING ======
            // Subtle canvas noise to prevent fingerprinting
            if (CanvasRenderingContext2D.prototype.getImageData) {
                const originalGetImageData = CanvasRenderingContext2D.prototype.getImageData;
                CanvasRenderingContext2D.prototype.getImageData = function(x, y, w, h) {
                    const imageData = originalGetImageData.call(this, x, y, w, h);
                    
                    // Add subtle noise to canvas data to prevent fingerprinting
                    if (imageData && imageData.data && imageData.data.length > 0) {
                        // Make the same seeds for each load to avoid detection of randomization
                        const canvasSeed = (x + 1) * (y + 1) * w * h % 233;
                        const data = imageData.data;
                        
                        for (let i = 0; i < data.length; i += 200) {
                            // Very subtle changes based on deterministic seed
                            const pixelSeed = (i * canvasSeed) % 7;
                            if (pixelSeed === 3) { // Only modify some pixels
                                const offset = 1;
                                if (data[i] + offset <= 255) data[i] += offset;
                                if (i+1 < data.length && data[i+1] + offset <= 255) data[i+1] += offset;
                            }
                        }
                    }
                    return imageData;
                };
            }
            
            // ====== BROWSER WINDOW PROPERTIES ======
            // Fix window.outerdimensions to realistic values
            if (window.outerWidth === 0) {
                Object.defineProperty(window, 'outerWidth', {
                    get: () => window.innerWidth
                });
            }
            if (window.outerHeight === 0) {
                Object.defineProperty(window, 'outerHeight', {
                    get: () => window.innerHeight + 74 // Header/toolbar height
                });
            }
            
            // ====== CHROME-SPECIFIC DETECTION PREVENTION ======
            // Fill in missing Chrome-specific properties
            if (!window.chrome) {
                window.chrome = {
                    app: {
                        isInstalled: false,
                        InstallState: { DISABLED: 'disabled', INSTALLED: 'installed', NOT_INSTALLED: 'not_installed' },
                        RunningState: { CANNOT_RUN: 'cannot_run', READY_TO_RUN: 'ready_to_run', RUNNING: 'running' }
                    },
                    runtime: {
                        OnInstalledReason: {
                            INSTALL: 'install',
                            UPDATE: 'update',
                            CHROME_UPDATE: 'chrome_update',
                            SHARED_MODULE_UPDATE: 'shared_module_update'
                        },
                        OnRestartRequiredReason: {
                            APP_UPDATE: 'app_update',
                            OS_UPDATE: 'os_update',
                            PERIODIC: 'periodic'
                        },
                        PlatformArch: {
                            ARM: 'arm',
                            ARM64: 'arm64',
                            MIPS: 'mips',
                            MIPS64: 'mips64',
                            X86_32: 'x86-32',
                            X86_64: 'x86-64'
                        },
                        PlatformNaclArch: {
                            ARM: 'arm',
                            MIPS: 'mips',
                            MIPS64: 'mips64',
                            X86_32: 'x86-32',
                            X86_64: 'x86-64'
                        },
                        PlatformOs: {
                            ANDROID: 'android',
                            CROS: 'cros',
                            LINUX: 'linux',
                            MAC: 'mac',
                            OPENBSD: 'openbsd',
                            WIN: 'win'
                        },
                        RequestUpdateCheckStatus: {
                            THROTTLED: 'throttled',
                            NO_UPDATE: 'no_update',
                            UPDATE_AVAILABLE: 'update_available'
                        }
                    }
                };
            }
            
            // Override window.Notification to avoid hanging permissions
            if (window.Notification) {
                const originalNotification = window.Notification;
                window.Notification = {
                    permission: 'default',
                    requestPermission: () => Promise.resolve('default')
                };
                for (const key in originalNotification) {
                    if (key !== 'permission' && key !== 'requestPermission') {
                        window.Notification[key] = originalNotification[key];
                    }
                }
            }
            
            // ====== TIMING ATTACK PREVENTION ======
            // Randomize performance values slightly to prevent timing-based detection
            if (window.performance && window.performance.now) {
                const originalNow = window.performance.now;
                let lastNow = 0;
                window.performance.now = function() {
                    const perfectNow = originalNow.call(window.performance);
                    // Add tiny random variance while ensuring time always increases
                    const variance = Math.random() * 0.01;
                    lastNow = Math.max(lastNow + variance, perfectNow);
                    return lastNow;
                };
            }
            
            // ====== RANDOM BEHAVIOR SIMULATION ======
            // Simulate human-like initial behavior
            setTimeout(() => {
                // Random mouse movement and scroll simulation
                const docHeight = Math.max(
                    document.body.scrollHeight, 
                    document.documentElement.scrollHeight,
                    document.body.offsetHeight, 
                    document.documentElement.offsetHeight
                );
                
                // Random small scroll
                if (docHeight > window.innerHeight && Math.random() > 0.3) {
                    window.scrollTo({
                        top: Math.floor(Math.random() * 100),
                        behavior: 'smooth'
                    });
                }
            }, Math.floor(Math.random() * 1000) + 500);
            
            // Defuse common anti-bot checks
            window.navigator.globalPrivacyControl = false;
            window.navigator.doNotTrack = Math.random() > 0.5 ? '0' : null;
        }
        """)
        
        # Additional security: add a custom script to hide WebDriver
        await self.browser_context.add_init_script("""
        () => {
            // Function to safely override properties
            function safeOverride(obj, prop, value) {
                try {
                    Object.defineProperty(obj, prop, { get: () => value });
                } catch (e) { 
                    console.log(`Failed to override ${prop}`); 
                }
            }
            
            // Advanced WebDriver hiding techniques
            safeOverride(navigator, 'webdriver', false);
            safeOverride(navigator, 'userAgent', navigator.userAgent.replace('Headless', ''));
            
            // Hide automation-specific properties
            const oldProto = navigator.__proto__;
            delete oldProto.webdriver;
            navigator.__proto__ = oldProto;
            
            // Anti detection for Playwright/Puppeteer
            if (window.navigator.plugins.length === 0) {
                safeOverride(navigator, 'plugins', [1, 2, 3, 4, 5]);
            }
            
            // Spoof console debugging tools detection
            const originalGetterDescriptor = Object.getOwnPropertyDescriptor(window, 'console');
            if (originalGetterDescriptor) {
                Object.defineProperty(window, 'console', {
                    ...originalGetterDescriptor,
                    get: function() {
                        // Check if being accessed via Function.prototype.toString
                        const stackTrace = (new Error()).stack || '';
                        if (stackTrace.includes('toString')) {
                            return {
                                log: function() {},
                                warn: function() {},
                                error: function() {},
                                info: function() {},
                                debug: function() {}
                            };
                        }
                        return originalGetterDescriptor.get.call(this);
                    }
                });
            }
        }
        """)
        
        # Set cookies for non-tracked browsing
        await self.browser_context.add_cookies([
            {
                'name': 'NID',
                'value': '511='+(''.join(random.choices('0123456789', k=10))),
                'domain': '.google.com',
                'path': '/'
            },
            {
                'name': 'NID',
                'value': '511='+(''.join(random.choices('0123456789', k=10))),
                'domain': '.google.co.in',
                'path': '/'
            }
        ])
    
    async def close_browser(self):
        """Close Playwright browser if it's open."""
        async with self.browser_lock:
            await self._cleanup_browser_resources()
            self.browser_initialized = False
                
    async def browser_get_page(self, url, max_retries=2):
        """Use Playwright browser automation to get a page with improved reliability."""
        # Ensure browser is initialized
        browser_ready = await self.initialize_browser()
        if not browser_ready:
            self.logger.warning("Browser not available. Falling back to requests.")
            return None
            
        domain = urlparse(url).netloc
        
        # Check for rate limiting on domain
        self._check_domain_rate_limit(domain)
        
        # Track this request
        self._track_domain_access(domain)
        
        retries = 0
        while retries < max_retries:
            try:
                self.logger.info(f"Browser requesting URL: {url}")
                
                async with self.browser_lock:
                    # Enhanced navigation with progressive wait strategy
                    try:
                        # First attempt with shorter timeout and minimal wait
                        response = await self.page.goto(
                            url, 
                            wait_until="domcontentloaded", 
                            timeout=15000
                        )
                        
                        if not response:
                            self.logger.warning(f"No response object from navigation to {url}")
                            # Wait a moment for page to settle
                            await asyncio.sleep(1)
                        elif response.status >= 400:
                            self.logger.warning(f"HTTP error {response.status} when loading {url}")
                            if response.status == 404:
                                # No need to retry for 404
                                return None
                        
                        # Wait for network to become idle - catches AJAX content
                        try:
                            await self.page.wait_for_load_state('networkidle', timeout=5000)
                            self.logger.info("Network became idle")
                        except PlaywrightTimeoutError:
                            # Continue even if networkidle times out - page might still be usable
                            self.logger.info("Network idle timeout, but continuing")
                            
                        # Wait for essential elements to appear if needed
                        essential_selectors = ['body', 'header', 'footer', 'main', '.content', '#content', '[role="main"]']
                        for selector in essential_selectors:
                            try:
                                # Try to find at least one important page element with short timeout
                                await self.page.wait_for_selector(selector, timeout=1000)
                                self.logger.debug(f"Found essential element: {selector}")
                                break  # Found one essential element, no need to check more
                            except PlaywrightTimeoutError:
                                continue  # Try next selector
                        
                        # Check if page loaded properly
                        page_content = await self.page.content()
                        content_length = len(page_content)
                        
                        self.logger.info(f"Initial page content length: {content_length}")
                        
                        if content_length < 100:  # Very short content might indicate a problem
                            self.logger.warning(f"Very short content ({content_length} bytes) received from {url}")
                            # Print the content for debugging
                            self.logger.debug(f"Full content: {page_content}")
                            retries += 1
                            if retries < max_retries:
                                # Wait a bit and try again
                                await asyncio.sleep(random.uniform(2, 4))
                                continue
                        
                        # Check for CAPTCHA presence with enhanced detection
                        if await self._detect_captcha_in_browser():
                            self.logger.warning(f"CAPTCHA detected on {domain}. Adding to blocked domains.")
                            self.captcha_detected_domains.add(domain)
                            retries += 1
                            
                            # Try CAPTCHA mitigation strategies
                            if retries < max_retries:
                                self.logger.info("Trying to bypass with CAPTCHA mitigation strategy...")
                                # Simulate scrolling to appear more human-like
                                await self._simulate_human_browsing()
                                # Wait longer to see if CAPTCHA clears or page loads alternate content
                                await asyncio.sleep(random.uniform(5, 10))
                                await self.page.reload(wait_until="domcontentloaded", timeout=30000)
                                # Try waiting for network idle after reload
                                try:
                                    await self.page.wait_for_load_state('networkidle', timeout=5000)
                                except PlaywrightTimeoutError:
                                    pass
                                continue
                        
                        # Check if content loaded properly (more efficient check)
                        page_content = await self.page.content()
                        if len(page_content) > 300:  # Smaller min size for valid page
                            # Extract cookies from the browser and save them to our session
                            browser_cookies = await self.browser_context.cookies()
                            for cookie in browser_cookies:
                                self.cookies[cookie['name']] = cookie['value']
                            
                            # Minimal scroll to make page more realistic but faster
                            await self.page.evaluate("window.scrollBy(0, 300)")
                            
                            # Look for lazy-loaded content
                            await self._ensure_lazy_content_loaded()
                            
                            # Get the final page content after all processing
                            final_content = await self.page.content()
                            
                            self.logger.info(f"Successfully retrieved content for {url}, length: {len(final_content)}")
                            
                            # Record successful access for this domain
                            proxy_id = '0'  # Default proxy ID
                            self.proxy_success_count[proxy_id] = self.proxy_success_count.get(proxy_id, 0) + 1
                            self.consecutive_failures = 0  # Reset failure counter
                            
                            return final_content
                        else:
                            self.logger.warning(f"Page content too short ({len(page_content)} bytes) for {url}")
                            retries += 1
                    except PlaywrightTimeoutError:
                        self.logger.warning(f"Timeout loading {url}. Retrying...")
                        retries += 1
                        continue
            
            except PlaywrightTimeoutError:
                self.logger.warning(f"Timeout loading {url}. Retrying...")
                retries += 1
                await asyncio.sleep(random.uniform(1, 3))
                
            except Exception as e:
                self.logger.error(f"Browser error for {url}: {e}")
                retries += 1
                
                # If it's a fatal error, try reinitializing the browser
                if "context already closed" in str(e) or "browser closed" in str(e):
                    self.browser_initialized = False
                    browser_ready = await self.initialize_browser()
                    if not browser_ready:
                        break
                await asyncio.sleep(random.uniform(1, 3))
        
        # All retries failed - record the failure
        proxy_id = '0'  # Default proxy ID
        self.proxy_failure_count[proxy_id] = self.proxy_failure_count.get(proxy_id, 0) + 1
        self.consecutive_failures += 1
        
        # Report detailed failure
        self.logger.warning(f"Failed to get content from {url} after {max_retries} attempts")
        return None
    
    async def _ensure_lazy_content_loaded(self):
        """Ensure lazy-loaded content appears by scrolling and waiting."""
        if not self.page:
            return
            
        try:
            # Get page height
            page_height = await self.page.evaluate("""
                () => Math.max(
                    document.body ? document.body.scrollHeight : 0,
                    document.documentElement ? document.documentElement.scrollHeight : 0
                )
            """)
            
            # Return early if page is very short
            if page_height < 1000:
                return
                
            # Scroll down in steps to trigger lazy loading
            viewport_height = await self.page.evaluate("window.innerHeight")
            scroll_steps = min(3, max(1, page_height // viewport_height))
            
            for i in range(1, scroll_steps + 1):
                # Scroll to position
                position = (page_height * i) // (scroll_steps + 1)
                await self.page.evaluate(f"window.scrollTo(0, {position})")
                
                # Wait briefly for lazy content to load
                await asyncio.sleep(0.5)
                
                # Look for infinite scroll markers
                try:
                    # Check if new content might be loading
                    has_loading_indicator = await self.page.evaluate("""
                        () => {
                            const loaders = document.querySelectorAll('.loading, .spinner, .loader, [class*="loading"], [class*="spinner"], [class*="loader"]');
                            return loaders.length > 0;
                        }
                    """)
                    
                    if has_loading_indicator:
                        # Wait a bit longer for content to load
                        await asyncio.sleep(1)
                except Exception:
                    pass
            
            # Scroll back to top
            await self.page.evaluate("window.scrollTo(0, 0)")
            
        except Exception as e:
            self.logger.warning(f"Error ensuring lazy content loaded: {e}")
    
    async def _detect_captcha_in_browser(self, page=None):
        """Detect if the page contains a CAPTCHA or anti-bot measures.
        
        Args:
            page: Optional page object to check for captchas.
                 If None, uses self.page.
                 
        Returns:
            bool: True if captcha/anti-bot measures detected, False otherwise.
        """
        # Use provided page or fallback to self.page
        target_page = page if page else self.page
        
        if not target_page:
            return False
            
        try:
            # Get the current page URL
            current_url = target_page.url
            
            # Take a screenshot for debugging if in debug mode
            if self.debug_mode:
                try:
                    screenshot_path = "captcha_check.png"
                    await target_page.screenshot(path=screenshot_path)
                    self.logger.info(f"Saved CAPTCHA check screenshot to {screenshot_path}")
                except Exception as e:
                    self.logger.error(f"Could not save CAPTCHA check screenshot: {e}")
            
            # Enhanced list of URL indicators
            captcha_url_indicators = [
                "captcha", "recaptcha", "hcaptcha", "challenge", "security-check", 
                "verify", "unusual_traffic", "sorry/index", "consent.google",
                "ipverification", "interstitial", "cloudflare", "blocked", "bot-protection",
                "cf-challenge", "shield", "firewall", "access-denied", "spam-protection",
                "ddos-guard", "attention-required", "human-verification", "distil-networks",
                "datadome", "imperva", "akamai", "/403", "perimeterx", "accessrestricted",
                "security-verification", "incapsula"
            ]
            
            for indicator in captcha_url_indicators:
                if indicator in current_url.lower():
                    self.logger.warning(f"CAPTCHA/anti-bot likely - URL contains '{indicator}'")
                    return True
            
            # Check for HTTP response status in the range that might indicate blocking
            response_status = None
            try:
                response_status = await target_page.evaluate("() => window.performance.getEntries()[0].responseStatus")
            except Exception:
                pass
                
            if response_status and response_status in [403, 429, 503]:
                self.logger.warning(f"Likely blocked - HTTP status {response_status}")
                return True
            
            # Enhanced checks with better browser fingerprinting detection
            captcha_detected = await target_page.evaluate("""
                () => {
                    const pageText = document.body ? document.body.innerText.toLowerCase() : '';
                    if (!pageText) return false;
                    
                    // Comprehensive list of text indicators for CAPTCHAs and blocks
                    const captchaTexts = [
                        "captcha", "robot", "unusual traffic", "unusual activity",
                        "automated queries", "automated requests", "verify you are a human",
                        "not a robot", "security check", "security challenge", 
                        "something about your browser", "suspicious activity",
                        "detected unusual traffic", "detected automated traffic",
                        "verify your identity", "complete this security check",
                        "solve this puzzle", "recaptcha", "suspicious request",
                        "automated software", "confirm you are not a robot",
                        "please show you're not a robot", "please verify you are a human",
                        "before continuing", "we need to verify", "please verify",
                        "our systems have detected", "please try your request again",
                        "checking if the site connection is secure", "check whether you are a human",
                        "this page checks to see", "automatic query", "automated access",
                        "javascript is disabled", "enable javascript", "cookies are disabled",
                        "temporarily unavailable", "you've been blocked", "your ip address",
                        "access denied", "suspicious request", "bot detected", "bot protection",
                        "browser check", "cloudflare", "ddos protection", "blocked", "abuse",
                        "forbidden", "security system", "verification required", "challenge",
                        "our site is protected", "we need to make sure", "security system",
                        "human visitor", "prove", "rate limited", "too many requests",
                        "temporarily limited", "temporarily blocked", "firewall", "cf-ray",
                        "security precaution", "perform security checks", "perform security analysis"
                    ];
                    
                    // Search for these indicators in the page text
                    for (const text of captchaTexts) {
                        if (pageText.includes(text)) {
                            return true;
                        }
                    }
                    
                    // Enhanced list of CAPTCHA elements selectors
                    const captchaElements = [
                        'iframe[src*="recaptcha"]',
                        'iframe[src*="captcha"]',
                        'iframe[src*="hcaptcha"]',
                        'iframe[src*="cloudflare"]',
                        'iframe[src*="challenge"]',
                        'iframe[src*="cf-"]',
                        'div.g-recaptcha',
                        'form#captcha-form',
                        'div[id*="captcha"]',
                        'div[class*="captcha"]',
                        'input[name="captcha"]',
                        '#captcha',
                        '.captcha',
                        'textarea[name="g-recaptcha-response"]',
                        'div[data-sitekey]',
                        '#challenge-form',
                        '#challenge-stage',
                        '#challenge-running',
                        '.h-captcha',
                        '#cf-challenge-running',
                        '#cf-please-wait',
                        '#cf-browser-status',
                        '#cf-error-details',
                        '#cf-content',
                        'div[class*="cf-"]',
                        'div[id*="cf-"]',
                        'div[class*="challenge"]',
                        'div[id*="challenge"]',
                        '.distil_r_captcha',
                        'div[class*="px-captcha"]',
                        'div[class*="firewall"]',
                        'div[id*="px-"]',
                        'div[class*="px-"]',
                        'div[id*="akamai-"]',
                        'div[class*="imperva-"]'
                    ];
                    
                    for (const selector of captchaElements) {
                        if (document.querySelector(selector)) {
                            return true;
                        }
                    }
                    
                    // Check for image elements with CAPTCHA-related attributes
                    const images = document.querySelectorAll('img');
                    for (const img of images) {
                        const src = img.src.toLowerCase();
                        const alt = (img.alt || '').toLowerCase();
                        if (
                            src.includes('captcha') || 
                            src.includes('challenge') || 
                            src.includes('verify') ||
                            src.includes('security') ||
                            alt.includes('captcha') ||
                            alt.includes('security') ||
                            alt.includes('verification')
                        ) {
                            return true;
                        }
                    }
                    
                    // Check for typical client-side fingerprinting/detection techniques
                    try {
                        // Detect canvas fingerprinting
                        const canvas = document.createElement('canvas');
                        const ctx = canvas.getContext('2d');
                        const canvasSupported = !!ctx;
                        
                        // Count how many scripts access navigator properties
                        let navigatorAccessCount = 0;
                        const origNavigatorProps = {};
                        
                        // Properties often accessed by fingerprinting scripts
                        const fingerprintProps = [
                            'userAgent', 'platform', 'language', 'languages', 
                            'deviceMemory', 'hardwareConcurrency', 'plugins'
                        ];
                        
                        // Set up getters to detect access
                        for (const prop of fingerprintProps) {
                            if (prop in navigator) {
                                const origValue = navigator[prop];
                                origNavigatorProps[prop] = origValue;
                                
                                // This won't actually modify the navigator object
                                // but will let us detect if scripts are checking these properties
                                // Object.defineProperty(navigator, prop, {
                                //     get: function() { 
                                //         navigatorAccessCount++; 
                                //         return origValue;
                                //     }
                                // });
                            }
                        }
                        
                        // If we're on a page with tons of fingerprinting, this might indicate anti-bot measures
                        // We'll check how many scripts are on the page as a heuristic
                        const scriptCount = document.querySelectorAll('script').length;
                        if (scriptCount > 40) {
                            // Pages with many scripts often have anti-bot measures
                            return true;
                        }
                        
                        // Check for Cloudflare specific elements/attributes
                        if (
                            document.getElementById('cf-content') ||
                            document.querySelector('*[class*="cf-browser-verification"]') ||
                            document.querySelector('*[id*="cf-please-wait"]') ||
                            document.getElementById('cf-error-details')
                        ) {
                            return true;
                        }
                    } catch(e) {
                        // Error in fingerprinting detection, ignore
                    }
                    
                    // Check for non-standard but common anti-bot techniques
                    if (
                        // Checks for hCaptcha
                        window.hcaptcha ||
                        
                        // Checks for reCAPTCHA v3
                        (window.grecaptcha && window.grecaptcha.enterprise) ||
                        
                        // Checks for Cloudflare browser check
                        window.a || window._cf_chl_opt ||
                        
                        // Check for DataDome
                        window.datadome || document.cookie.includes('datadome=')
                    ) {
                        return true;
                    }
                    
                    return false;
                }
            """)
            
            if captcha_detected:
                self.logger.warning("CAPTCHA/anti-bot protection detected via page content")
                return True
            
            # Check for specific Google recaptcha elements with added hCaptcha detection
            protection_present = await target_page.evaluate("""
                () => {
                    // Check for reCAPTCHA
                    const recaptchaPresent = !!document.querySelector('script[src*="recaptcha"]') || 
                                             !!document.querySelector('[name="g-recaptcha-response"]');
                    
                    // Check for hCaptcha
                    const hcaptchaPresent = !!document.querySelector('script[src*="hcaptcha"]') ||
                                            !!document.querySelector('[name="h-captcha-response"]') ||
                                            !!document.querySelector('iframe[src*="hcaptcha.com"]');
                    
                    // Check for Cloudflare browser check
                    const cloudflarePresent = typeof _cf_chl_opt !== 'undefined' ||
                                             !!document.querySelector('script[src*="cloudflare"]') ||
                                             !!document.getElementById('cf-please-wait') ||
                                             document.body?.classList.contains('no-js');
                    
                    // Check for Imperva/Incapsula
                    const impervaPresent = typeof _imp_apg_clk !== 'undefined' ||
                                           document.cookie.includes('incap_ses') ||
                                           document.cookie.includes('visid_incap');
                    
                    // Check for Akamai Bot Manager
                    const akamaiPresent = typeof _abm !== 'undefined' ||
                                          document.cookie.includes('ak_bmsc') ||
                                          document.cookie.includes('bm_sv');
                    
                    // Check for PerimeterX
                    const perimeterXPresent = typeof PX !== 'undefined' ||
                                              document.cookie.includes('_px') ||
                                              document.cookie.includes('_pxff');
                    
                    return recaptchaPresent || hcaptchaPresent || cloudflarePresent || 
                           impervaPresent || akamaiPresent || perimeterXPresent;
                }
            """)
            
            if protection_present:
                self.logger.warning("Bot protection elements detected on the page")
                return True
            
            # Check content length and structure (often a sign of blocking)
            content_check = await target_page.evaluate("""
                () => {
                    // Check if the page has suspiciously little content
                    const contentText = document.body ? document.body.innerText.trim() : '';
                    const links = document.querySelectorAll('a[href]');
                    
                    // If page has almost no text and few links, might be blocked/captcha
                    return {
                        textLength: contentText.length,
                        linkCount: links.length,
                        hasBody: !!document.body,
                        hasContent: !!document.querySelector('main, #content, .content, article, section')
                    };
                }
            """)
            
            # A real page should have substantial content and structure
            if ((content_check['textLength'] < 150 and content_check['linkCount'] < 3) or 
               (not content_check['hasBody']) or 
               (content_check['textLength'] < 500 and not content_check['hasContent'])):
                self.logger.warning(f"Suspiciously little content: {content_check['textLength']} chars, {content_check['linkCount']} links")
                
                # For Google search results and similar pages, check for expected structure
                if 'google' in current_url.lower() or 'search' in current_url.lower():
                    # Check specifically if the page has typical search result structure
                    has_results = await target_page.evaluate("""
                        () => {
                            // Look for typical search result elements
                            const hasGoogleResults = !!document.querySelector('div.g, div[data-sokoban-container], div.yuRUbf');
                            const hasBingResults = !!document.querySelector('.b_algo, .b_title');
                            const hasDuckDuckGoResults = !!document.querySelector('.result, .result__body');
                            const hasYahooResults = !!document.querySelector('.algo, .algo-sr');
                            
                            // Generic search results selectors
                            const hasGenericResults = !!document.querySelector('.results, .search-results, .serp');
                            
                            // Search box indicators
                            const hasSearchBox = !!document.querySelector('input[name="q"], input[type="search"], .search-box, .searchbox');
                            
                            // Logo indicators
                            const hasLogo = !!document.querySelector('img[alt*="Google"], img[alt*="Bing"], img[alt*="Search"], .logo');
                            
                            return {
                                hasSearchResults: hasGoogleResults || hasBingResults || hasDuckDuckGoResults || 
                                                  hasYahooResults || hasGenericResults,
                                hasSearchBox: hasSearchBox,
                                hasLogo: hasLogo
                            };
                        }
                    """)
                    
                    # If it has search elements but no results, likely a CAPTCHA/block
                    if (not has_results['hasSearchResults'] and 
                        (has_results['hasSearchBox'] or has_results['hasLogo'])):
                        self.logger.warning("Page appears to be a search engine but has no search results - likely CAPTCHA/blocking")
                        return True
            
            # No CAPTCHA or anti-bot protection detected
            return False
            
        except Exception as e:
            self.logger.error(f"Error in browser CAPTCHA detection: {e}")
            # If we can't check properly, return False to avoid false positives
            return False
    
    async def _simulate_human_browsing(self):
        """Simulate human-like browsing behavior to avoid detection."""
        if not self.page:
            return
            
        try:
            # Get page dimensions for natural movements
            viewport_size = await self.page.evaluate("""
                () => {
                    return {
                        width: window.innerWidth,
                        height: window.innerHeight,
                        scrollHeight: Math.max(
                            document.body ? document.body.scrollHeight : 0,
                            document.documentElement ? document.documentElement.scrollHeight : 0
                        )
                    };
                }
            """)
            
            width = viewport_size.get('width', 1366)
            height = viewport_size.get('height', 768)
            scroll_height = viewport_size.get('scrollHeight', 1500)
            
            # Calculate reasonable scroll positions
            scroll_positions = []
            if scroll_height > height:
                # Create a natural scroll pattern with randomness
                # Start with smaller scrolls, then larger ones
                current_position = 0
                while current_position < scroll_height:
                    # Random scroll amount between 200-600 pixels
                    scroll_amount = random.randint(200, 600)
                    current_position += scroll_amount
                    if current_position > scroll_height:
                        current_position = scroll_height
                    scroll_positions.append(current_position)
            
            # Initial pause before interacting
            await asyncio.sleep(random.uniform(1.0, 2.5))
            
            # If page has scrollable content, perform natural scrolling
            if scroll_positions:
                # Start with slow scrolls, then faster
                for i, position in enumerate(scroll_positions):
                    # Scrolling speed increases as we go down the page
                    speed_factor = min(1.5, 0.5 + (i * 0.25))
                    
                    # Scroll to position
                    await self.page.evaluate(f"window.scrollTo(0, {position})")
                    
                    # Use variable pauses between scrolls
                    if i == 0:
                        # Longer pause at the beginning
                        await asyncio.sleep(random.uniform(1.0, 2.0))
                    elif i == len(scroll_positions) - 1:
                        # Longer pause at the end
                        await asyncio.sleep(random.uniform(1.0, 3.0))
                    else:
                        # Shorter pauses in the middle
                        await asyncio.sleep(random.uniform(0.5, 1.0) / speed_factor)
                
                # Sometimes scroll back up partially
                if random.random() < 0.7 and len(scroll_positions) > 2:
                    middle_position = scroll_positions[len(scroll_positions) // 2]
                    await self.page.evaluate(f"window.scrollTo(0, {middle_position})")
                    await asyncio.sleep(random.uniform(0.5, 1.5))
            
            # Simulate random mouse movements
            for _ in range(random.randint(2, 5)):
                # Generate random coordinates within the viewport
                x = random.randint(100, width - 100)
                y = random.randint(100, min(height - 100, 700))  # Avoid the very bottom
                
                # Move mouse to coordinates
                await self.page.mouse.move(x, y)
                
                # Pause briefly between movements
                await asyncio.sleep(random.uniform(0.1, 0.3))
            
            # Simulate hovering over search results occasionally
            result_selector = random.choice([
                "div.g", "div.yuRUbf > a", "h3.LC20lb", "a[ping]", "div[data-sokoban-container]"
            ])
            
            try:
                # Try to find search results
                results = await self.page.query_selector_all(result_selector)
                
                # If we found results, hover over a random selection
                if results and len(results) > 0:
                    # Choose 1-3 random results to hover over
                    random_results = random.sample(results, min(len(results), random.randint(1, 3)))
                    
                    for result in random_results:
                        # Get element position
                        bbox = await result.bounding_box()
                        if bbox:
                            # Move to the result
                            await self.page.mouse.move(
                                bbox['x'] + bbox['width'] / 2,
                                bbox['y'] + bbox['height'] / 2
                            )
                            
                            # Pause as if reading
                            await asyncio.sleep(random.uniform(0.3, 1.2))
            except Exception as e:
                # Non-critical error, can be ignored
                pass
            
            # Final random pause
            await asyncio.sleep(random.uniform(0.5, 1.5))
            
        except Exception as e:
            self.logger.warning(f"Error in human browsing simulation: {e}")
            # Non-critical error, continue with the scraping
    
    def get_random_user_agent(self) -> str:
        """Return a random user agent with enhanced browser fingerprinting evasion."""
        # Try fake_useragent first for more diverse and realistic agents
        if self.ua:
            try:
                # Choose a random modern browser with weighted probability
                browser_choices = [
                    ('chrome', 0.55),  # More weight to Chrome as it's most common
                    ('firefox', 0.25),
                    ('safari', 0.10),
                    ('edge', 0.10)
                ]
                
                # Weighted random choice
                browser_type = random.choices(
                    [b[0] for b in browser_choices],
                    weights=[b[1] for b in browser_choices],
                    k=1
                )[0]
                
                return self.ua[browser_type]
            except Exception as e:
                self.logger.warning(f"Error using fake_useragent: {e}, falling back to static list")
        
        # Enhanced fallback user agents with version diversity and modern patterns
        # These are organized by browser type for better selection
        modern_user_agents = {
            'chrome': [
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36',
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36',
                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36',
                'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36',
                # Chrome on Android
                'Mozilla/5.0 (Linux; Android 12; SM-G998B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Mobile Safari/537.36',
                # Chrome on iOS 
                'Mozilla/5.0 (iPhone; CPU iPhone OS 16_4 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) CriOS/112.0.5615.46 Mobile/15E148 Safari/604.1'
            ],
            'firefox': [
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/111.0',
                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:109.0) Gecko/20100101 Firefox/112.0',
                'Mozilla/5.0 (X11; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/111.0',
                # Firefox on Android
                'Mozilla/5.0 (Android 13; Mobile; rv:109.0) Gecko/111.0 Firefox/111.0',
                # Firefox on iOS (actually WebKit but branded as Firefox)
                'Mozilla/5.0 (iPhone; CPU iPhone OS 16_4 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) FxiOS/110.0 Mobile/15E148 Safari/605.1.15'
            ],
            'safari': [
                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.4 Safari/605.1.15',
                'Mozilla/5.0 (iPhone; CPU iPhone OS 16_4 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.4 Mobile/15E148 Safari/604.1',
                'Mozilla/5.0 (iPad; CPU OS 16_4 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.4 Mobile/15E148 Safari/604.1'
            ],
            'edge': [
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36 Edg/111.0.1661.62',
                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36 Edg/111.0.1661.62',
                # Edge on Android
                'Mozilla/5.0 (Linux; Android 12; Pixel 6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Mobile Safari/537.36 EdgA/110.0.1587.66',
                # Edge on iOS
                'Mozilla/5.0 (iPhone; CPU iPhone OS 16_4 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 EdgiOS/110.0.1587.63 Mobile/15E148 Safari/605.1.15'
            ]
        }
        
        # First decide which browser type to use
        browser_weights = {'chrome': 55, 'firefox': 25, 'safari': 10, 'edge': 10}
        browser_type = random.choices(
            list(browser_weights.keys()),
            weights=list(browser_weights.values()),
            k=1
        )[0]
        
        # Then select a random user agent of that browser type
        ua_list = modern_user_agents.get(browser_type, modern_user_agents['chrome'])
        return random.choice(ua_list)
    
    def _track_proxy_success(self, success=True, proxy_id=None):
        """Track success/failure of proxy usage for better rotation decisions.
        
        Args:
            success: Whether the proxy was used successfully
            proxy_id: Identifier for the proxy (defaults to current proxy)
        """
        if proxy_id is None:
            proxy_id = str(self.rotation_counter % len(self.proxy_list))
            
        if success:
            self.proxy_success_count[proxy_id] = self.proxy_success_count.get(proxy_id, 0) + 1
            self.consecutive_failures = 0
        else:
            self.proxy_failure_count[proxy_id] = self.proxy_failure_count.get(proxy_id, 0) + 1
            self.consecutive_failures += 1
    
    def get_random_proxy(self):
        """Get a random proxy from the list, with intelligent selection based on performance.
        
        Returns:
            dict: A proxy configuration dictionary or None for direct connection
        """
        if not self.proxy_list:
            self.logger.warning("No proxies available")
            return None
        
        # Check if we have too many consecutive failures, fall back to direct connection occasionally
        if self.consecutive_failures >= self.max_consecutive_failures and random.random() < 0.3:
            self.logger.info("Too many consecutive failures, temporarily trying direct connection")
            return None
            
        # If we have performance data, use it to make smarter choices
        if self.proxy_success_count:
            # Calculate success rates and weighted scores for each proxy
            proxy_scores = {}
            
            for i, proxy in enumerate(self.proxy_list):
                proxy_id = str(i)  # Use index as identifier
                successes = self.proxy_success_count.get(proxy_id, 0)
                failures = self.proxy_failure_count.get(proxy_id, 0)
                
                # Skip direct connection (None) unless absolutely necessary
                if proxy is None and failures < 3:
                    continue
                
                # Calculate a score - higher is better
                if successes + failures == 0:
                    # No data yet, give a neutral score with bias for new proxies
                    proxy_scores[proxy_id] = 0.5 + random.random() * 0.1
                else:
                    # Success rate with a minimum value to avoid division by zero
                    success_rate = successes / max(1, (successes + failures))
                    
                    # Apply multipliers based on proxy config
                    score = success_rate
                    
                    # Boost Indian proxies
                    if proxy and isinstance(proxy, dict) and 'http' in proxy:
                        proxy_str = str(proxy['http'])
                        if 'country=in' in proxy_str.lower() or 'country=IN' in proxy_str:
                            score *= 1.2  # 20% boost for Indian proxies
                            
                        # Small boost for residential proxies (often more reliable)
                        if 'residential' in proxy_str:
                            score *= 1.1
                    
                    # Cap at 0.95 to allow some exploration of other proxies
                    proxy_scores[proxy_id] = min(0.95, score)
                    
                    # Add a small random factor to break ties and ensure exploration
                    proxy_scores[proxy_id] += random.random() * 0.05
            
            # Get proxy IDs sorted by score (highest first)
            sorted_proxies = sorted(proxy_scores.keys(), key=lambda k: proxy_scores[k], reverse=True)
            
            # Weighted random selection - higher chance for better performing proxies
            total = sum(proxy_scores.values())
            if total > 0:
                r = random.random() * total
                running_total = 0
                
                for proxy_id in sorted_proxies:
                    running_total += proxy_scores[proxy_id]
                    if running_total >= r:
                        # Get the proxy by index
                        selected_index = int(proxy_id)
                        selected_proxy = self.proxy_list[selected_index]
                        
                        # Record which proxy we're using
                        proxy_desc = "direct connection" if selected_proxy is None else f"proxy {selected_index}"
                        self.logger.info(f"Selected {proxy_desc} with score {proxy_scores[proxy_id]:.2f}")
                        
                        return selected_proxy
            
            # Fallback to first proxy if something went wrong with weighted selection
            return self.proxy_list[0]
        else:
            # No performance data yet, prioritize Indian proxies but occasionally try others
            indian_proxies = []
            other_proxies = []
            
            for proxy in self.proxy_list:
                if proxy is None:
                    other_proxies.append(proxy)
                    continue
                    
                # Check if it's an Indian proxy
                if isinstance(proxy, dict) and 'http' in proxy:
                    proxy_str = str(proxy['http'])
                    if 'country=in' in proxy_str.lower() or 'country=IN' in proxy_str:
                        indian_proxies.append(proxy)
                    else:
                        other_proxies.append(proxy)
                else:
                    other_proxies.append(proxy)
            
            # 70% chance to use Indian proxy if available
            if indian_proxies and random.random() < 0.7:
                return random.choice(indian_proxies)
            elif other_proxies:
                return random.choice(other_proxies)
            else:
                return random.choice(self.proxy_list)
                
    def validate_indian_phone(self, phone: str, source: str = "unknown") -> Optional[Dict]:
        """Validate and format Indian phone numbers with enhanced validation.
        
        Args:
            phone: The phone number to validate
            source: String indicating where the number was found
            
        Returns:
            Dictionary with phone number metadata if valid, otherwise None
        """
        # Use our improved validator with source tracking
        return validate_indian_phone(phone, source)
    
    def make_request(self, url, max_retries=3):
        """Make an HTTP request with retry logic and proxy rotation."""
        # Extract domain for rate limiting
        domain = urlparse(url).netloc
        
        # Skip known blocked domains
        if domain in self.blocked_domains:
            self.logger.warning(f"Skipping known blocked domain: {domain}")
            return None
            
        # Check for CAPTCHA detection
        if domain in self.captcha_detected_domains:
            self.logger.info(f"Domain {domain} previously showed CAPTCHA. Trying with browser.")
            
            # Use our run_async helper function for browser operations
            if self.use_browser:
                try:
                    # First initialize the browser if needed
                    browser_initialized = run_async(self.initialize_browser())
                    
                    if browser_initialized:
                        # Use browser to get the page
                        html_content = run_async(self.browser_get_page(url))
                        if html_content:
                            return html_content
                except Exception as e:
                    self.logger.error(f"Browser request failed for {url}: {e}")
                    # Continue with regular request as fallback
        
        # Check if we've exceeded max requests for this domain
        if self.domain_request_count.get(domain, 0) >= self.max_requests_per_domain:
            self.logger.warning(f"Maximum request limit reached for domain {domain}")
            # Try with browser as fallback
            if self.use_browser:
                loop = get_or_create_event_loop()
                try:
                    browser_initialized = loop.run_until_complete(self.initialize_browser())
                    if browser_initialized:
                        return loop.run_until_complete(self.browser_get_page(url, max_retries=1))
                except Exception as e:
                    self.logger.error(f"Browser fallback request failed: {e}")
            return None
            
        # Check and enforce rate limiting
        self._check_domain_rate_limit(domain)
        
        # Track this request
        self._track_domain_access(domain)
        
        retries = 0
        headers = self._get_realistic_headers(url)
        
        while retries < max_retries:
            try:
                # Add a shorter random delay before request
                delay = random.uniform(0.5, 2.0)
                self.logger.info(f"Waiting {delay:.2f} seconds before requesting {url}")
                time.sleep(delay)
                
                # Get a random proxy for this attempt
                proxies = self.get_random_proxy()
                
                # Use shorter timeout
                timeout = 15 if proxies else 10
                
                # Use our session with cookies
                response = self.session.get(
                    url,
                    headers=headers,
                    proxies=proxies,
                    timeout=timeout,
                    verify=True,
                    cookies=self.cookies
                )
                
                # Store cookies from this response
                if response.cookies:
                    self.cookies.update(dict(response.cookies))
                
                # Check response status
                if response.status_code == 200:
                    # Check for CAPTCHA or bot detection in response - simplified check
                    content_lower = response.text.lower()
                    if 'captcha' in content_lower or 'robot' in content_lower or 'automated' in content_lower:
                        self.logger.warning(f"CAPTCHA detected at {url}. Adding to CAPTCHA domains list.")
                        self.captcha_detected_domains.add(domain)
                        
                        # Fall back to browser automation if needed
                        if self.use_browser and retries >= 1:
                            loop = get_or_create_event_loop()
                            try:
                                browser_initialized = loop.run_until_complete(self.initialize_browser())
                                if browser_initialized:
                                    return loop.run_until_complete(self.browser_get_page(url, max_retries=1))
                            except Exception as e:
                                self.logger.error(f"Browser fallback request failed: {e}")
                            finally:
                                loop.close()
                        
                        retries += 1
                        time.sleep(random.uniform(1, 3))  # Shorter cooldown
                    else:
                        return response
                        
                elif response.status_code == 403 or response.status_code == 429:
                    self.logger.warning(f"Request blocked or rate limited (status {response.status_code}). Rotating proxy and retrying...")
                    
                    # Add to blocked domains if consistently getting blocked
                    if retries >= 1:
                        self.blocked_domains.add(domain)
                        
                    # Try browser automation as a fallback
                    if retries >= 1 and self.use_browser:
                        loop = get_or_create_event_loop()
                        try:
                            browser_initialized = loop.run_until_complete(self.initialize_browser())
                            if browser_initialized:
                                return loop.run_until_complete(self.browser_get_page(url, max_retries=1))
                        except Exception as e:
                            self.logger.error(f"Browser fallback request failed: {e}")
                        finally:
                            loop.close()
                        
                    retries += 1
                    # Shorter backoff
                    time.sleep(random.uniform(1 * (retries + 1), 3 * (retries + 1)))
                    
                else:
                    self.logger.warning(f"Request failed with status code: {response.status_code}")
                    retries += 1
                    time.sleep(random.uniform(1, 2))
                    
            except (requests.exceptions.ProxyError, requests.exceptions.SSLError) as e:
                self.logger.warning(f"Proxy error: {e}. Trying different proxy...")
                retries += 1
                time.sleep(random.uniform(0.5, 1))
                
            except (requests.exceptions.ConnectionError, requests.exceptions.Timeout, 
                   requests.exceptions.ReadTimeout, socket.timeout) as e:
                self.logger.warning(f"Connection error: {e}. Retrying...")
                retries += 1
                time.sleep(random.uniform(0.5, 1))
                
            except Exception as e:
                self.logger.error(f"Unexpected error: {e}")
                retries += 1
                time.sleep(random.uniform(0.5, 1))
        
        # If all retries failed with regular requests, try browser automation
        if self.use_browser:
            self.logger.info(f"All request retries failed for {url}. Trying with browser automation...")
            loop = get_or_create_event_loop()
            try:
                browser_initialized = loop.run_until_complete(self.initialize_browser())
                if browser_initialized:
                    return loop.run_until_complete(self.browser_get_page(url, max_retries=1))
            except Exception as e:
                self.logger.error(f"Final browser fallback failed: {e}")
            finally:
                loop.close()
            
        # If all retries failed, return None
        self.logger.error(f"All retries failed for URL: {url}")
        return None
    
    def _get_realistic_headers(self, url):
        """Generate realistic HTTP headers that vary between requests with improved browser fingerprinting."""
        domain = urlparse(url).netloc
        user_agent = self.get_random_user_agent()
        
        # Browser type detection for consistent headers
        is_chrome = 'Chrome/' in user_agent or 'CriOS/' in user_agent
        is_firefox = 'Firefox/' in user_agent or 'FxiOS/' in user_agent
        is_safari = 'Safari/' in user_agent and not is_chrome and not is_firefox and 'Version/' in user_agent
        is_edge = 'Edg/' in user_agent or 'EdgA/' in user_agent or 'EdgiOS/' in user_agent
        
        # Pick language list appropriate for the browser
        if 'en-IN' in user_agent or 'in' in domain.split('.')[-1]:
            # Indian locale
            accept_language = random.choice([
                'en-IN,en-US;q=0.9,en;q=0.8,hi;q=0.7',
                'en-IN,en;q=0.9,hi;q=0.8',
                'en-US,en;q=0.9,hi;q=0.8',
                'en;q=0.9,hi;q=0.8,en-GB;q=0.7',
                'hi-IN,hi;q=0.9,en;q=0.8'
            ])
        else:
            # Generic English locale
            accept_language = random.choice([
                'en-US,en;q=0.9', 
                'en-GB,en;q=0.9', 
                'en-CA,en;q=0.9',
                'en,en-US;q=0.9,fr;q=0.8'
            ])
            
        # Base headers that most browsers send
        headers = {
            'User-Agent': user_agent,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': accept_language,
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Cache-Control': random.choice(['max-age=0', 'no-cache', 'no-store', 'must-revalidate', 'max-age=0, private'])
        }
        
        # Add Sec-* headers which are important for modern browsers
        headers.update({
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1'
        })
        
        # Browser-specific headers
        if is_chrome:
            headers['sec-ch-ua'] = '"Google Chrome";v="111", "Not(A:Brand";v="8", "Chromium";v="111"'
            headers['sec-ch-ua-mobile'] = '?0'
            headers['sec-ch-ua-platform'] = random.choice(['"Windows"', '"macOS"', '"Linux"', '"Android"'])
        elif is_firefox:
            # Firefox doesn't send sec-ch-ua headers
            pass
        elif is_safari:
            # Safari specific behaviors
            headers['Accept'] = 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'
        elif is_edge:
            headers['sec-ch-ua'] = '"Microsoft Edge";v="111", "Not(A:Brand";v="8", "Chromium";v="111"'
            headers['sec-ch-ua-mobile'] = '?0'
            headers['sec-ch-ua-platform'] = random.choice(['"Windows"', '"macOS"'])
            
        # Sometimes include a referer for requests to look more natural
        if random.random() > 0.3:  # 70% chance to include referer
            # Choose referer based on URL
            if 'google' in domain:
                # For Google, no referer or same-origin referer
                if random.random() > 0.5:
                    headers['Referer'] = f'https://{domain}/'
            elif 'search' in url or 'query' in url or 'q=' in url:
                # For search pages, referer usually comes from a search engine
                referers = [
                    'https://www.google.com/',
                    'https://www.google.co.in/',
                    'https://www.bing.com/',
                    'https://search.yahoo.com/',
                    'https://duckduckgo.com/'
                ]
                headers['Referer'] = random.choice(referers)
            else:
                # For regular pages, referer could be the domain's homepage or search engine
                if random.random() > 0.5:
                    headers['Referer'] = f'https://{domain}/'
                else:
                    headers['Referer'] = random.choice([
                        'https://www.google.com/search?q=',
                        'https://www.google.co.in/search?q=',
                        'https://www.bing.com/search?q='
                    ]) + quote(domain)
                    
        # Add random viewport dimensions for more realism
        resolutions = [
            (1366, 768), (1920, 1080), (1536, 864), (1440, 900),
            (1280, 720), (1600, 900), (2560, 1440), (3840, 2160)
        ]
        if random.random() > 0.5:
            chosen_res = random.choice(resolutions)
            headers['Viewport-Width'] = str(chosen_res[0])
            headers['viewport-width'] = str(chosen_res[0])
        
        # Add Privacy and DNT flags with some randomness
        if random.random() > 0.5:
            headers['DNT'] = '1'
            
        if random.random() > 0.7:
            headers['Sec-GPC'] = '1'  # Global Privacy Control
            
        return headers
    
    def _detect_captcha(self, response):
        """Detect if a response contains a CAPTCHA page."""
        try:
            # Check status code first
            if response.status_code in [429, 403]:
                self.logger.warning(f"CAPTCHA likely - HTTP status {response.status_code}")
                return True
                
            # Check for common CAPTCHA markers in the text
            captcha_indicators = [
                'captcha', 'CAPTCHA', 
                'robot', 'Robot', 
                'unusual traffic', 'unusual activity',
                'automated queries', 'automated requests',
                'verify you are a human', 'not a robot',
                'security check', 'Security Challenge',
                'something about your browser', 'suspicious activity',
                'detected unusual traffic', 'detected automated traffic',
                'verify your identity', 'complete this security check',
                'solve this puzzle', 'reCAPTCHA',
                'suspicious request', 'automated software'
            ]
            
            # Check response text for captcha indicators
            for indicator in captcha_indicators:
                if indicator in response.text:
                    self.logger.warning(f"CAPTCHA detected: '{indicator}' found in response")
                    return True
            
            # Look for specific HTML elements commonly used in CAPTCHA pages
            captcha_elements = [
                'input[name="captcha"]', 
                'iframe[src*="recaptcha"]',
                'iframe[src*="captcha"]',
                'div.g-recaptcha',
                'form#captcha-form',
                'div[id*="captcha"]',
                'div[class*="captcha"]'
            ]
            
            # Parse HTML to check for CAPTCHA elements
            try:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                for element_selector in captcha_elements:
                    selector_type, selector_value = element_selector.split('[')[0], element_selector.split('[')[1].rstrip(']')
                    
                    if selector_type and ']' in element_selector:
                        # CSS selector with attribute
                        attr_name, attr_value = selector_value.split('=')
                        attr_value = attr_value.strip('"*')
                        
                        for elem in soup.find_all(selector_type):
                            if attr_name in elem.attrs and attr_value in elem.attrs[attr_name]:
                                self.logger.warning(f"CAPTCHA element found: {element_selector}")
                                return True
                    elif selector_type and '[' not in element_selector:
                        # Simple element or class selector
                        if '.' in selector_type:
                            elem_type, elem_class = selector_type.split('.')
                            if soup.find(elem_type, class_=elem_class):
                                self.logger.warning(f"CAPTCHA element found: {element_selector}")
                                return True
                        elif '#' in selector_type:
                            elem_type, elem_id = selector_type.split('#')
                            if elem_type:
                                if soup.find(elem_type, id=elem_id):
                                    self.logger.warning(f"CAPTCHA element found: {element_selector}")
                                    return True
                            else:
                                if soup.find(id=elem_id):
                                    self.logger.warning(f"CAPTCHA element found: {element_selector}")
                                    return True
            except Exception as e:
                self.logger.error(f"Error parsing HTML for CAPTCHA detection: {e}")
            
            # No CAPTCHA detected
            return False
        except Exception as e:
            self.logger.error(f"Error in CAPTCHA detection: {e}")
            # If we can't parse the response properly, assume it might be a CAPTCHA
            return True
    
    def search_google(self, keyword: str, num_results: int = 10, page: int = 0) -> List[str]:
        """
        Search Google for a keyword and return a list of URLs
        
        Args:
            keyword (str): Search keyword
            num_results (int): Maximum number of results to return
            page (int): Page number (0-indexed)
            
        Returns:
            List[str]: List of URLs
        """
        results = []
        
        # Add quotation marks for exact match and site:.in to prioritize Indian websites
        if 'site:' not in keyword:
            if self.is_indian_domain(keyword):
                search_query = f'"{keyword}" site:.in'
            else:
                search_query = f'"{keyword}"'
        else:
            search_query = keyword
        
        start_index = page * 10
        
        # Prepare the URL
        url = f"https://www.google.co.in/search?q={quote(search_query)}&start={start_index}"
        
        try:
            # Apply rate limiting
            base_domain = "google.com"
            self._check_domain_rate_limit(base_domain)
            
            # Get a session with appropriate headers
            session = self.get_request_session()
            
            # Get realistic headers for this domain
            headers = self._get_realistic_headers(url)
            
            # Make the request with multiple retries and exponential backoff
            max_retries = 3
            retry_count = 0
            
            while retry_count < max_retries:
                try:
                    # Track the domain access and respect rate limits
                    self._track_domain_access(base_domain)
                    
                    response = session.get(
                        url, 
                        headers=headers,
                        timeout=30,
                        allow_redirects=True
                    )
                    
                    # Check if we've been blocked or rate limited
                    if response.status_code == 429:
                        # Record error to adjust rate limiting
                        self.rate_limiter.record_error(base_domain, status_code=429)
                        
                        # Wait longer before retry (exponential backoff)
                        backoff_time = 30 * (2 ** retry_count)
                        logging.warning(f"âš ï¸ Got status code 429 from Google - backing off for {backoff_time}s")
                        time.sleep(backoff_time)
                        retry_count += 1
                        continue
                        
                    if response.status_code != 200:
                        # Other error - record and retry with backoff
                        self.rate_limiter.record_error(base_domain, status_code=response.status_code)
                        retry_count += 1
                        time.sleep(5 * retry_count)
                        continue
                    
                    # Check if response contains a CAPTCHA
                    if self._detect_captcha(response):
                        logging.warning("âŒ CAPTCHA detected in Google search results")
                        self.rate_limiter.record_error(base_domain, status_code=403)  # Treat as forbidden
                        
                        # After CAPTCHA detection, it's better to try with browser
                        logging.info("Switching to browser-based search after CAPTCHA")
                        return self._fallback_to_browser_search(keyword, num_results, page)
                    
                    # Process the response
                    html_content = response.text
                    
                    # Use BeautifulSoup to parse the HTML
                    soup = BeautifulSoup(html_content, 'html.parser')
                    
                    # Extract search result links - multiple selector patterns for robustness
                    result_links = []
                    
                    # Look for standard Google search result links
                    for a_tag in soup.select('div.yuRUbf > a') + soup.select('.DKV0Md a') + soup.select('.g a'):
                        href = a_tag.get('href', '')
                        if href.startswith('http') and 'google.' not in href:
                            result_links.append(href)
                    
                    # Also search for organic results using main result container
                    for result_div in soup.select('.g'):
                        links = result_div.select('a')
                        for link in links:
                            href = link.get('href', '')
                            if href.startswith('http') and 'google.' not in href:
                                result_links.append(href)
                    
                    # Record successful request
                    self.rate_limiter.record_success(base_domain)
                    
                    # Return unique list of results
                    results = list(dict.fromkeys(result_links))  # Remove duplicates while preserving order
                    
                    if not results:
                        logging.warning("âš ï¸ No results extracted from Google search, might be blocked")
                        
                        # If no results are found, it might be a blocked response
                        with open('google_search_page.html', 'w', encoding='utf-8') as f:
                            f.write(html_content)
                        
                        # Try browser-based search as fallback
                        return self._fallback_to_browser_search(keyword, num_results, page)
                    
                    return results[:num_results]
                    
                except (requests.RequestException, ConnectionError, TimeoutError) as e:
                    retry_count += 1
                    logging.warning(f"Error during Google search (attempt {retry_count}/{max_retries}): {e}")
                    
                    # Record error for rate limiting
                    self.rate_limiter.record_error(base_domain)
                    
                    if retry_count >= max_retries:
                        # After all HTTP-based retries fail, try with browser
                        logging.info("Switching to browser-based search after HTTP failures")
                        return self._fallback_to_browser_search(keyword, num_results, page)
                    
                    # Wait before retry with exponential backoff
                    time.sleep(5 * (2 ** retry_count))
        
        except Exception as e:
            logging.error(f"Unexpected error in Google search: {e}")
            logging.error(traceback.format_exc())
            # Try browser-based search as a last resort
            return self._fallback_to_browser_search(keyword, num_results, page)
        
        return results[:num_results]

    def _fallback_to_browser_search(self, keyword: str, num_results: int, page: int) -> List[str]:
        """
        Fallback to browser-based search when HTTP search fails
        
        Args:
            keyword (str): Search keyword
            num_results (int): Number of results to fetch
            page (int): Page number
            
        Returns:
            List[str]: Search result URLs
        """
        logging.info(f"Falling back to browser-based search for: {keyword}")
        try:
            # Create a new event loop if needed
            loop = get_or_create_event_loop()
            
            # Run the browser search
            search_results = loop.run_until_complete(
                self._search_google_with_browser(keyword, num_results, page)
            )
            
            if search_results:
                logging.info(f"Browser search succeeded with {len(search_results)} results")
                return search_results
        except Exception as e:
            logging.error(f"Browser fallback search also failed: {e}")
            logging.error(traceback.format_exc())
        
        # Return empty list if all methods fail
        return []

    async def _search_google_with_browser(self, keyword: str, num_results: int = 10, page_num: int = 0) -> List[str]:
        """
        Search Google using a browser (Playwright) to bypass bot detection
        
        Args:
            keyword (str): Search keyword
            num_results (int): Maximum number of results to return
            page_num (int): Page number (0-indexed)
            
        Returns:
            List[str]: List of URLs
        """
        # Check if we've already been rate limited recently for Google
        base_domain = "google.com"
        await self._check_domain_rate_limit_async(base_domain)
        
        # Prepare the search query
        if 'site:' not in keyword:
            if self.is_indian_domain(keyword):
                search_query = f'"{keyword}" site:.in'
            else:
                search_query = f'"{keyword}"'
        else:
            search_query = keyword
        
        # Initialize a new browser if needed
        if not hasattr(self, 'browser') or self.browser is None:
            await self.initialize_browser()
        
        if not self.browser:
            logging.error("Failed to initialize browser for Google search")
            return []
        
        results = []
        max_retries = 3
        retry_count = 0
        
        while retry_count < max_retries and len(results) < num_results:
            try:
                # Create a new page with stealth settings
                page = await self.browser.new_page()
                await self._apply_stealth_settings(page)
                
                # Record this access for rate limiting
                self.rate_limiter.record_request(base_domain)
                
                # Set up a longer timeout for Google
                page.set_default_timeout(60000)  # 60 seconds
                
                # Construct the Google search URL with .co.in to target Indian results
                start_index = page_num * 10
                google_url = f"https://www.google.co.in/search?q={quote(search_query)}&start={start_index}"
                
                # Log the URL being accessed
                logging.info(f"Browser accessing: {google_url}")
                
                # Add a random delay before accessing Google (between 5-15 seconds)
                delay = random.uniform(5, 15)
                logging.info(f"Adding pre-request delay of {delay:.1f}s")
                await asyncio.sleep(delay)
                
                # Navigate to Google
                response = await page.goto(google_url, wait_until="networkidle", timeout=90000)
                
                # Check for rate limiting or other issues
                if response.status == 429:
                    logging.warning("âš ï¸ Browser received 429 Too Many Requests from Google")
                    self.rate_limiter.record_error(base_domain, status_code=429)
                    
                    # Save the page for debugging
                    await page.screenshot(path=f"google_rate_limit_{int(time.time())}.png")
                    
                    # Wait longer before retry
                    backoff_time = 60 * (2 ** retry_count)
                    logging.info(f"Backing off for {backoff_time}s before retry")
                    await asyncio.sleep(backoff_time)
                    retry_count += 1
                    await page.close()
                    continue
                
                if response.status != 200:
                    logging.warning(f"Browser received non-200 status ({response.status}) from Google")
                    self.rate_limiter.record_error(base_domain, status_code=response.status)
                    retry_count += 1
                    await page.close()
                    continue
                
                # Wait some time for JavaScript to execute
                await asyncio.sleep(3)
                
                # Check for CAPTCHA
                captcha_detected = await self._detect_captcha_in_browser(page)
                if captcha_detected:
                    logging.warning("âŒ CAPTCHA detected in browser Google search")
                    self.rate_limiter.record_error(base_domain, status_code=403)
                    
                    # Take a screenshot of the CAPTCHA
                    await page.screenshot(path=f"google_captcha_{int(time.time())}.png")
                    
                    # Close this page and try again after a longer delay
                    await page.close()
                    await asyncio.sleep(120)  # 2 minute delay after CAPTCHA
                    retry_count += 1
                    continue
                
                # Simulate human behavior to avoid detection
                await self._simulate_human_browsing(page)
                
                # Extract URLs from the search results
                page_results = await self._extract_search_results_from_page(page)
                
                if not page_results:
                    logging.warning("No results found in browser Google search - may be blocked")
                    self.rate_limiter.record_error(base_domain)
                    
                    # Save the page content for debugging
                    content = await page.content()
                    with open(f"google_search_page_{page_num}.html", "w", encoding="utf-8") as f:
                        f.write(content)
                    
                    # Take a screenshot
                    await page.screenshot(path=f"google_search_page_{page_num}.png")
                    
                    # Close this page and try again
                    await page.close()
                    retry_count += 1
                    await asyncio.sleep(30)  # 30 second delay
                    continue
                
                # Record successful access
                self.rate_limiter.record_success(base_domain)
                
                # Add results to the list
                results.extend(page_results)
                
                # Close the page
                await page.close()
                
                # Wait before potentially going to the next page (if needed)
                await asyncio.sleep(random.uniform(3, 8))
                
                # If we haven't collected enough results and there are more pages
                if len(results) < num_results and page_num < 2:  # Limit to first 3 pages (0, 1, 2)
                    page_num += 1
                else:
                    break
                    
            except Exception as e:
                logging.error(f"Error in browser-based Google search: {e}")
                logging.error(traceback.format_exc())
                retry_count += 1
                
                # Wait before retry
                await asyncio.sleep(10 * retry_count)
        
        # Return unique results up to the requested number
        unique_results = list(dict.fromkeys(results))
        return unique_results[:num_results]
    
    def rotate_proxy(self):
        """Rotate to the next available proxy in the list."""
        if not self.proxy_list or len(self.proxy_list) <= 1:
            self.logger.warning("No alternative proxies available to rotate")
            return
            
        # Move first proxy to the end of the list
        current_proxy = self.proxy_list.pop(0)
        self.proxy_list.append(current_proxy)
        
        # Log the rotation
        proxy_desc = "direct connection" if current_proxy is None else f"proxy {self.proxy_list[0]}"
        self.logger.info(f"Rotated to next proxy: {proxy_desc}")
        print(f"ðŸ”„ Rotated to next proxy")

    def rotate_proxy_if_needed(self):
        """Check if proxy rotation is needed and rotate if so."""
        if random.random() < 0.2:  # 20% chance to rotate on any request
            self.rotate_proxy()

    def search_duckduckgo(self, keyword: str, num_results: int = 10) -> List[str]:
        """Search DuckDuckGo and return a list of result URLs."""
        urls = []
        
        # Use browser for DuckDuckGo which is better at handling their JavaScript
        if self.use_browser:
            self.logger.info("Using browser automation for DuckDuckGo search")
            
            # Create a new event loop for this synchronous method
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                # First initialize the browser
                browser_initialized = loop.run_until_complete(self.initialize_browser())
                if browser_initialized:
                    # Then run the search function
                    results = loop.run_until_complete(self._search_duckduckgo_with_browser(keyword, num_results))
                    return results
                else:
                    self.logger.warning("Failed to initialize browser for DuckDuckGo search")
            except Exception as e:
                self.logger.warning(f"Browser-based DuckDuckGo search failed: {e}, falling back to regular search")
            finally:
                # Close the loop when done
                loop.close()
        
        try:
            # DuckDuckGo's HTML frontend is more scraping-friendly
            search_url = f"https://html.duckduckgo.com/html/?q={quote(keyword)}"
            response = self.make_request(search_url)
            
            if response:
                soup = BeautifulSoup(response.text, 'html.parser')
                results = soup.find_all('a', {'class': 'result__a'})
                
                for result in results:
                    if result.get('href'):
                        href = result.get('href')
                        if 'duckduckgo.com' in href:
                            # Extract actual URL from DuckDuckGo's redirect
                            href = href.split('uddg=')[1].split('&')[0] if 'uddg=' in href else None
                        if href and href.startswith('http'):
                            urls.append(href)
                
        except Exception as e:
            self.logger.error(f"Error in DuckDuckGo search: {e}")
        
        return list(set(urls))[:num_results]
    
    def _find_contact_urls(self, html_content: str, base_url: str, domain: str) -> List[str]:
        """Find contact page URLs from the main page.
        
        Args:
            html_content: HTML content to parse
            base_url: Base URL for resolving relative URLs
            domain: Domain of the website
            
        Returns:
            List of contact page URLs
        """
        contact_urls = []
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Common patterns for contact page links
        contact_patterns = [
            'contact', 'contact-us', 'reach-us', 'connect', 'get-in-touch',
            'about-us', 'feedback', 'support', 'help', 'contactus'
        ]
        
        # Look for links with contact-related text or URLs
        for a in soup.find_all('a', href=True):
            href = a['href']
            text = a.get_text().lower().strip()
            
            # Skip empty links, anchor links, and javascript links
            if not href or href.startswith('#') or href.startswith('javascript:'):
                continue
                
            # Skip external links to other domains
            if href.startswith(('http://', 'https://')) and domain not in href:
                continue
            
            # Check if the link text contains contact-related keywords
            is_contact_text = any(pattern in text for pattern in contact_patterns)
            
            # Check if the URL contains contact-related keywords
            is_contact_url = any(pattern in href.lower() for pattern in contact_patterns)
            
            if is_contact_text or is_contact_url:
                # Use our URL resolver to build absolute URL if relative
                contact_url = self._resolve_url(base_url, href)
                
                # Verify the URL is valid and within the same domain
                try:
                    parsed_contact = urlparse(contact_url)
                    parsed_base = urlparse(base_url)
                    
                    # Only include links to the same domain
                    if parsed_contact.netloc and parsed_contact.netloc != parsed_base.netloc:
                        continue
                        
                    # Apply additional filtering to exclude non-content pages
                    if contact_url != base_url and contact_url not in contact_urls:
                        # Apply normalization and filtering
                        normalized_url = self._normalize_url(contact_url)
                        if not normalized_url.startswith('noindex:') and self._filter_url(normalized_url):
                            contact_urls.append(contact_url)
                except Exception as e:
                    self.logger.warning(f"Error parsing contact URL {href}: {e}")
                    continue
        
        # Prioritize URLs with clearer contact indicators
        contact_urls.sort(key=lambda url: 
            (1 if 'contact' in url.lower() else 2) +
            (1 if 'about' in url.lower() else 2)
        )
        
        # Limit to a reasonable number to avoid excessive requests
        return contact_urls[:3]
    
    def _extract_emails_from_text(self, text: str) -> Set[str]:
        """Extract and validate email addresses from text."""
        emails = set()
        
        # Standard email pattern
        email_matches = self.email_pattern.findall(text)
        for email in email_matches:
            # Basic validation to filter out false positives
            if self._validate_email(email):
                emails.add(email.lower())
        
        # Obfuscated email pattern (e.g., "user at domain dot com")
        obfuscated_matches = self.obfuscated_email_pattern.findall(text)
        for match in obfuscated_matches:
            if len(match) == 3:  # Should have 3 parts: username, domain, TLD
                reconstructed_email = f"{match[0]}@{match[1]}.{match[2]}"
                if self._validate_email(reconstructed_email):
                    emails.add(reconstructed_email.lower())
        
        return emails
    
    def _validate_email(self, email: str) -> bool:
        """Validate an email address with enhanced filtering for HTML/CSS artifacts."""
        # Use our improved validator
        return validate_email(email)
    
    def _extract_phones_from_text(self, text: str, source: str = "unknown") -> Set[Union[str, Dict]]:
        """Extract and validate phone numbers from text with enhanced logic for better detection.
        
        Args:
            text: The text to extract phone numbers from
            source: Where the text was found (e.g., 'homepage', 'contact_page')
            
        Returns:
            Set of validated phone numbers in E.164 format or as dictionaries with metadata
        """
        phones = set()
        
        if not text or len(text) < 5:
            return phones
        
        # ENHANCEMENT: Better preprocessing for phone number detection
        # 1. Normalize different types of separators
        text = re.sub(r'(\d)\s*[.\-â€“â€”Â·â€¢|:/\\]\s*(\d)', r'\1-\2', text)
        
        # 2. Better handle non-breaking spaces and other Unicode whitespace
        text = re.sub(r'\u00A0|\u2007|\u202F', ' ', text)
        
        # 3. Normalize parentheses with proper spacing 
        text = re.sub(r'(\d)\s*\(\s*(\d)', r'\1 (\2', text)
        text = re.sub(r'(\d)\s*\)\s*(\d)', r'\1) \2', text)
        
        # 4. Replace known formats like "Tel: ", "Phone: " with space for better extraction
        text = re.sub(r'(?i)(phone|mobile|telephone|contact|call|ph|tel|mob|cell)(\s*)(:|at|us|on|no|\#|number)(\s*)', ' ', text)
        
        # 5. Clean multiple spaces
        text = re.sub(r'\s+', ' ', text)
        
        # 6. Replace or with digits to handle formats like +91 or +91-
        text = re.sub(r'(\+\d+)\s+or\s+', r'\1 ', text)
        
        # 7. Explicitly handle hyphenated numbers and phone extensions
        text = re.sub(r'(\d+)\s*-\s*(\d+)', r'\1-\2', text)  # Fix spaced hyphens
        text = re.sub(r'(?i)ext\.?\s*(\d+)', r' ext\1', text)  # Normalize extensions
        
        # 8. Handle "dot" text separators sometimes used to obfuscate numbers
        text = re.sub(r'(\d+)\s*dot\s*(\d+)', r'\1.\2', text, flags=re.IGNORECASE)
        
        # Primary extraction using enhanced detection patterns
        # Try different patterns including our main patterns
        extraction_patterns = [
            # Standard 10-digit Indian mobile with optional +91 prefix
            r'(?:\+91[\s\-.]?)?[6789]\d{9}',
            
            # Format with spaces or hyphens every 3-4 digits
            r'(?:\+91[\s\-.]?)?[6789]\d{2,4}[\s\-.]?\d{2,4}[\s\-.]?\d{2,4}',
            
            # Format with STD code (landline)
            r'0\d{2,4}[\s\-.]?\d{6,8}',
            
            # Format with country code and parentheses
            r'\(\+?91\)[\s\-.]?[6789]\d{9}',
            
            # Format with parentheses around area/STD code
            r'\(\d{2,5}\)[\s\-.]?\d{5,8}',
            
            # International format without explicit country code (often business numbers)
            r'\+\d{1,3}[\s\-.]?\d{6,14}',
            
            # Explicit check for WhatsApp numbers which often use different formatting
            r'(?:whatsapp|wa)[\s:]*(?:\+91[\s\-.]?)?[6789]\d{9}'
        ]
        
        for pattern in extraction_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                valid_phone = self.validate_indian_phone(match, f"{source}_enhanced_pattern")
                if valid_phone:
                    phones.add(valid_phone)
        
        # Use our existing patterns as fallback
        # Main pattern
        phone_matches = self.phone_pattern.findall(text)
        for phone_match in phone_matches:
            if isinstance(phone_match, tuple):
                for group in phone_match:
                    if group and len(group.strip()) >= 5:
                        valid_phone = self.validate_indian_phone(group, source)
                        if valid_phone:
                            phones.add(valid_phone)
            else:
                if phone_match and len(phone_match.strip()) >= 5:
                    valid_phone = self.validate_indian_phone(phone_match, source)
                    if valid_phone:
                        phones.add(valid_phone)
        
        # Alternative pattern
        alt_matches = self.phone_pattern_alt.findall(text)
        for phone_match in alt_matches:
            if isinstance(phone_match, tuple):
                for group in phone_match:
                    if group and len(group.strip()) >= 5:
                        valid_phone = self.validate_indian_phone(group, source)
                        if valid_phone:
                            phones.add(valid_phone)
            else:
                if phone_match and len(phone_match.strip()) >= 5:
                    valid_phone = self.validate_indian_phone(phone_match, source)
                    if valid_phone:
                        phones.add(valid_phone)
        
        # ENHANCEMENT: Look for specific contexts that strongly indicate phone numbers
        contexts = [
            # Look for phrases like "Call us at" followed by numbers
            (r'(?i)(?:call|dial|phone|contact)\s+(?:us|our|me)?\s*(?:at|on|:)?\s*((?:\+?91)?[\s\-.]?[0-9\s\-\.]{8,16})', 1),
            # Look for "WhatsApp" followed by numbers
            (r'(?i)(?:whatsapp|wa)[\s:]+([0-9\s\+\-\.]{8,16})', 1),
            # Look for For Sales: followed by numbers
            (r'(?i)(?:for|sales|support|help|service)[\s:]+([0-9\s\+\-\.]{8,16})', 1),
            # Look for Mobile: followed by numbers
            (r'(?i)(?:mobile|cell|m)[\s:]+([0-9\s\+\-\.]{8,16})', 1),
            # Look for tel: links which often contain phone numbers
            (r'tel:([0-9\s\+\-\.]{8,16})', 1)
        ]
        
        for pattern, group_idx in contexts:
            context_matches = re.findall(pattern, text)
            for match in context_matches:
                if isinstance(match, tuple) and len(match) > group_idx:
                    phone_text = match[group_idx]
                else:
                    phone_text = match
                    
                valid_phone = self.validate_indian_phone(phone_text, f"{source}_context")
                if valid_phone:
                    phones.add(valid_phone)
        
        # ENHANCEMENT: Additional check for Indian number formats with different prefixes
        # Some Indian numbers might be displayed with different formatting for better readability
        for raw_digits in re.findall(r'\b(0?[6789]\d{9})\b', text):
            if len(raw_digits) >= 10:
                valid_phone = self.validate_indian_phone(raw_digits, f"{source}_raw_digits")
                if valid_phone:
                    phones.add(valid_phone)
        
        return phones
    
    def test_extraction(self, test_urls=None):
        """Test the extraction functionality with sample URLs."""
        if test_urls is None:
            # Default test URLs with known contact information
            test_urls = [
                "https://www.digitalmarketingdelhi.in/",
                "https://www.socialbeat.in/", 
                "https://digitalready.co/",
                "https://www.webchutney.com/contact",
                "https://www.techmagnate.com/contact-us.html"
            ]
        
        print("\n=== CONTACT EXTRACTION TEST ===")
        all_emails = set()
        all_phones = set()
        
        for url in test_urls:
            print(f"\nTesting URL: {url}")
            try:
                emails, phones = self.extract_contacts_from_url(url)
                
                if emails:
                    print("Emails found:")
                    for email in emails:
                        print(f"  - {email}")
                        all_emails.add(email)
                else:
                    print("No emails found")
                    
                if phones:
                    print("Phones found:")
                    for phone in phones:
                        print(f"  - {phone}")
                        all_phones.add(phone)
                else:
                    print("No phones found")
                    
            except Exception as e:
                print(f"Error: {e}")
        
        print("\nTest Summary:")
        print(f"Total unique emails found: {len(all_emails)}")
        print(f"Total unique phones found: {len(all_phones)}")
        return all_emails, all_phones
    
    def _check_domain_rate_limit(self, domain):
        """Check if we should rate limit a domain access and wait if needed."""
        current_time = time.time()
        
        # If we've accessed this domain recently, enforce a delay
        if domain in self.domain_access_times:
            last_access = self.domain_access_times[domain]
            elapsed = current_time - last_access
            
            if elapsed < self.domain_min_interval:
                wait_time = self.domain_min_interval - elapsed + random.uniform(1, 5)
                self.logger.info(f"Rate limiting domain {domain}. Waiting {wait_time:.2f} seconds")
                time.sleep(wait_time)
    
    def _track_domain_access(self, domain):
        """Track when we accessed a domain and how many times."""
        self.domain_access_times[domain] = time.time()
        
        if domain in self.domain_request_count:
            self.domain_request_count[domain] += 1
        else:
            self.domain_request_count[domain] = 1
            
        # Add to recent domains set for rate limiting
        self.recent_domains.add(domain)
        
    async def _check_domain_rate_limit_async(self, domain):
        """Async version of domain rate limiting."""
        current_time = time.time()
        
        # If we've accessed this domain recently, enforce a delay
        if domain in self.domain_access_times:
            last_access = self.domain_access_times[domain]
            elapsed = current_time - last_access
            
            if elapsed < self.domain_min_interval:
                wait_time = self.domain_min_interval - elapsed + random.uniform(1, 5)
                self.logger.info(f"Rate limiting domain {domain}. Waiting {wait_time:.2f} seconds")
                await asyncio.sleep(wait_time)
                
    def is_indian_domain(self, url):
        """Check if a domain is likely to be Indian based on TLD or content."""
        # First check TLD for .in domains
        domain = urlparse(url).netloc
        if self.indian_domain_pattern.search(domain):
            return True
            
        # Check for common Indian domain names
        indian_terms = ['india', 'bharat', 'desi', 'hindustan', 'bharatiya', 'sarkari']
        for term in indian_terms:
            if term in domain.lower():
                return True
                
        # Use TLD extract to check if the site is from India
        extract_result = tldextract.extract(url)
        if extract_result.suffix == 'in':
            return True
            
        return False
        
    def __enter__(self):
        """Support for 'with' context manager."""
        return self
        
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Clean up resources when exiting 'with' context."""
        # Use our run_async helper function for cleanup
        try:
            run_async(self.close_browser())
        except Exception as e:
            self.logger.warning(f"Error during browser cleanup in context exit: {e}")
        
        return False  # Don't suppress exceptions
        
    def __del__(self):
        """Destructor to ensure proper cleanup."""
        try:
            # Attempt to safely clean up browser resources
            if hasattr(self, 'browser') and self.browser:
                # Use our run_async helper for proper event loop management
                try:
                    run_async(self.close_browser())
                except Exception as e:
                    self.logger.warning(f"Browser cleanup failed in destructor: {e}")
        except Exception as e:
            # Log but continue since we're in destructor
            if hasattr(self, 'logger'):
                self.logger.warning(f"Error in destructor: {e}")
            else:
                print(f"Error in destructor: {e}")

    def test_regex(self, sample_html=None):
        """Unit test function to validate email and phone extraction patterns."""
        if sample_html is None:
            sample_html = """
            <p>Contact us at: contact@example.com, support@company.co.in</p>
            <p>Call us: +91 9876543210, 8765432109, 07654321098</p>
            <p>Email: info@domain.in or marketing@site.com</p>
            """
        
        print("=== REGEX PATTERN TEST ===")
        soup = BeautifulSoup(sample_html, 'html.parser')
        text = soup.get_text()
        
        print("Text sample:", text.strip()[:100] + "..." if len(text) > 100 else text.strip())
        print("\nEmail pattern:", self.email_pattern.pattern)
        emails = self.email_pattern.findall(text)
        print("Emails found:", emails)
        
        print("\nPhone pattern:", self.phone_pattern.pattern)
        phone_matches = self.phone_pattern.findall(text)
        print("Phone matches:", phone_matches)
        
        # Test the phone validation with source information
        print("\nTesting phone validation:")
        for phone_match in phone_matches:
            if isinstance(phone_match, tuple):
                for group in phone_match:
                    if group:
                        print(f"\nTesting: {group}")
                        valid_phone = self.validate_indian_phone(phone_match, "test_sample")
                        if valid_phone:
                            print(f"  âœ“ Valid: {valid_phone['phone']} (Source: {valid_phone['source']})")
                        else:
                            print(f"  âœ— Invalid")
            else:
                print(f"\nTesting: {phone_match}")
                valid_phone = self.validate_indian_phone(phone_match, "test_sample")
                if valid_phone:
                    print(f"  âœ“ Valid: {valid_phone['phone']} (Source: {valid_phone['source']})")
                else:
                    print(f"  âœ— Invalid")
        
        # Test using the improved validator directly
        print("\nTesting improved validator directly:")
        for phone_match in ["+91 9876543210", "8765432109", "07654321098"]:
            print(f"\nDirect test: {phone_match}")
            result = validate_indian_phone(phone_match, "test_sample")
            if result:
                print(f"  âœ“ Valid: {result['phone']} (Original: {result['original']}, Source: {result['source']})")
            else:
                print(f"  âœ— Invalid")

    def save_detailed_results_to_csv(self, keyword: str, results_by_url: List[Dict]):
        """Save detailed extraction results to a CSV file, including per-URL findings."""
        # Create directory if it doesn't exist
        os.makedirs('scraped_data', exist_ok=True)
        
        # Create a safe filename
        safe_keyword = re.sub(r'[^\w\s-]', '', keyword).strip().replace(' ', '_')
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        filename = f"scraped_data/{safe_keyword}_{timestamp}_detailed.csv"
        
        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
            fieldnames = ['url', 'domain', 'emails', 'phones', 'phone_sources', 'error']
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            
            for result in results_by_url:
                # Process phone numbers to extract source information
                phones_list = []
                sources_list = []
                
                for phone in result.get('phones', []):
                    if isinstance(phone, dict):
                        # If phone is in the new dictionary format
                        phones_list.append(phone.get('phone', ''))
                        sources_list.append(f"{phone.get('phone', '')}: {phone.get('source', 'unknown')}")
                    else:
                        # If phone is a string (legacy format)
                        phones_list.append(phone)
                        sources_list.append(f"{phone}: unknown")
                
                # Convert list fields to comma-separated strings
                row = {
                    'url': result.get('url', ''),
                    'domain': result.get('domain', ''),
                    'emails': ','.join(result.get('emails', [])),
                    'phones': ','.join(phones_list),
                    'phone_sources': '; '.join(sources_list),
                    'error': result.get('error', '')
                }
                writer.writerow(row)
            
        self.logger.info(f"Detailed results saved to {filename}")
        return filename
        
    def save_results_to_csv(self, keyword: str, emails: Set[str], phones: Set[str]):
        """Save extracted contacts to a CSV file with enhanced phone information."""
        # Create directory if it doesn't exist
        os.makedirs('scraped_data', exist_ok=True)
        
        # Create a safe filename
        safe_keyword = re.sub(r'[^\w\s-]', '', keyword).strip().replace(' ', '_')
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        filename = f"scraped_data/{safe_keyword}_{timestamp}_contacts.csv"
        
        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.writer(csvfile)
            # Updated header with source information
            writer.writerow(['Type', 'Contact', 'Original', 'Source'])
            
            # Write emails
            for email in emails:
                writer.writerow(['Email', email, '', ''])
            
            # Write phones with original format and source if available
            for phone in phones:
                if isinstance(phone, dict):
                    # If phone is already in the new dictionary format
                    writer.writerow(['Phone', phone.get('phone', ''), 
                                    phone.get('original', ''), 
                                    phone.get('source', 'unknown')])
                else:
                    # If phone is in the legacy string format
                    writer.writerow(['Phone', phone, '', ''])
                
        self.logger.info(f"Results saved to {filename}")
        return filename

    def scrape(self, keyword: str, num_results: int = 50, max_runtime_minutes: int = 15, task_id=None, task_record=None):
        """
        Main method to scrape contact information based on a keyword.
        
        Args:
            keyword: Search keyword
            num_results: Maximum number of results to return
            max_runtime_minutes: Maximum runtime in minutes
            task_id: Optional task ID for tracking progress
            task_record: Optional database record for progress tracking
            
        Returns:
            Dict containing emails and phones
        """
        start_time = time.time()
        max_runtime_seconds = max_runtime_minutes * 60
        
        emails = set()
        phones = set()
        scraped_urls = set()  # Track already scraped URLs
        search_page = 1  # Track which search result page we're on
        max_search_pages = 10  # Maximum search pages to try
        
        # Tracking proxy performance
        successful_proxy_requests = 0
        failed_proxy_requests = 0
        proxy_rotation_threshold = 3  # Rotate proxy after this many consecutive failures
        consecutive_failures = 0
        
        # For better time management
        time_per_url = 10  # Initial estimate of seconds needed per URL
        urls_processed = 0
        
        # Set of known spam domains to filter out early
        spam_domains = {
            'pinterest.com', 'youtube.com', 'facebook.com', 'instagram.com', 'twitter.com',
            'linkedin.com', 'tiktok.com', 'reddit.com', 'quora.com', 'indeed.com', 'glassdoor.com',
            'alibaba.com', 'indiamart.com', 'tradeindia.com', 'amazon.com', 'flipkart.com',
            'justdial.com', 'sulekha.com', 'craigslist.org', 'naukri.com', 'monster.com',
            'yelp.com', 'zomato.com', 'swiggy.com', 'imdb.com', 'wikipedia.org', 'wikihow.com'
        }
        
        # URLs where we've found results
        successful_urls = []
        results_by_url = []
        
        # For tracking timeouts
        timeout_count = 0
        max_timeouts = 5  # Maximum consecutive timeouts before changing approach
        
        # Initialize target counts
        self.target_results = num_results
        self.found_emails = set()
        self.found_phones = set()
        
        self.logger.info(f"Starting scrape for keyword: '{keyword}'")
        print(f"ðŸ” Starting search for '{keyword}', targeting {num_results} contacts within {max_runtime_minutes} minutes")
        
        # Initialize task status if tracking is enabled
        if task_id:
            status_data = {
                "status": "processing",
                "progress": 5,
                "message": f"Starting search for '{keyword}'",
                "keyword": keyword,
                "task_id": task_id,
                "results_count": 0,
                "elapsed_time": 0
            }
            # Use the global update_task_status function
            if 'update_task_status' in globals():
                globals()['update_task_status'](task_id, status_data, task_record)
        
        try:
            while len(phones) < num_results and search_page <= max_search_pages:
                # Check if we've exceeded our runtime
                elapsed_time = time.time() - start_time
                if elapsed_time > max_runtime_seconds:
                    self.logger.info(f"Reached maximum runtime of {max_runtime_minutes} minutes")
                    print(f"â±ï¸ Time limit reached after {elapsed_time/60:.1f} minutes")
                    break
                
                # Calculate remaining time and adjust strategy
                remaining_seconds = max_runtime_seconds - elapsed_time
                print(f"â±ï¸ {remaining_seconds/60:.1f} minutes remaining of {max_runtime_minutes} minutes")
                
                # Update task progress if tracking is enabled
                if task_id:
                    # Calculate progress based on time or results
                    time_progress = min(int((elapsed_time / max_runtime_seconds) * 90), 90)
                    results_progress = min(int((len(phones) / num_results) * 90), 90)
                    progress = max(time_progress, results_progress)
                    
                    status_data = {
                        "status": "processing",
                        "progress": progress,
                        "message": f"Searching page {search_page}, found {len(emails)} emails and {len(phones)} phones",
                        "keyword": keyword,
                        "task_id": task_id,
                        "results_count": len(emails) + len(phones),
                        "elapsed_time": elapsed_time / 60.0
                    }
                    if 'update_task_status' in globals():
                        globals()['update_task_status'](task_id, status_data, task_record)
                
                # Update time per URL estimate if we have data
                if urls_processed > 0:
                    time_per_url = min(elapsed_time / urls_processed, 30)  # Cap at 30 seconds
                
                # Estimate how many URLs we can process in remaining time
                estimated_urls_remaining = int(remaining_seconds / time_per_url)
                print(f"ðŸ“Š Can process approximately {estimated_urls_remaining} more URLs in the remaining time")
                
                # Check if we've exceeded max requests for this domain
                if self.domain_request_count.get(domain, 0) >= self.max_requests_per_domain:
                    self.logger.warning(f"Maximum request limit reached for domain {domain}")
                    # Try with browser as fallback
                    if self.use_browser:
                        loop = get_or_create_event_loop()
                        try:
                            browser_initialized = loop.run_until_complete(self.initialize_browser())
                            if browser_initialized:
                                return loop.run_until_complete(self.browser_get_page(url, max_retries=1))
                        except Exception as e:
                            self.logger.error(f"Browser fallback request failed: {e}")
                    return None
                
                # Check and enforce rate limiting
                self._check_domain_rate_limit(domain)
                
                # Track this request
                self._track_domain_access(domain)
                
                retries = 0
                headers = self._get_realistic_headers(url)
                
                while retries < max_retries:
                    try:
                        # Add a shorter random delay before request
                        delay = random.uniform(0.5, 2.0)
                        self.logger.info(f"Waiting {delay:.2f} seconds before requesting {url}")
                        time.sleep(delay)
                        
                        # Get a random proxy for this attempt
                        proxies = self.get_random_proxy()
                        
                        # Use shorter timeout
                        timeout = 15 if proxies else 10
                        
                        # Use our session with cookies
                        response = self.session.get(
                            url,
                            headers=headers,
                            proxies=proxies,
                            timeout=timeout,
                            verify=True,
                            cookies=self.cookies
                        )
                        
                        # Store cookies from this response
                        if response.cookies:
                            self.cookies.update(dict(response.cookies))
                        
                        # Check response status
                        if response.status_code == 200:
                            # Check for CAPTCHA or bot detection in response - simplified check
                            content_lower = response.text.lower()
                            if 'captcha' in content_lower or 'robot' in content_lower or 'automated' in content_lower:
                                self.logger.warning(f"CAPTCHA detected at {url}. Adding to CAPTCHA domains list.")
                                self.captcha_detected_domains.add(domain)
                                
                                # Fall back to browser automation if needed
                                if self.use_browser and retries >= 1:
                                    loop = get_or_create_event_loop()
                                    try:
                                        browser_initialized = loop.run_until_complete(self.initialize_browser())
                                        if browser_initialized:
        "results_count": 0,
        "elapsed_time": 0
    }
    
    # Update the status
    update_task_status(task_id, status, task_record)
    
    # Create scraper and start time tracking
    start_time = time.time()
    scraper = ContactScraper(use_browser=not args.no_browser)
    
    try:
        # Update status before scraping
        status["progress"] = 5
        status["message"] = "Searching for websites..."
        update_task_status(task_id, status, task_record)
        
        # Run the scraping process
        results = scraper.scrape(
            args.keyword, 
            num_results=args.results, 
            max_runtime_minutes=args.time,
            task_id=task_id,
            task_record=task_record
        )
        
        # Calculate elapsed time
        elapsed_time = (time.time() - start_time) / 60.0  # in minutes
        
        # Update final results
        status["status"] = "completed"
        status["progress"] = 100
        status["message"] = f"Found {len(results['emails'])} emails and {len(results['phones'])} phone numbers"
        status["results_count"] = len(results['emails']) + len(results['phones'])
        status["elapsed_time"] = elapsed_time
        update_task_status(task_id, status, task_record)
        
        if not args.api_mode:
            print(f"\nâœ… Found {len(results['emails'])} emails and {len(results['phones'])} phone numbers")
        
        # Save results to CSV
        csv_file = scraper.save_results_to_csv(args.keyword, results['emails'], results['phones'])
        if not args.api_mode:
            print(f"Results saved to: {csv_file}")
        
    except Exception as e:
        # Update status with error
        elapsed_time = (time.time() - start_time) / 60.0
        status["status"] = "error"
        status["message"] = f"Error: {str(e)}"
        status["elapsed_time"] = elapsed_time
        update_task_status(task_id, status, task_record)
        
        if not args.api_mode:
            print(f"\nâŒ Error during scraping: {e}")
            import traceback
            traceback.print_exc()
    finally:
        # Clean up browser resources
        if not args.no_browser:
            if not args.api_mode:
                print("Cleaning up browser resources...")
            try:
                import asyncio
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                try:
                    loop.run_until_complete(scraper.close_browser())
                finally:
                    loop.close()
                if not args.api_mode:
                    print("âœ… Browser cleanup completed")
            except Exception as e:
                if not args.api_mode:
                    print(f"âš ï¸ Error during browser cleanup: {e}")
        
        # Update the task record if it was created (final status update)
        if task_record:
            try:
                from django.utils import timezone
                if status["status"] == "completed":
                    task_record.status = 'completed'
                    task_record.progress = 100
                    task_record.completed_at = timezone.now()
                elif status["status"] == "error":
                    task_record.status = 'failed'
                    task_record.error_message = status["message"]
                    task_record.completed_at = timezone.now()
                task_record.save()
                if not args.api_mode:
                    print(f"âœ… Updated BackgroundTask record: status={task_record.status}")
            except Exception as e:
                if not args.api_mode:
                    print(f"âš ï¸ Could not update BackgroundTask record: {e}")

